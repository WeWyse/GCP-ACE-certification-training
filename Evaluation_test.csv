Question;A;B;C;D;E;result;explanation
Instance templates are used to create a group of identical VMs. The instance templates include:;Machine type, boot disk image or container image, zone, and labels;Cloud Storage bucket definitions;A load balancer description;App Engine configuration file;;A;Machine type, boot disk image or container image, zone, and labels are all configuration parameters or attributes of a VM and therefore would be included in an instance group configuration that creates those VMs.
The command-line command to create a Cloud Storage bucket is:;gcloud mb;gsutil mb;gcloud mkbucket;gsutil mkbucket;;B;gsutil is the command line for accessing and manipulating Cloud Storage from the command line. mb is the specific command for creating, or making, a bucket.
Your company has an object management policy that requires that objects stored in Cloud Storage be migrated from regional storage to nearline storage 90 days after the object is created. The most efficient way to do this is to:;Create a cloud function to copy objects from regional storage to nearline storage.;Set the MigrateObjectAfter property on the stored object to 90 days.;Copy the object to persistent storage attached to a VM and then copy the object to a bucket created on nearline storage.;Create a lifecycle management configuration policy specifying an age of 90 days and SetStorageClass as nearline.;;D;The lifecycle configuration policy allows administrators to specify criteria for migrating data to other storage systems without having to concern themselves with running jobs to actually execute the necessary steps. The other options are inefficient or do not exist.
"An education client maintains a site where users can upload videos, and your client needs to assure redundancy for the files; therefore, you have created two buckets for Cloud Storage. Which command do you use to synchronize the contents of the two buckets?";gsutil rsync;gcloud cp sync;gcloud rsync;gsutil cp sync;;A;gsutil is the command-line tool for working with Cloud Storage. rsync is the specific command in gsutil for synchronizing buckets.
VPCs are ______ resources.;Regional;Zonal;Global;Subnet;;C;Google operates a global network, and VPCs are resources that can span that global network.
A remote component in your network has failed, which results in a transient network error. When you submit a gsutil command, it fails because of a transient error. By default, the command will:;Terminate and log a message to Stackdriver;Retry using a truncated binary exponential back-off strategy;Prompt the user to decide to retry or quit;Terminate and log a message to Cloud Shell;;B;gcloud by default will retry a failed network operation and will wait a long time before each retry. The time to wait is calculated using a truncated binary exponential back-off strategy.
All of the following are components of firewall rules except which one?;Direction of traffic;Action on match;Time to live (TTL);Protocol;;C;Firewall rules do not have TTL parameters. Direction of traffic, action on match, and protocol are all components of firewall rules.
Adding virtual machines to an instance group can be triggered in an autoscaling policy by all of the following, except which one?;CPU utilization;Stackdriver metrics;IAM policy violation;Load balancing serving capacity;;C;IAM policy violations do not trigger changes in the size of clusters. All other options can be used to trigger a change in cluster size.
Your company’s finance department is developing a new account management application that requires transactions and the ability to perform relational database operations using fully compliant SQL. Data store options in GCP include:;Spanner and Cloud SQL;Datastore and Bigtable;Spanner and Cloud Storage;Datastore and Cloud SQL;;A;"Only Spanner and Cloud SQL databases support transactions and have a SQL interface. Datastore has transactions but does not support fully compliant SQL; it has a SQL-like query language. Cloud Storage does not support transactions or SQL."
The marketing department in your company wants to deploy a web application but does not want to have to manage servers or clusters. A good option for them is:;Compute Engine;Kubernetes Engine;App Engine;Cloud Functions;;C;App Engine is a PaaS that allows developers to deploy full applications without having to manage servers or clusters. Compute Engine and Kubernetes Engine require management of servers. Cloud Functions is suitable for short-running Node.js or Python functions but not full applications.
Your company is building an enterprise data warehouse and wants SQL query capabilities over petabytes of data, but does not want to manage servers or clusters. A good option for them is:;Cloud Storage;BigQuery;Bigtable;Datastore;;B;BigQuery is designed for petabyte-scale analytics and provides a SQL interface.
You have been hired as a consultant to a startup in the Internet of Things (IoT) space. The startup will stream large volumes of data into GCP. The data needs to be filtered, transformed, and analyzed before being stored in GCP Datastore. A good option for the stream processing component is:;Dataproc;Cloud Dataflow;Cloud Endpoints;Cloud Interconnect;;B;Cloud Dataflow allows for stream and batch processing of data and is well suited for this kind of ETL work. Dataproc is a managed Hadoop and Spark service that is used for big data analytics. Cloud Endpoints is an API service, and Cloud Interconnect is a network service.
Preemptible virtual machines may be shut down at any time but will always be shut down after running:;6 hours;12 hours;24 hours;48 hours;;C;If a preemptible machine has not been shut down within 24 hours, Google will stop the instance.
You have been tasked with designing an organizational hierarchy for managing departments and their cloud resources. What organizing components are available in GCP?;Organization, folders, projects;Buckets, directories, subdirectories;Organizations, buckets, projects;Folders, buckets, projects;;A;Organizations, folders, and projects are the components used to manage an organizational hierarchy. Buckets, directories, and subdirectories are used to organize storage.
During an incident that has caused an application to fail, you suspect some resource may not have appropriate roles granted. The command to list roles granted to a resource is:;gutil iam list-grantable-roles;gcloud iam list-grantable-roles;gcloud list-grantable-roles;gcloud resources grantable-roles;;B;gcloud is the command-line tool for working with IAM, and list-grantable-roles is the correct command.
The availability of CPU platforms can vary between zones. To get a list of all CPU types available in a particular zone, you should use:;gcloud compute zones describe;gcloud iam zones describe;gutil zones describe;gcloud compute regions list;;A;gcloud is the command-line tool for manipulating compute resources, and zones describe is the correct command.
To create a custom role, a user must possess which role?;iam.create;compute.roles.create;iam.roles.create;Compute.roles.add;;C;"iam.roles.create is correct; the other roles do not exist."
You have been asked to create a network with 1,000 IP addresses. In the interest of minimizing unused IP addresses, which CIDR suffix would you use to create a network with at least 1,000 addresses but no more than necessary?;/20;/22;/28;/32;;B;The /22 suffi x produces 1,022 usable IP addresses.
A team of data scientists have asked for your help setting up an Apache Spark cluster. You suggest they use a managed GCP service instead of managing a cluster themselves on Compute Engine. The service they would use is:;Cloud Dataproc;Cloud Dataflow;Cloud Hadoop;BigQuery;;A;Cloud Dataproc is the managed Spark service. Cloud Datafl ow is for stream and batch processing of data, BigQuery is for analytics, and Cloud Hadoop is not a GCP service.
You have created a web application that allows users to upload files to Cloud Storage. When files are uploaded, you want to check the file size and update the user’s total storage used in their account. A serverless option for performing this action on load is:;Cloud Dataflow;Cloud Dataproc;Cloud Storage;Cloud Functions;;D;Cloud Functions responds to events in Cloud Storage, making them a good choice for taking an action after a fi le is loaded.
Your company has just started using GCP, and executives want to have a dedicated connection from your data center to the GCP to allow for large data transfers. Which networking service would you recommend?;Google Cloud Carrier Internet Peering;Google Cloud Interconnect – Dedicated;Google Cloud Internet Peering;Google Cloud DNS;;B;Google Cloud Interconnect – Dedicated is the only option for a dedicated connection between a customer’s data center and a Google data center.
You want to have GCP manage cryptographic keys, so you’ve decided to use Cloud Key Management Services. Before you can start creating cryptographic keys, you must:;Enable Google Cloud Key Management Service (KMS) API and set up billing;Enable Google Cloud KMS API and create folders;Create folders and set up billing;Give all users grantable roles to create keys;;A;Enabling the Google Cloud KMS API and setting up billing are steps common to using GCP services.
In Kubernetes Engine, a node pool is:;A subset of nodes across clusters;A set of VMs managed outside of Kubernetes Engine;A set of preemptible VMs;A subset of node instances within a cluster that all have the same configuration;;D;A node pool is a subset of node instances within a cluster that all have the same confi guration.
The GCP service for storing and managing Docker containers is:;Cloud Source Repositories;Cloud Build;Container Registry;Docker Repository;;C;The GCP service for storing and managing Docker containers is Container Registry. Cloud Build is for creating images. The others are not GCP services.
Code for Cloud Functions can be written in:;Node.js and Python;Node.js, Python, and Go;Python and Go;Python and C;;B;Node.js 6, Node.js 8, Python, and Go are the languages supported by Cloud Functions.
What is the fundamental unit of computing in cloud computing?;Physical server;VM;Block;Subnet;;B;The basic unit for purchasing computing resources is the virtual machine (VM). A physical server underlies VMs, but the resources of a physical server are allocated to VMs. Blocks and subnets are not relevant to the fundamental unit of computing.
If you use a cluster that is managed by a cloud provider, which of these will be managed for you by the cloud provider?;Monitoring;Networking;Some security management tasks;All of the above;;D;When using managed clusters, the cloud provider will monitor the health of nodes in the cluster, set up networking between nodes in the cluster, and configure firewall and other security controls.
"You need serverless computing for file processing and running the backend of a website; which two products can you choose from Google Cloud Platform?";Kubernetes Engine and Compute Engine;App Engine and Cloud Functions;Cloud Functions and Compute Engine;Cloud Functions and Kubernetes Engine;;B;App Engine is a serverless platform for running applications, while Cloud Functions is a service for executing short-running functions in response to events. Kubernetes Engine is a managed cluster service, and both Kubernetes Engine and Compute Engine require you to configure servers.
You have been asked to design a storage system for a web application that allows users to upload large data files to be analyzed by a business intelligence workflow. The files should be stored in a high-availability storage system. File system functionality is not required. Which storage system in Google Cloud Platform should be used?;Block storage;Object storage;Cache;Network File System;;B;Object storage, like Cloud Storage, provides redundantly stored objects without limits on the amount of data you can store, which makes option B correct. Since file system functionality is not required, option D is not a good option. Block storage could be used, but you would have to manage your own replication to ensure high availability. Caches are transient, in-memory storage and are not high-availability, persistent storage systems.
All block storage systems use what block size?;4KB;8KB;16KB;Block size can vary.;;D;"Block sizes in a block storage system can vary; therefore, option D is the correct answer. Block size is established when a file system is created. 4KB block sizes are commonly used in Linux."
You have been asked to set up network security in a virtual private cloud. Your company wants to have multiple subnetworks and limit traffic between the subnetworks. Which network security control would you use to control the flow of traffic between subnets?;Identity access management;Router;Firewall;IP address table;;C;"Firewalls in Google Cloud Platform (GCP) are software-defined network controls that limit the flow of traffic into and out of a network or subnetwork, so option C is the correct answer. Routers are used to move traffic to appropriate destinations on the network. Identity access management is used for authenticating and authorizing users; it is not relevant to network controls between subnetworks. IP address tables are not a security control."
When you create a machine learning service to identify text in an image, what type of servers should you choose to manage compute resources?;VMs;Clusters of VMs;"No servers; specialized services are serverless";VMs running Linux only;;C;Option C is correct because specialized services in GCP are serverless. Google manages the compute resources used by the services. There is no need for a user to allocate or monitor VMs.
Investing in servers for extended periods of time, such as committing to use servers for three to five years, works well when?;A company is just starting up;A company can accurately predict server need for an extended period of time;A company has a fixed IT budget;A company has a variable IT budget;;B;"Option B is correct; investing in servers works well when an organization can accurately predict the number of servers and other equipment it will need for an extended period and can utilize that equipment consistently. Startups are not established businesses with histories that can guide expected needs in three to five years. It does not matter if a budget is fixed or variable; investing in servers should be based on demand for server capacity."
Your company is based in X and will be running a virtual server for Y. What factor determines the unit per minute cost?;The time of day the VM is run;The characteristics of the server;The application you run;None of the above;;B;The characteristics of the server, such as the number of virtual servers, the amount of memory, and the region where you run the VM, influence the cost, so option B is correct. Time of day is not a factor, nor is the type of application you run on the VM.
You plan to use Cloud Vision to analyze images and extract text seen in the image. You plan to process between 1,000 and 2,500 images per hour. How many VMs should you allocate to meet peak demand?;1;10;25;"None; Cloud Vision is a serverless service.";;D;Cloud Vision is one of GCP’s specialized services. Users of the service do not need to configure any VMs to use the service.
You have to run a number of services to support an application. Which of the following is a good deployment model?;Run on a large, single VM;Use containers in a managed cluster;Use two large VMs, making one of them read only;Use a small VM for all services and increase the size of the VM when CPU utilization exceeds 90 percent;;B;Containers give the most flexibility for using the resources of a cluster efficiently and orchestration platforms reduce the operations overhead, which makes option B correct. Running in a single cluster is not recommended because if the server fails, all services will be down. Using two VMs with one read-only is not useful. Read-only servers are sometimes used with databases, but there was no mention of databases in the question. Using a small VM and upgrading when it is no longer able to keep up with the workload delivers poorquality service to users and should be avoided.
You have created a VM. Which of the following system administration operations are you allowed to perform on it?;Configure the file system;Patch operating system software;Change file and directory permissions;All of the above;;D;All of the operations are available to a system administrator after creating a VM, so option D is correct.
Cloud Filestore is based on what file system technology?;Network File System (NFS);XFS;EXT4;ReiserFS;;A;"Option A is correct; Cloud Filestore is based on Network Filesystem (NSF), which is a distributed file management system. The other options are file systems supported by Linux but are not the foundation of Cloud Filestore."
When setting up a network in GCP, your network the resources in it are treated as what?;Virtual private cloud;Subdomain;Cluster;None of the above;;A;When you create a network, it is treated as a virtual private cloud, which makes option A correct. Resources are added to the VPC and are not accessible outside the VPC unless you explicitly configure them to be. A subdomain is related to web domains and not related to GPC network configuration. Clusters, such as Kubernetes clusters, may be in your network, but are not a characteristic of the network.
You need to store data for X and therefore you are using a cache for Y. How will the cache affect data retrieval?;A cache improves the execution of client-side JavaScript.;A cache will continue to store data even if power is lost, improving availability.;Caches can get out of sync with the system of truth.;Using a cache will reduce latency, since retrieving from a cache is faster than retrieving from SSDs or HDDs.;;D;"Caches use memory, and that makes them the fastest storage type for reading data, so option D is right. Caches are data stores on the backend of distributed systems, not the clients. A cache would have no effect on client-side JavaScript execution. Caches do not store data in a cache if power is lost; the data would have to be reloaded. Caches can get out of sync with the system of truth because the system of truth could be updated, but the cache may not be updated. Caches have faster read times than SSDs and HDDs."
Why can cloud providers offer elastic resource allocation?;Cloud providers can take resources from lower-priority customers and give them to higher-priority customers.;Extensive resources and the ability to quickly shift resources between customers enables public cloud providers to offer elastic resource allocation more efficiently than can be done in smaller data centers.;They charge more the more resources you use.;They don’t.;;B;"Option B is correct; cloud providers have large capacity and can quickly allocate those resources to different customers. With a mix of customers and workloads, they can optimize the allocation of resources. Option A is incorrect; cloud providers do not take resources from one customer to give them to another, with the exception of preemptible instances. Option C is incorrect; cloud providers usually offer discounts for increased use."
What is not a characteristic of specialized services in Google Cloud Platform?;"They are serverless; you do not need to configure servers or clusters.";They provide a specific function, such as translating text or analyzing images.;They require monitoring by the user.;They provide an API to access the functionality of the service.;;C;"Specialized services are monitored by Google so users do not have to monitor them; therefore, option C is correct. Specialized services provide a specific compute functionality but do not require the user to configure any resources. They also provide APIs."
Your client’s transactions must access a drive attached to a VM that allows for random access to parts of files. What kind of storage does the attached drive provide?;Object storage;Block storage;NoSQL storage;Only SSD storage;;B;Attached drives are block storage devices. Cloud Storage is the object storage service and does not attach directly to a VM. NoSQL is a type of database, not a storage system. Attached drives may be either SSDs or hard drives.
You are deploying a new relational database to support a web application. Which type of storage system would you use to store data files of the database?;Object storage;Data storage;Block storage;Cache;;C;Databases require persistent storage on block devices. Object storage does not provide data block or file system storage, making option C the correct answer. Data storage is not a type of storage system. Caches are often used with databases to improve read performance, but they are volatile and are not suitable for persistently storing data files.
"A user prefers services that require minimal setup; why would you recommend Cloud Storage, App Engine, and Cloud Functions?";They are charged only by time.;They are serverless.;They require a user to configure VMs;They can only run applications written in Go.;;B;"All three services are serverless, so the user does not need to configure VMs; therefore, option B is correct. Cloud Storage is charged based on time and size of data stored. App Engine Standard and Cloud Functions are not restricted to just the Go language."
You are planning to deploy a SaaS application for customers in North America, Europe, and Asia. To maintain scalability, you will need to distribute workload across servers in multiple regions. Which GCP service would you use to implement the workload distribution?;Cloud DNS;Cloud Spanner;Cloud Load Balancing;Cloud CDN;;C;Cloud Load Balancing distributes workloads within and across regions, provides health checks, and implements autoscaling. Cloud DNS provides domain name services, such as translating a URL like www.example.com to an IP address. Cloud Spanner is a distributed relational database but does not implement workload distribution. Cloud CDN distributes content across regions to reduce latency when delivering content to users across the globe.
You have decided to deploy a set of microservices using containers. You could install and manage Docker on Compute Engine instances, but you’d rather have GCP provide some container management services. Which two GCP services allow you to run containers in a managed service?;App Engine standard environment and App Engine flexible environment;Kubernetes Engine and App Engine standard environment;Kubernetes Engine and App Engine flexible environment;App Engine standard environment and Cloud Functions;;C;App Engine flexible environments allow you to run containers on the App Engine PaaS. Kubernetes Engine is an orchestration platform for running containers. Both provide container management services. The App Engine standard environment runs applications in language-specific sandboxes and is not a general container management system. Cloud Functions is a serverless service for running code in response to events. It does not provide container services.
Why would an API developer want to use the Apigee API platform?;To get the benefits of routing and rate-limiting;Authentication services;Version control of code;A and B;All of the above;D;"Options A and B are both correct answers. The Apigee API platform provides policybased rate-limiting and routing services to help accommodate spikes in traffic. It also provides OAuth 2.0 and SAML authentication. It does not provide version control; Cloud Source Repositories is the service user for version control."
You are deploying an API to the public Internet and are concerned that your service will be subject to DDoS attacks. Which GCP service should you consider to protect your API?;Cloud Armor;Cloud CDN;Cloud IAM;VPCs;;A;Cloud Armor builds on GCP’s load balancing services to provide the ability to allow or restrict access based on IP address, deploy rules to counter cross-site scripting attacks, and provide countermeasures to SQL injection attacks. Cloud CDN is a content distribution service, not a security service. Identity Access Management is a security service, but it is for authentication and authorization, not denial-of-service mitigation. Virtual private clouds are used to restrict network access to an organization’s resources, but it does not have features to mitigate denial-of-service attacks. Also, Cloud CDN acts as a first line of defense in the case of DDoS attacks.
You have an application that uses a Pub/Sub message queue to maintain a list of tasks that are to be processed by another application. The application that consumes messages from the Pub/Sub queue removes the message only after completing the task. It takes approximately 10 seconds to complete a task. It is not a problem if two or more VMs perform the same task. What is a cost-effective configuration for processing this workload?;Use preemptible VMs;Use standard VMs;Use DataProc;Use Spanner;;A;This is a good use case for preemptible VMs because they could reduce the cost of running the second application without the risk of losing work. Since tasks are deleted from the queue only after they are completed if a preemptible VM is shut down before completing the task, another VM can perform the task. Also, there is no harm in running a task more than once, so if two VMs do the same task, it will not adversely affect the output of the application. DataProc and Spanner are not appropriate products for this task.
Your department is deploying an application that has a database backend. You are concerned about the read load on the database server and want to have data available in memory to reduce the time to respond to queries and to reduce the load on the database server. Which GCP service would you use to keep data in memory?;Cloud SQL;Cloud Memorystore;Cloud Spanner;Cloud Datastore;;B;Cloud Memorystore is the only GCP designed to cache data in memory. Cloud SQL is a relational database service and might be a good option for the backend database. Cloud Spanner is a global relational database and is a good option when you need a globally consistent database. Cloud Datastore is a document database suitable for product catalogs, user profiles, and other semistructured data.
The Cloud SDK can be used to configure and manage resources in which of the following services?;Compute Engine;Cloud Storage;Network firewalls;All of the above;;D;All three of the services listed, Compute Engine, Cloud Storage, and network firewalls, can be managed and configured using Cloud SDK.
What server configuration is required to use Cloud Functions?;VM configuration;Cluster configuration;Pub/Sub configuration;None;;D;Cloud Functions is a serverless product, no configuration is required.
You have been assigned the task of consolidating log data generated by each instance of an application. Which of the Stackdriver management tools would you use?;Monitoring;Trace;Debugger;Logging;;D;The Stackdriver Logging product is used to consolidate and manage logs generated by applications and servers.
Which specialized services are most likely to be used to build a data warehousing platform that requires complex extraction, transformation, and loading operations on batch data as well as processing streaming data?;Apigee API platform;Data analytics;AI and machine learning;Cloud SDK;;B;The data analytics set of specialized services includes products that help with extraction, transformation, and loading (ETL) and work with both batch and streaming data. The Apigee API platform is used for managing APIs and does not meet the needs described. AI and machine learning might be useful for analyzing data in the data warehouse, but the services in that set are not always helpful for ETL operations. Cloud SDK is used to control services but by itself is not directly able to perform the operations needed.
Your company has deployed 100,000 Internet of Things (IoT) sensors to collect data on the state of equipment in several factories. Each sensor will collect and send data to a data store every 5 seconds. Sensors will run continuously. Daily reports will produce data on the maximum, minimum, and average value for each metric collected on each sensor. There is no need to support transactions in this application. Which database product would you recommend?;Cloud Spanner;Cloud Bigtable;Cloud SQL MySQL;Cloud SQL PostgreSQL;;B;Bigtable is designed to accept billions of rows of data. Collecting data from 100,000 sensors every 5 seconds will generate 6,000,000 data points every minute, or 8,640,000,000 data points per day. Spanner is a relational database and supports transactions, but they are not needed. Cloud SQL MySQL and Cloud SQL PostgreSQL would be difficult to scale to this level of read and write performance.
You are the lead developer on a medical application that uses patients’ smartphones to capture biometric data. The app is required to collect data and store it on the smartphone when data cannot be reliably transmitted to the backend application. You want to minimize the amount of development you have to do to keep data synchronized between smartphones and backend data stores. Which data store option should you recommend?;Cloud Firestore;Cloud Spanner;Cloud Datastore;Cloud SQL;;A;Cloud Firestore is a mobile database service that can synchronize data between mobile devices and centralized storage. Spanner is a global relational database for large-scale applications that require transaction support in highly scaled databases. Datastore and Cloud SQL could be used but would require more custom development to synchronize data between mobile devices and the centralized data store.
A software engineer comes to you for a recommendation. She has implemented a machine learning algorithm to identify cancerous cells in medical images. The algorithm is computationally intensive, makes many mathematical calculations, requires immediate access to large amounts of data, and cannot be easily distributed over multiple servers. What kind of Compute Engine configuration would you recommend?;High memory, high CPU;High memory, high CPU, GPU;Mid-level memory, high CPU;High CPU, GPU;;B;A computationally intensive application obviously requires high CPUs, but the fact that there are many mathematical calculations indicates that a GPU should be used. You might consider running this in a cluster, but the work is not easily distributed over multiple servers, so you will need to have a single server capable of handling the load. Immediate access to large amounts of data indicates that a high-memory machine should be recommended.
You are tasked with mapping the authentication and authorization policies of your on-premises applications to GPC’s authentication and authorization mechanisms. The GCP documentation states that an identity must be authenticated in order to grant privileges to that identity. What does the term identity refer to?;VM ID;User;Role;Set of privileges;;B;Identities are abstractions of users. They can also represent characteristics of processes that run on behalf of a human user or a VM in the GCP. Identities are not related to VM IDs. Roles are collections of privileges that can be granted to identities. Option D is synonymous with option C.
A client is developing an application that will need to analyze large volumes of text information. The client is not expert in text mining or working with language. What GCP service would you recommend they use?;Cloud Vision;Cloud ML;Cloud Natural Language Processing;Cloud Text Miner;;C;Cloud Natural Language Processing provides functionality for analyzing text. Cloud Text Miner does not exist. Cloud ML is a general-purpose machine learning service that could be applied to text analysis but would require knowledge of language processing, which the client does not have. Cloud Vision is for image processing.
Data scientists in your company want to use a machine learning library available only in Apache Spark. They want to minimize the amount of administration and DevOps work. How would you recommend they proceed?;Use Cloud Spark;Use Cloud Dataproc;Use Bigquery;Install Apache Spark on a cluster of VMs;;B;Both options B and D would meet the need of running Spark, which would give the data scientists access to the machine library they need. However, option D requires that they manage and monitor the cluster of servers, which would require more DevOps and administration work than if they used the Dataproc service. Option C, BigQuery, is a scalable database, not a platform for running Spark. Cloud Spark is a fictitious product and does not exist in the GCP.
Database designers at your company are debating the best way to move a database to GCP. The database supports an application with a global user base. Users expect support for transactions and the ability to query data using commonly used query tools. The database designers decide that any database service they choose will need to support ANSI 2011 and global transactions. Which database service would you recommend?;Cloud SQL;Cloud Spanner;Cloud Datastore;Cloud Bigtable;;B;Option B is correct. Spanner supports ANSI 2011 standard SQL and global transactions. Cloud SQL supports standard SQL but does not have global transaction. Datastore and Bigtable are NoSQL databases.
Which specialized service supports both batch and stream processing workflows?;Cloud Dataproc;Bigquery;Cloud Datastore;AutoML;;A;Dataproc is designed to execute workflows in both batch and streaming modes, which makes option A correct. BigQuery is a data warehouse service. Datastore is a document database. AutoML is a machine learning service.
You have a Python application you’d like to run in a scalable environment with the least amount of management overhead. Which GCP product would you select?;App Engine flexible environment;Cloud Engine;App Engine standard environment;Kubernetes Engine;;C;App Engine standard environment provides a serverless Python sandbox that scales automatically, so option C is correct. App Engine flexible environment runs containers and requires more configuration. Cloud Engine and Kubernetes Engine both require significant management and monitoring.
A product manager at your company reports that customers are complaining about the reliability of one of your applications. The application is crashing periodically, but developers have not found a common pattern that triggers the crashes. They are concerned that they do not have good insight into the behavior of the application and want to perform a detailed review of all crash data. Which Stackdriver tool would you use to view consolidated crash information?;DataProc;Monitoring;Logging;Error Reporting;;D;"Error reporting consolidates crash information, which makes Error Reporting the right answer. Monitoring collects metrics on application and server performance. Logging is a log management service. Dataproc is not part of Stackdriver; it is a managed Hadoop and Spark service."
You are designing cloud applications for a healthcare provider. The records management application will manage medical information for patients. Access to this data is limited to a small number of employees. The billing department application will have insurance and payment information. Another group of employees will have access billing information. In addition, the billing system will have two components: a private insurance billing system and a government payer billing system. Government regulations require that software used to bill the government must be isolated from other software systems. Which of the following resource hierarchies would meet these requirements and provide the most flexibility to adapt to changing requirements?;One organization, with folders for records management and billing. The billing folder would have private insurer and government payer folders within it. Common constraints would be specified in organization-level policies. Other policies would be defined at the appropriate folder.;One folder for records management, one for billing, and no organization. Policies defined at the folder level.;One organization, with folders for records management, private insurer, and government payer below the organization. All constraints would be specified in organizationlevel policies. All folders would have the same policy constraints.;None of the above.;;A;Option A, the correct answer, separates the two main applications into their own folders and further allows separating private insurance from government payer, but using folders for each. This satisfies the regulatory need to keep the government payer software isolated from other software. Option B does not include an organization, which is the root of the resource hierarchy. Option C is not flexible with regard to differences in constraints on different applications. Option D is false because option A does meet the requirements.
When you create a hierarchy, you can have more than one of which structure?;Organization only;Folder only;Folder and project;Project only;;C;Resource hierarchies have a single organization at the root, which makes option C correct. Below that, there are folders that can contain other folders or projects. Folders can contain multiple folders and multiple projects.
You are designing an application that uses a series of services to transform data from its original form into a format suitable for use in a data warehouse. Your transformation application will write to the message queue as it processes each input file. You don’t want to give users permission to write to the message queue. You could allow the application to write to the message queue by using which of the following?;Billing account;Service account;Messaging account;Folder;;B;Service accounts are designed to give applications or VMs permission to perform tasks. Billing accounts are for associating charges with a payment method. Folders are part of resource hierarchies and have nothing to do with enabling an application to perform a task. Messaging accounts are a fictitious option.
Your company has a number of policies that need to be enforced for all projects. You decide to apply policies to the resource hierarchy. Not long after you apply the policies, an engineer finds that an application that had worked prior to implementing policies is no longer working. The engineer would like you to create an exception for the application. How can you override a policy inherited from another entity in the resource hierarchy?;Inherited policies can be overridden by defining a policy at a folder or project level.;;Inherited policies cannot be overridden;Policies can be overridden by linking them to service accounts.;;B;Inherited policies can be overridden by defining a policy at a folder or project level. Service accounts and billing accounts are not part of the resource hierarchy and are not involved in overriding policies.
Constraints are used in resource hierarchy policies. Which of the following are types of constraints allowed?;Allow a specific set of values;Deny a specific set of values;Deny a value and all its child values;Allow all allowed values;All of the above;E;All of the listed types of constraints are supported in policies.
A team with four members needs you to set up a project that needs only general permissions for all resources. You are granting each person a primitive role for different levels of access, depending on their responsibilities in the project. Which of the following are not included as primitive roles in Google Cloud Platform?;Owner;Publisher;Editor;Viewer;;B;Option B is the correct answer because Publisher is not a primitive role. Owner, Editor, and Viewer are the three primitive privileges in GCP.
You are deploying a new custom application and want to delegate some administration tasks to DevOps engineers. They do not need all the privileges of a full application administrator, but they do need a subset of those privileges. What kind of role should you use to grant those privileges?;Primitive;Predefined;Advanced;Custom;;D;Primitive roles only include the Owner, Editor, and View permissions. Predefined roles are designed for GCP products and services, like App Engine and BigQuery. For a custom application, you can create sets of privileges that give the user with that role as much permission as needed but not more.
An app for a finance company needs access to a database and a Cloud Storage bucket. There is no predefined role that grants all the needed permissions without granting some permissions that are not needed. You decide to create a custom role. When defining custom roles, you should follow which of the following principles?;Rotation of duties;Least principle;Defense in depth;Least privilege;;D;Users should have only the privileges that are needed to carry out their duties. This is the principle of least privilege. Rotation of duties is another security principle related to having different people perform a task at a different times. Defense in depth is the practice of using multiple security controls to protect the same asset. Option B is not a real security principal.
How many organizations can you create in a resource hierarchy?;1;2;3;Unlimited;;A;A resource hierarchy has only one organization, which makes option A correct. You can, however, create multiple folders and projects within a resource hierarchy.
You are contacted by the finance department of your company for advice on how to automate payments for GCP services. What kind of account would you recommend setting up?;Service account;Billing account;Resource account;Credit account;;B;In option B, the correct answer, the billing account is used to specify payment information and should be used to set up automatic payments. Service accounts are used to grant privileges to a VM and are not related to billing and payments. Resource accounts and credit accounts do not exist.
You are experimenting with GCP for your company. You do not have permission to incur costs. How can you experiment with GCP without incurring charges?;"You can’t; all services incur charges.";You can use a personal credit card to pay for charges.;You can use only free services in GCP.;You can use only serverless products, which are free to use.;;C;GCP offers a free service level for many products, which makes option C the correct answer. You can use these services without having to set up a billing account. Google charges for serverless products, such as Cloud Functions and App Engine, when customers exceed the amount allowed under the free tier.
Your DevOps team has decided to use Stackdriver monitoring and logging. You have been asked to set up Stackdriver workspaces. When you set up a Stackdriver workspace, what kind of resource is it associated with?;A Compute Engine instance only;A Compute Engine instance or Kubernetes Engine cluster only;A Compute Engine instance, Kubernetes Engine cluster, or App Engine app;A project;;D;Stackdriver Workspaces are linked to projects, not individual resources like VM instances, clusters, or App Engine apps, so option D is correct. Options A, B, and C all incorrectly indicate that Workspaces are associated with individual compute resources.
A large enterprise is planning to use GCP across a number of subdivisions. Each subdivision is managed independently and has its own budget. Most subdivisions plan to spend tens of thousands of dollars per month. How would you recommend they set up their billing account(s)?;Use a single self-service billing account.;Use multiple self-service billing accounts.;Use a single invoiced billing account.;Use multiple invoiced billing accounts.;;D;Large enterprises should use invoicing when incurring large charges, which makes option D the right answer. A self-service account is appropriate only for amounts that are within the credit limits of credit cards. Since the subdivisions are independently managed and have their own budgets, each should have its own billing accounts.
An application administrator is responsible for managing all resources in a project. She wants to delegate responsibility for several service accounts to another administrator. If additional service accounts are created, the other administrator should manage those as well. What is the best way to delegate privileges needed to manage the service accounts?;Grant iam.serviceAccountUser to the administrator at the project level.;Grant iam.serviceAccountUser to the administrator at the service account level.;Grant iam.serviceProjectAccountUser to the administrator at the project level.;Grant iam.serviceProjectAccountUser to the administrator at the service account level.;;A;When a user is granted iam.serviceAccountUser at the project level, that user can manage all service accounts in the project, so option A is correct. If a new service account is created, they will automatically have privilege to manage that service account. You could grant iam.serviceAccountUser to the administrator at the service account level, but that would require setting the role for all service accounts. If a new service account is created, the application administrator would have to grant iam.serviceAccountUser to the other administrator on the new service account. iam.serviceProjectAccountUser is a fictional role.
You work for a retailer with a large number of brick and mortar stores. Every night the stores upload daily sales data. You have been tasked with creating a service that verifies the uploads every night. You decide to use a service account. Your manager questions the security of your proposed solution, particularly about authenticating the service account. You explain the authentication mechanism used by service accounts. What authentication mechanism is used?;Username and password;Two-factor authentication;Encrypted keys;Biometrics;;C;When a service account is created, Google generates encrypted keys for authentication, making option C correct. Usernames and passwords are not an option for service accounts. Two-factor authentication is an authentication practice that requires two forms of authentication, such as a username password pair and a code from an authentication device. Biometrics cannot be used by services and is not an option.
What objects in GCP are sometimes treated as resources and sometimes as identities?;Billing accounts;Service accounts;Projects;Roles;;B;"Service accounts are resources that are managed by administrators, but they also function as identities that can be assigned roles, which makes option B the correct answer. Billing accounts are not related to identities. Projects are not identities; they cannot take on roles. Roles are resources but not identities. They can take on privileges, but those privileges are used only when they are attached to an identity."
You plan to develop a web application using products from the GCP that already include established roles for managing permissions such as read-only access or the ability to delete old versions. Which of the following roles offers these capabilities?;Primitive roles;Predefined roles;Custom roles;Application roles;;B;"Predefined roles are defined for a particular product, such as App Services or Compute Engine, so option B is the right answer. They bundle privileges often needed together when managing or using a service. Primitive roles are building blocks for other roles. Custom roles are created by users to meet their particular needs; Application roles is a fictitious role."
You are reviewing a new GCP account created for use by the finance department. An auditor has questions about who can create projects by default. You explain who has privileges to create projects by default. Who is included?;Only project administrators;All users;Only users without the role resourcemanager.projects.create;Only billing account users;;B;By default all users in an organization can create projects, which makes option B correct. The role resourcemanager.projects.create is the role that allows users to create projects. The billing account is not associated with creating projects.
How many projects can be created in an account?;10;25;There is no limit.;Each account has a limit determined by Google.;;D;The maximum number of organizations is determined on a per-account basis by Google, so option D is the correct answer. If you need additional organizations, you can contact Google and ask for an increase in your limit.
You are planning how to grant privileges to users of your company’s GCP account. You need to document what each user will be able to do. Auditors are most concerned about a role called Organization IAM roles. You explain that users with that role can perform a number of tasks, which include all of the following except which one?;Defining the structure of the resource hierarchy;Determining what privileges a user should be assigned;Defining IAM policies over the resource hierarchy;Delegating other management roles to other users;;B;Users with the Organization IAM role are not necessarily responsible for determining what privileges should be assigned to users. That is determined based on the person’s role in the organization and the security policies established within the organization, which makes option B correct.
You are deploying a Python web application to GCP. The application uses only custom code and basic Python libraries. You expect to have sporadic use of the application for the foreseeable future and want to minimize both the cost of running the application and the DevOps overhead of managing the application. Which computing service is the best option for running the application?;Compute Engine;App Engine standard environment;App Engine flexible environment;Kubernetes Engine;;B;The App Engine standard environment can run Python applications, which can autoscale down to no instances when there is no load and thereby minimize costs. Compute Engine and the App Engine flexible environment both require more configuration management than the App Engine standard environment. Kubernetes Engine is used when a cluster of servers is needed to support large or multiple applications using the same computing resources.
Your manager is concerned about the rate at which the department is spending on cloud services. You suggest that your team use preemptible VMs for all of the following except which one?;Database server;Batch processing with no fixed time requirement to complete;High-performance computing cluster;None of the above;;A;Database servers require high availability to respond to queries from users or applications. Preemptible machines are guaranteed to shut down in at most 24 hours. A batch processing job with no fixed time requirements could use preemptible machines as long as the VM is restarted. High-performance computing clusters can use preemptible machines because work on a preemptible machine can be automatically rescheduled for another node on the cluster when a server is preempted. D is incorrect because there is a correct answer in the set of options.
What parameters need to be specified when creating a VM in Compute Engine?;Project and zone;Username and admin role;Billing account;Cloud Storage bucket;;A;VMs are created in projects, which are part of the resource hierarchy. They are also located in geographic regions and data centers, so a zone is specified as well. Usernames and admin roles are not specified during creation. The billing account is tied to a project and so does not have to be specified when the VM is created. Cloud storage buckets are created independently of VMs. Not all VMs will make use of storage buckets.
Your company has licensed a third-party software package that runs on Linux. You will run multiple instances of the software in a Docker container. Which of the following GCP services could you use to deploy this software package?;Compute Engine only;Kubernetes Engine only;Compute Engine, Kubernetes Engine, and the App Engine flexible environment only;Compute Engine, Kubernetes Engine, the App Engine flexible environment, or the App Engine standard environment;;C;Compute Engine can run Docker containers if you install Docker on the VM. Kubernetes and the App Engine flexible environment support Docker containers. The App Engine standard environment provides language-specific runtime environments and does not allow customers to specify custom Docker images for use.
You can specify packages to install into a Docker container by including commands in which file?;Docker.cfg;Dockerfile;Config.dck;install.cfg;;B;The name of the file that is used to build and configure a Docker container is Dockerfile.
How much memory of a node does Kubernetes require as overhead?;10GB to 20GB;1GB to 2GB;1.5GB;A scaled amount starting at 25 percent of memory and decreasing to 2 percent of marginal memory as the total amount of memory increases.;;D;Kubernetes uses 25 percent of memory up to 4GB and then slightly less for the next 4GB, and it continues to reduce the percentage of additional memory down to 2 percent of memory over 128GB.
Your manager is making a presentation to executives in your company advocating that you start using Kubernetes Engine. You suggest that the manager highlight all the features Kubernetes provides to reduce the workload on DevOps engineers. You describe several features, including all of the following except which one?;Load balancing across Compute Engine VMs that are deployed in a Kubernetes cluster;Security scanning for vulnerabilities;Automatic scaling of nodes in the cluster;Automatic upgrading of cluster software as needed;;B;Kubernetes provides load balancing, scaling, and automatic upgrading of software. It does not provide vulnerability scanning. GCP does have a Cloud Security Scanner product, but that is designed to work with App Engine to identify common application vulnerabilities.
Your company is about to release a new online service that builds on a new user interface experience driven by a set of services that will run on your servers. There is a separate set of services that manage authentication and authorization. A data store set of services keeps track of account information. All three sets of services must be highly reliable and scale to meet demand. Which of the GCP services is the best option for deploying this?;App Engine standard environment;Compute Engine;Cloud Functions;Kubernetes Engine;;D;The scenario described is a good fit for Kubernetes. Each of the groups of services can be structured in pods and deployed using Kubernetes deployment. Kubernetes Engine manages node health, load balancing, and scaling. App Engine Standard Edition has language-specific sandboxes and is not a good fit for this use case. Cloud Functions is designed for short-running event processing and is not the kind of continuous processing needed in this scenario. Compute Engine could meet the requirements of this use case, but it would require more effort on the part of application administrators and DevOps professionals to configure load balancers, monitor health, and manage software deployments.
A mobile application uploads images for analysis, including identifying objects in the image and extracting text that may be embedded in the image. A third party has created the mobile application, and you have developed the image analysis service. You both agree to use Cloud Storage to store images. You want to keep the two services completely decoupled, but you need a way to invoke the image analysis as soon as an image is uploaded. How should this be done?;Change the mobile app to start a VM running the image analysis service and have that VM copy the file from storage into local storage on the VM. Have the image service run on the VM.;Write a function in Python that is invoked by Cloud Functions when a new image file is written to the Cloud Storage bucket that receives new images. The function should submit the URL of the uploaded file to the image analysis service. The image analysis service will then load the image from Cloud Storage, perform analysis, and generate results, which can be saved to Cloud Storage.;Have a Kubernetes cluster running continuously, with one pod dedicated to listing the contents of the upload bucket and detecting new files in Cloud Storage and another pod dedicated to running the image analysis software.;Have a Compute Engine VM running and continuously listing the contents of the upload bucket in Cloud Storage to detect new files. Another VM should be continually running the image analysis software.;;B;This is an ideal use case for Cloud Functions. The cloud function is triggered by a file upload event. The cloud function calls the image processing service. With this setup, the two services are independent. No additional servers are required. Option A violates the requirement to keep the services independent. Options C and D incur more management overhead and will probably cost more to operate than option B.
Your team is developing a new pipeline to analyze a stream of data from sensors on manufacturing devices. The old pipeline occasionally corrupted data because parallel threads overwrote data written by other threads. You decide to use Cloud Functions as part of the pipeline. As a developer of a Cloud Function, what do you have to do to prevent multiple invocations of the function from interfering with each other?;Include a check in the code to ensure another invocation is not running at the same time.;Schedule each invocation to run in a separate process.;Schedule each invocation to run in a separate thread.;Nothing. GCP ensures that function invocations do not interfere with each other.;;D;Each invocation of a cloud function runs in a secure, isolated runtime environment. There is no need to check whether other invocations are running. With the Cloud Functions service, there is no way for a developer to control code execution at the process or thread level.
A client of yours processes personal and health information for hospitals. All health information needs to be protected according to government regulations. Your client wants to move their application to Google Cloud but wants to use the encryption library that they have used in the past. You suggest that all VMs running the application have the encryption library installed. Which kind of image would you use for that?;Custom image;Public image;CentOS 6 or 7;;;A;You would create a custom image after you installed the custom code, in this case the encryption library. A public image does not contain custom code, but it could be used as the base that you add custom code to. Both CentOS and Ubuntu are Linux distributions. You could use either as the base image that you add custom code to, but on their own, they do not have custom code.
What is the lowest level of the resource hierarchy?;Folder;Project;File;VM instance;;B;Projects are the lowest level of the resource hierarchy. The organization is at the top of the hierarchy, and folders are between the organization and projects. VM instances are not part of the resource hierarchy.
Your company is seeing a marked increase in the rate of customer growth in Europe. Latency is becoming an issue because your application is running in us-central1. You suggest deploying your services to a region in Europe. You have several choices. You should consider all of the following factors except which one?;Cost;Latency;Regulations;Reliability;;D;All Google regions have the same level of service level agreement, so reliability is the same. Costs may differ between regions. Regulations may require that data stay within a geographic area, such as the European Union. Latency is a consideration when you want a region that is close to end users or data you will need is already stored in a particular region.
What role gives users full control over Compute Engine instances?;Compute Manager role;Compute Admin role;Compute Manager role;Compute Security Admin;;B;Compute Engine Admin Role is the role that gives users complete control over instances. Options A and C are fictitious roles. Compute Engine Security Admin gives users the privileges to create, modify, and delete SSL certificates and firewall rules.
Which of the following are limitations of a preemptible VM?;Will be terminated within 24 hours.;May not always be available. Availability may vary across zones and regions.;Cannot migrate to a regular VM.;All of the above;;D;Preemptible VMs will be terminated after 24 hours. Google does not guarantee that preemptible VMs will be available. Once an instance is started as a preemptible machine, it cannot migrate to a regular VM. You could, however, save a snapshot and use that to create a new regular instance.
Custom VMs can have up to how many vCPUs?;16;32;64;128;;C;Custom VMs can have up to 64 CPUs and up to 6.5GB of memory per vCPU.
When using the App Engine standard environment, which of the following language’s runtime is not supported?;Java;Python;C;Go;;C;The C programming language is not supported in the App Engine standard environment. If you need to run a C application, it can be compiled and run in a container running in the App Engine flexible environment.
Kubernetes reserves CPU resources in percentage of cores available. The percentage is what range?;1 percent to 10 percent;0.25 percent to 6 percent;0.25 percent to 2 percent;10 percent to 12 percent;;B;Kubernetes reserves CPU capacity according to the following schedule: 1. 6 percent of the first core 2. 1 percent of the next core (up to two cores) 3. 0.5 percent of the next two cores (up to four cores) 4. 0.25 percent of any cores above four cores
Kubernetes deployments can be in what states?;Progressing, stalled, completed;Progressing, completed, failed;Progressing, stalled, failed, completed;Progressing, stalled, running, failed, completed;;B;The only states a Kubernetes deployment can be in are progressing, completed, and failed.
A client has brought you in to help reduce their DevOps overhead. Engineers are spending too much time patching servers and optimizing server utilization. They want to move to serverless platforms as much as possible. Your client has heard of Cloud Functions and wants to use them as much as possible. You recommend all of the following types of applications except which one?;Long-running data warehouse data load procedures;IoT backend processing;Mobile application event processing;Asynchronous workflows;;A;Cloud Functions is best suited for event-driven processing, such as a file being uploaded to Cloud Storage or an event being writing to a Pub/Sub queue. Long-running jobs, such as loading data into a data warehouse, are better suited to Compute Engine or App Engine.
You have just opened the GCP console at console.google.com. You have authenticated with the user you want to use. What is one of the first things you should do before performing tasks on VMs?;Open Cloud Shell.;Verify you can SSH into a VM.;Verify that the selected project is the one you want to work with.;Review the list of running VMs.;;C;You should verify the project selected because all operations you perform will apply to resources in the selected project, making option C the correct answer. You do not need to open Cloud Shell unless you want to work with the command line, and if you did, you should verify that the project is correctly selected first. Logging into a VM using SSH is one of the tasks that requires you to be working with the correct project, so logging in via SSH should not happen before verifying the project. The list of VMs in the VM Instance window is a list of VMs in the current project. You should verify which project you are using to ensure you are viewing the set of VMs you think you are using.
What is a one-time task you will need to complete before using the console?;Set up billing;Create a project;Create a storage bucket;Specify a default zone;;A;"You will need to set up billing if it is not already enabled when you start using the console, so option A is the right answer. You may create a project, but you will be able to do this only if billing is enabled. You do not need to create a storage bucket to work with the console. Specifying a default zone is not a one-time task; you may change zones throughout the life of your project."
A colleague has asked for your assistance setting up a test environment in Google Cloud. They have never worked in GCP. You suggest starting with a single VM. Which of the following is the minimal set of information you will need?;A name for the VM and a machine type;A name for the VM, a machine type, a region, and a zone;A name for the VM, a machine type, a region, a zone, and a CIDR block;A name for the VM, a machine type, a region, a zone, and an IP address;;B;The name of the VM, the region and zone, and the machine type can all be specified in the console along with other parameters, so option B is correct. Option A is missing required parameters. A CIDR block is a range of IP addresses that is associated with a subnet and not needed to create a VM. An IP address is assigned automatically so it is not required.
An architect has suggested a particular machine type for your workload. You are in the console creating a VM and you don’t see the machine type in the list of available machine types. What could be the reason for this?;You have selected the incorrect subnet.;That machine type is not available in the zone you specified.;You have chosen an incompatible operating system.;You have not specified a correct memory configuration.;;B;"Different zones may have different machine types available, so you will need to specify a region first and then a zone to determine the set of machine types available. If the machine type does not appear in the list, it is not available in that zone. This makes option B the correct answer. Options A and C are incorrect. Subnets and IP addresses are not related to the machine types available. Unless you are specifying a custom machine type, you do not specify the amount of memory; that is defined by the machine type, so option D is incorrect."
Your manager asks for your help with understanding cloud computing costs. Your team runs dozens of VMs for three different applications. Two of the applications are for use by the marketing department and one is use by the finance department. Your manager wants a way to bill each department for the cost of the VMs used for their applications. What would you suggest to help solve this problem?;Access controls;Persistent disks;Labels and descriptions;Descriptions only;;C;"Labels and descriptions are for helping us track our own attributes of resources; GCP does not need them to perform its tasks. As the number of servers grows, it can become difficult to track which VMs are used for which applications and services, so option C is the correct answer. Labels and a general description will help administrators track numbers of VMs and their related costs. Options A and B are used for security and storage but do not help with managing multiple VMs. Option D is only partially correct. Descriptions are helpful but so are labels."
If you wanted to set the preemptible property using Cloud Console, in which section of the Create An Instance page would you find the option?;Availability Policy;Identity And API Access;Sole Tenancy;Networking;;A;The Availability Policy section within the Management tab is where you set preemptibility, so option A is correct. Identity And API Access is used to control the VM’s access to Google Cloud APIs and which service account is used with the VM. Sole Tenancy is used if you need to run your VMs on physical servers that only run your VMs. Networking is used to set network tags and change the network interface.
You need to set up a server with a high level of security. You want to be prepared in case of attacks on your server by someone trying to inject a rootkit (a kind of malware that can alter the operating system). Which option should you select when creating a VM?;Firewall;Shield VM;Project-wide SSH keys;Boot disk integrity check;;B;Shield VM is an advanced set of security controls that includes Integrity Monitoring, a check to ensure boot images have not been tampered with, which makes option B the right answer. Firewalls are used to control ingress and egress of network traffic to a server or subnet. Project-wide SSH keys are used for authenticating users across servers within a project. Boot disk integrity check is a fictional feature.
All of the following parameters can be set when adding an additional disk through Google Cloud Console, except one. Which one?;Disk type;Encryption key management;Block size;Source image for the disk;;C;Block size is not an option in the Additional Disks dialog, so option C is correct. Encryption key management, disk type, and the option of specifying a source image are all available options.
You lead a team of cloud engineers who maintain cloud resources for several departments in your company. You’ve noticed a problem with configuration drift. Some machine configurations are no longer in the same state as they were when created. You can’t find notes or documentation on how the changes were made or why. What practice would you implement to solve this problem?;Have all cloud engineers use only command-line interface in Cloud Shell.;Write scripts using gcloud commands to change configuration and store those scripts in a version control system.;Take notes when making changes to configuration and store them in Google Drive.;Limit privileges so only you can make changes so you will always know when and why configurations were changed.;;B;Using version-controlled scripts is the best approach of the four options. Scripts can be documented with reasons for the changes and they can be run repeatedly on different machines to implement the same change. This reduces the chance of error when manually entering a command. Option A does not help to improve documenting why changes were made. Option C could help improve documentation, but executable scripts are precise and accurate reflections of what was executed. Notes may miss details. Option D is not advisable. You could become a bottleneck to making changes, changes cannot be made when you are unavailable, and your memory may not be a reliable way to track all configuration changes.
When using the Cloud SDK command-line interface, which of the following is part of commands for administering resources in Compute Engine?;gcloud compute instances;gcloud instances;gcloud instances compute;None of the above;;A;gcloud compute instances is the start of commands for administering Compute Engine resources, making option A the right answer. Option B, gcloud instances, is missing the compute keyword that indicates we are working with Compute Engine. Option C has switched the order of compute and instances. Option D is false because option A is the correct answer.
A newly hired cloud engineer is trying to understand what VMs are running in a particular project. How could the engineer get summary information on each VM running in a project?;Execute the command gcloud compute list;Execute the command gcloud compute instances list;Execute the command gcloud instances list;Execute the command gcloud list instances;;B;Option B follows the pattern of the glcoud command, which is hierarchical and starts with the glcoud name of the service, in this case compute for Compute Engine, followed by the next level down, which in this case is instances. Finally, there is the action or verb, in this case list. Option A is missing the term instances to indicate you are working with VM instances. Option C is missing the compute keyword to indicate you are working with Compute Engine. Option D is missing the compute instance keyword and has switched the order of instances and list.
When creating a VM using the command line, how should you specify labels for the VM?;Use the --labels option with labels in the format of KEYS:VALUES.;Use the --labels option with labels in the format of KEYS=VALUE.;Use the --labels option with labels in the format of KEYS,VALUES.;This is not possible in the command line.;;B;The correct format is to use the --labels parameter and specify the key followed by an equal sign followed by the value in option B. Options A and C have the wrong character separating the key and value. Option D is incorrect because it is possible to specify labels in the command line.
In the boot disk advanced configuration, which operations can you specify when creating a new VM?;Add a new disk, reformat an existing disk, attach an existing disk;Add a new disk and reformat an existing disk;Add a new disk and attach an existing disk;Reformat an existing disk and attach an existing disk;;C;The two operations you can specify when using the book disk configuration are adding a new disk and attaching an existing disk, so option C is correct. Reformatting an existing disk is not an option, so options A, B, and D cannot be the correct answer.
You have acquired a 10 GB data set from a third-party research firm. A group of data scientists would like to access this data from their statistics programs written in R. R works well with Linux and Windows file systems, and the data scientists are familiar with file operations in R. The data scientists would each like to have their own dedicated VM with the data available in the VM’s file system. What is a way to make this data readily available on a VM and minimize the steps the data scientists will have to take?;Store the data in Cloud Storage.;Create VMs using a source image created from a disk with the data on it.;Store the data in Google Drive.;Load the data into BigQuery.;;B;10 GB of data is small enough to store on a single disk. By creating an image of a disk with the data stored on it, you can specify that source image when creating a VM. Option A would require the data scientist to copy the data from Cloud Storage to a disk on the VM. Option C would similarly require copying the data. Option D would load data into a database, not a file system as specified in the requirements.
The Network tab of the create VM form is where you would perform which of the following operations?;Set the IP address of the VM;Add a network interface to the VM;Specify a default router;Change firewall configuration rules;;B;In the Network tab of the VM form, you can add another network interface, so option B is correct. GCP sets the IP address, so option A is incorrect. There is no option to specify a router or change firewall rules on the Network tab, so options C and D are incorrect.
You want to create a VM using the gcloud command. What parameter would you include to specify the type of boot disk?;boot-disk-type;boot-disk;disk-type;type-boot-disk;;A;The correct option is boot-disk-type, which is option A. The other three options are not parameters to the gcloud compute instances command.
Which of the following commands will create a VM with four CPUs that is named webserver-1?;gcloud compute instances create --machine-type=n1-standard-4 web-server-1;gcloud compute instances create --cpus=4 web-server-1;gcloud compute instances create --machine-type=n1-standard-4 –instancename web-server-1;gcloud compute instances create --machine-type=n1-4-cpu web-server-1;;A;Option A is the correct command. It is the only option that includes a correct machine type and properly specifies the name of the instance. Option B uses the --cpus parameter, which does not exist. Option C uses the parameter instance-name, which does not exist. The instance name is passed as an argument and does not need a parameter name. Option D is incorrect because machine type n1-4-cpu is not a valid machine type.
Which of the following commands will stop a VM named web-server-1?;gcloud compute instances halt web-server-1;gcloud compute instances --terminate web-server1;gcloud compute instances stop web-server-1;gcloud compute stop web-server-1;;C;Option C is the correct command, which is gcloud compute instances, to indicate you are working with VMs, followed by the stop command and the name of the VM. Option A is incorrect because halt is not an option. Option B is incorrect because –terminate is not a parameter. Option D is missing the word instances, which indicates you are working with VMs.
You have just created an Ubuntu VM and want to log into the VM to install some software packages. Which network service would you use to access the VM?;FTP;SSH;RDP;ipconfig;;B;SSH is service for connecting to a remote server and logging into a terminal window. Once logged in, you would have access to a command line, so option B is the right answer. FTP is a file transfer protocol and does not allow you to log in and perform system administration tasks. RDP is a protocol used to remotely access Windows servers, not Ubuntu, which is a Linux distribution. ipconfig is a command-line utility for configuring IP stacks on a device and does not allow you to log into a remote server.
Your management team is considering three different cloud providers. You have been asked to summarize billing and cost information to help the management team compare cost structures between clouds. Which of the following would you mention about the cost of VMs in GCP?;VMs are billed in 1-second increments, cost varies with the number of CPUs and amount of memory in a machine type, you can create custom machine types, preemptible VMs cost up to 80 percent less than standard VMs, and Google offers discounts for sustained usage.;VMs are billed in 1-second increments and VMs can run up to 24 hours before they will be be shut down.;Google offers discounts for sustained usage in only some regions, cost varies with the number of CPUs and amount of memory in a machine type, you can create custom machine types, preemptible VMs cost up to 80 percent less than standard VMs.;VMs are charged for a minimum of 1 hour of use and cost varies with the number of CPUs and amount of memory in a machine type.;;A;All of the statements in option A are true and relevant to billing and costs. Option B is correct that VMs are billed in 1-second increments, but the only preemptible VMs are shut down within 24 hours of starting. Option C is incorrect because discounts are not limited to some regions. Option D is incorrect because VMs are not charged for a minimum of 1 hour.
Which page in Google Cloud Console would you use to create a single instance of a VM?;Compute Engine;App Engine;Kubernetes Engine;Cloud Functions;;A;The Compute Engine page is where you have the option of creating a single VM instance, so option A is the correct answer. App Engine is used for containers and running applications in language-specific runtime environments. Kubernetes Engine is used to create and manage Kubernetes clusters. Cloud Functions is where you would create a function to run in Google’s serverless cloud function environment.
You view a list of Linux VM instances in the console. All have public IP addresses assigned. You notice that the SSH option is disabled for one of the instances. Why might that be the case?;The instance is preemptible and therefore does not support SSH.;The instance is stopped.;The instance was configured with the No SSH option.;The SSH option is never disabled.;;B;Instances can be stopped, and when they are, then you cannot connect to them via SSH, which makes option B the correct answer. Starting the instance will enable SSH access. Option A is not correct because you can log into preemptible machines. Option C is incorrect because there is no No SSH option. Option D is incorrect because the SSH option can be disabled.
You have noticed unusually slow response time when issuing commands to a Linux server, and you decide to reboot the machine. Which command would you use in the console to reboot?;Reboot;Reset;Restart;Shutdown followed by Startup;;B;"The Reset command can be used to restart a VM; thus, option B is correct. The properties of the VM will not change, but data in memory will be lost. There is no Reboot, Restart, Shutdown, or Startup option in the console."
In the console, you can filter the list of VM instances by which of the following?;Labels only;Member of managed instance group only;Labels, status, or members of managed instance group;Labels and status only;;C;Labels, members of a managed instance group, and status are all available for filtering, so option C is the correct answer. You can also filter by internal IP, external IP, zone, network, deletion protection, and member of a managed or unmanaged instance group.
You will be building a number of machine learning models on an instance and attaching GPU to the instance. When you run your machine learning models they take an unusually long time to run. It appears that GPU is not being used. What could be the cause of this?;GPU libraries are not installed.;The operating system is based on Ubuntu.;You do not have at least eight CPUs in the instance.;There isn’t enough persistent disk space available.;;A;To function properly, the operating system must have GPU libraries installed, so option A is correct. The operating system does not have to be Ubuntu based, and there is no need to have at least eight CPUs in an instance before you can attach and use a GPU. Available disk space does not determine if a GPU is used or not.
When you add a GPU to an instance, you must ensure that:;The instance is set to terminate during maintenance.;The instance is preemptible.;The instance does not have nonboot disks attached.;The instance is running Ubuntu 14.02 or later.;;A;If you add a GPU to a VM, you must set the instance to terminate during maintenance, which makes option A the correct response. This is set in the Availability Policies section of the VM configuration form. The instance does not need to be preemptible and it can have non-boot disks attached. The instance is not required to run Ubuntu 14.02 or later.
You are using snapshots to save copies of a 100GB disk. You make a snapshot and then add 10GB of data. You create a second snapshot. How much storage is used in total for the two snapshots (assume no compression)?;210 GB, with 100GB for the first and 110GB for the second;110 GB, with 100GB for the first and 10GB for the second;110 GB, with 110 for the second (the first snapshot is deleted automatically);221 GB, with 100GB for the first, 110GB for the second, plus 10 percent of the second snapshot (11 GB) for metadata overhead;;B;"When you first create a snapshot, GCP will make a full copy of the data on the persistent disk. The next time you create a snapshot from that disk, GCP will only copy the data that has changed since the last snapshot. Option A is incorrect; GCP does not store a full copy of the second snapshot. Option C is incorrect; the first snapshot is not deleted automatically. Option D is incorrect, subsequent snapshots do not incur 10 percent overhead."
You have decided to delegate the task of making backup snapshots to a member of your team. What role would you need to grant to your team member to create snapshots?;Compute Image Admin;Storage Admin;Compute Snapshot Admin;Compute Storage Admin;;D;To work with snapshots, a user must be assigned the Compute Storage Admin role, which makes option D the correct answer. The other options are fictitious roles.
The source of an image may be:;Only disks;Snapshots or disks only;Disks, snapshots, or another image;Disks, snapshots, or any database export file;;C;Images can be created from four sources, namely, disks, snapshots, cloud storage files, or another image, so option C is the right answer. Database export files are not sources for images.
You have built images using Ubuntu 14.04 and now want users to start using Ubuntu 16.04. You don’t want to just delete images based on Ubuntu 14.04, but you want users to know they should start using Ubuntu 16.04. What feature of images would you use to accomplish this?;Redirection;Deprecated;Unsupported;Migration;;B;Deprecated marks the image as no longer supported and allows you to specify a replacement image to use going forward, making option B the correct answer. Deprecated images are available for use but may not be patched for security flaws or have other updates. The other options are fictitious features of images.
You want to generate a list of VMs in your inventory and have the results in JSON format. What command would you use?;gcloud compute instances list;gcloud compute instances describe;gcloud compute instances list --format json;gcloud compute instances list --output json;;C;The base command for working with instances is gcloud compute instances, which makes option C the correct answer. The list command is used to show details of all instances. By default, output is in human-readable form, not json. Using the --format json option forces the output to be in JSON format. --output is not a valid option.
You would like to understand details of how GCP starts a virtual instance. Which optional parameter would you use when starting an instance to display those details?;#NOM?;#NOM?;#NOM?;#NOM?;;B;"–-async causes information about the start process to be displayed; therefore, option B is correct. --verbose is an analogous parameter in many Linux commands. --describe provides details about an instance but not necessarily the startup process. --details is not a valid parameter."
Which command will delete an instance named ch06-instance-3?;gcloud compute instances delete instance=ch06-instance-3;gcloud compute instance stop ch06-instance-3;gcloud compute instances delete ch06-instance-3;gcloud compute delete ch06-instance-3;;C;The command to delete an instance is gcloud compute instances delete followed by the name of the instance, so option C is correct. Option A is incorrect because there is no instance parameter. Option B is incorrect because that command stops but does not delete the instance. Option D is missing instances in the command, which is required to indicate what type of entity is being deleted.
You are about to delete an instance named ch06-instance-1 but want to keep its boot disk. You do not want to keep other attached disks. What gcloud command would you use?;gcloud compute instances delete ch06-instance-1 ––keep-disks=boot;gcloud compute instances delete ch06-instance-1 ––save-disks=boot;gcloud compute instances delete ch06-instance-1 ––keep-disks=filesystem;gcloud compute delete ch06-instance-1 ––keep-disks=filesystem;;A;gcloud compute instances is the base command followed by delete, the name of the instance, and --keep-disks=boot, so option A is correct. There is no --save-disk parameter. Option C is wrong because filesystem is not a valid value for the keep-disk parameter. Option D is missing the instances option which is required in the command.
You want to view a list of fields you can use to sort a list of instances. What command would you use to see the field names?;gcloud compute instances list;gcloud compute instances describe;gcloud compute instances list --detailed;gcloud compute instances describe --detailed;;B;The correct answer is option B, which is to use the describe command. Option A will show some fields but not all. Options C and D are incorrect because there is no detailed parameter.
You are deploying an application that will need to scale and be highly available. Which of these Compute Engine components will help achieve scalability and high availability?;Preemptible instances;Instance groups;Cloud Storage;GPUs;;B;Instance groups are sets of VMs that can be configured to scale and are used with load balancers, which contribute to improving availability, so option B is correct. Preemptible instances are not highly available because they can be shut down at any time by GCP. Cloud Storage is not a Compute Engine component. GPUs can help improve throughput for math-intensive operations but do not contribute to high availability.
Before creating an instance group, you need to create what?;Instances in the instance group;Instance group template;Boot disk image;Source snapshot;;B;An instance group template is used to specify how the instance group should be created, which makes option B the correct answer. Option A is incorrect because instances are created automatically when an instance group is created. Boot disk images and snapshots do not have to be created before creating an instance group.
How would you delete an instance group template using the command line?;gcloud compute instances instance-template delete;glcoud compute instance-templates delete;gcloud compute delete instance-template;gcloud compute delete instance-templates;;B;The command to delete an instance group is gcloud compute instance-template delete, so option B is correct. Option A incorrectly includes the term instances. Option C is in incorrect order. Option D is wrong because instance-template is in the wrong position and is plural in the option.
What can be the basis for scaling up an instance group?;CPU utilization and operating system updates;Disk usage and CPU utilization only;Network latency, load balancing capacity, and CPU utilization;Disk usage and operating system updates only;;C;You can configure an autoscaling policy to trigger adding or removing instances based on CPU utilization, monitoring metric, load balancing capacity, or queue-based workloads. Disk, network latency, and memory can trigger scaling if monitoring metrics on those resources are configured. So, option C is correct.
An architect is moving a legacy application to Google Cloud and wants to minimize the changes to the existing architecture while administering the cluster as a single entity. The legacy application runs on a load-balanced cluster that runs nodes with two different configurations. The two configurations are required because of design decisions made several years ago. The load on the application is fairly consistent, so there is rarely a need to scale up or down. What GCP Compute Engine resource would you recommended using?;Preemptible instances;Unmanaged instance groups;Managed instance groups;GPUs;;B;Unmanaged instance groups are available for limited use cases such as this. Unmanaged instance groups are not recommended in general. Managed instance groups are the recommended way to use instance groups, but the two different configurations prevents their use. Preemptible instances and GPUs are not relevant to this scenario.
A new engineer is asking for clarification about when it is best to use Kubernetes and when to use instance groups. You point out that Kubernetes uses instance groups. What purpose do instance groups play in a Kubernetes cluster?;They monitor the health of instances.;They create pods and deployments.;They create sets of VMs that can be managed as a unit.;They create alerts and notification channels;;C;"Kubernetes creates instance groups as part of the process of creating a cluster, which makes option C the correct answer. Stackdriver, not instance groups, is used to monitor the health of nodes and to create alerts and notifications. Kubernetes creates pods and deployments; they are not provided by instance groups."
What kinds of instances are required to have a Kubernetes cluster?;A cluster master and nodes to execute workloads;A cluster master, nodes to execute workloads, and Stackdriver nodes to monitor node health.;"Kubernetes nodes; all instances are the same.";Instances with at least four vCPUs.;;A;"A Kubernetes cluster has a single cluster master and one or more nodes to execute workloads, so option A is the correct answer. Stackdriver is not part of the Kubernetes cluster; it is a separate GCP service. Kubernetes does not require instances with at least four vCPUs; in fact, the default node configuration uses one vCPU."
What is a pod in Kubernetes?;A set of containers;Application code deployed in a Kubernetes cluster;A single instance of a running process in a cluster;A controller that manages communication between clients and Kubernetes services;;C;Pods are single instances of a running process in a cluster, so option C is correct. Pods run containers but are not sets of containers. Application code runs in containers that are deployed in pods. Pods are not controllers, so they cannot manage communication with clients and Kubernetes services.
You have developed an application that calls a service running in a Kubernetes cluster. The service runs in pods that can be terminated if they are unhealthy and replaced with other pods that might have a different IP address. How should you code your application to ensure it functions properly in this situation?;Query Kubernetes for a list of IP addresses of pods running the service you use.;Communicate with Kubernetes services so applications do not have to be coupled to specific pods.;Query Kubernetes for a list of pods running the service you use.;Use a gcloud command to get the IP addresses needed.;;B;Services are applications that provide API endpoints that allow applications to discover pods running a particular application, making option B correct. Options A and C, if they could be coded using the API designed for managing clusters, would require more code than working with services and are subject to changes in a larger set of API functions. Option D is not an actual option.
You have noticed that an application’s performance has degraded significantly. You have recently made some configuration changes to resources in your Kubernetes cluster and suspect that those changes have alerted the number of pods running in the cluster. Where would you look for details on the number of pods that should be running?;Deployments;Stackdriver;ReplicaSet;Jobs;;C;ReplicaSets are controllers that are responsible for maintaining the correct number of pods, which makes option C the correct answer. Deployments are versions of application code running on a cluster. Stackdriver is a monitoring and logging service that monitors but does not control Kubernetes clusters. Jobs is an abstraction of workloads and is not tied to the number of pods running in a cluster.
You are deploying a high availability application in Kubernetes Engine. You want to maintain availability even if there is a major network outage in a data center. What feature of Kubernetes Engine would you employ?;Multiple instance groups;Multizone/region cluster;Regional deployments;Load balancing;;B;"Multizone/multiregion clusters are available in Kubernetes Engine and are used to provide resiliency to an application, so option B is correct. Option A refers to instance groups that are a feature of Compute Engine, not directly of Kubernetes Engine. Option C is incorrect; regional deployments is a fictitious term. Load balancing distributes load and is part of Kubernetes by default. If load is not distributed across zones or regions, it does not help to add resiliency across data centers."
You want to write a script to deploy a Kubernetes cluster with GPUs. You have deployed clusters before, but you are not sure about all the required parameters. You need to deploy this script as quickly as possible. What is one way to develop this script quickly?;Use the GPU template in the Kubernetes Engine cloud console to generate the gcloud command to create the cluster;Search the Web for a script;Review the documentation on gcloud parameters for adding GPUs;Use an existing script and add parameters for attaching GPUs;;A;Option A is the best answer. Starting with an existing template, filling in parameters, and generating the gcloud command is the most reliable way. Option D may work, but multiple parameters that are needed for your configuration may not be in the script you start with. There may be some trial and error with this option. Options B and C may lead to a solution but could take some time to complete.
What gcloud command will create a cluster named ch07-cluster-1 with four nodes?;gcloud beta container clusters create ch07-cluster-1 --num-nodes=4;gcloud container beta clusters create ch07-cluster-1 --num-nodes=4;gcloud container clusters create ch07-cluster-1 --num-nodes=4;gcloud beta container clusters create ch07-cluster-1 4;;A;The correct command is option A. Option B has beta in the wrong position. Option C is missing beta. Option D is missing the --num-nodes parameter name.
When using Create Deployment from Cloud Console, which of the following cannot be specified for a deployment?;Container image;Application name;Time to live (TTL);Initial command;;C;Time to Live is not an attribute of deployments, so option C is the correct answer. Application name, container image, and initial command can all be specified.
Deployment configuration files created in Cloud Console use what type of file format?;CSV;YAML;TSV;JSON;;B;Deployment configuration files created in Cloud Console are saved in YAML format. CSV, TSV, and JSON are not used.
What command is used to run a Docker image on a cluster?;gcloud container run;gcloud beta container run;kubectl run;kubectl beta run;;C;The kubectl command is used to control workloads on a Kubernetes cluster once it is created, so option C is correct. Options A and B are incorrect because gcloud is not used to manipulate Kubernetes processes. Option D is wrong because beta is not required in kubectl commands.
What command would you use to have 10 replicas of a deployment named ch07-app-deploy?;kubectl upgrade deployment ch07-app-deploy --replicas=5;gcloud containers deployment ch07-app-deploy --replicas=5;kubectl scale deployment ch07-app-deploy --replicas=10;kubectl scale deployment ch07-app-deploy --pods=5;;C;Option C is the correct command. Option A uses the term upgrade instead of scale. Option B incorrectly uses gcloud. Option D uses the incorrect parameter pods.
Stackdriver is used for what operations on Kubernetes clusters?;Notifications only;Monitoring and notifications only;Logging only;Notifications, monitoring, and logging;;D;Stackdriver is a comprehensive monitoring, logging, alerting, and notification service that can be used to monitor Kubernetes clusters.
Before monitoring a Kubernetes cluster, what must you create with Stackdriver?;Log;Workspace;Pod;ReplicaSet;;B;Workspaces are logical structures for storing information about resources in a project that are being monitored, so option B is correct. Stackdriver works with logs, but a log is not required before starting to use Stackdriver. Pods and ReplicaSets are part of Kubernetes, not Stackdriver.
What kind of information is provided in the Details page about an instance in Stackdriver?;CPU usage only;Network traffic only;Disk I/O, CPU usage, and network traffic; CPU usage and disk I/O;;C;The Stackdriver Instance Detail page includes time-series charts on CPU usage, network traffic, and disk I/O.
When creating an alerting policy, what can be specified?;Conditions, notifications, and time to live;Conditions, notifications, and documentation;Conditions only;Conditions, documentation, and time to live;;B;When creating an alert policy, you can specify conditions, notifications, and documentation, making option B the correct answer. Options A and D are incorrect because there is no Time to Live attribute on policies. Option C is wrong because it does not include notifications and documentation.
Your development team needs to be notified if there is a problem with applications running on several Kubernetes clusters. Different team members prefer different notification methods in addition to Stackdriver alerting. What is the most efficient way to send notifications and meet your team’s requests?;Set up SMS text messaging, Slack, and email notifications on an alert.;Create a separate alert for each notification channel.;Create alerts with email notifications and have those notification emails forwarded to other notification systems.;Use a single third-party notification mechanism.;;A;Alerts can have multiple channels, so Option A is correct. Channels include email, webhooks, and SMS text messaging as well as third-party tools such as PagerDuty, Campfire, and Slack. There is no need for multiple alerts with individual notifications. Option C is ad hoc and would require additional maintenance overhead. Option D does not meet requirements.
A new engineer is trying to set up alerts for a Kubernetes cluster. The engineer seems to be creating a large number of alerts and you are concerned this is not the most efficient way and will lead to more maintenance work than required. You explain that a more efficient way is to create alerts and apply them to what?;One instance only;An instance or entire group;A group only;A pod;;B;"Alerts are assigned to instances or sets of instances; therefore, option B is correct. Option A is incorrect because it does not include groups. Option C is incorrect because it does not include instances. Option D is wrong because alerts are not assigned to pods."
You are attempting to execute commands to initiate a deployment on a Kubernetes cluster. The commands are not having any effect. You suspect that a Kubernetes component is not functioning correctly. What component could be the problem?;The Kubernetes API;A StatefulSet;Cloud SDK gcloud commands;ReplicaSet;;A;All interactions with the cluster are done through the master using the Kubernetes API. If an action is to be taken on a node, the command is issued by the cluster master, so option A is the correct answer. Options B and D are incorrect because they are controllers within the cluster and do not impact how commands are received from client devices. Option C is incorrect because kubectl, not gcloud, is used to initiate deployments.
You have deployed an application to a Kubernetes cluster. You have noticed that several pods are starved for resources for a period of time and the pods are shut down. When resources are available, new instantiations of those pods are created. Clients are still able to connect to pods even though the new pods have different IP addresses from the pods that were terminated. What Kubernetes component makes this possible?;Services;ReplicaSet;Alerts;StatefulSet;;A;Services provide a level of indirection to accessing pods. Pods are ephemeral. Clients connect to services, which can discover pods. ReplicaSets and StatefulSets provide managed pods. Alerts are for reporting on the state of resources.
You are running several microservices in a Kubernetes cluster. You’ve noticed some performance degradation. After reviewing some logs, you begin to think the cluster may be improperly configured, and you open Cloud Console to investigate. How do you see the details of a specific cluster?;Type the cluster name into the search bar.;Click the cluster name.;Use the gcloud cluster details command.;None of the above.;;B ;"When on the Cloud Console pages, you can click the cluster name to see a Details page, so option B is the correct answer. Typing the name of cluster in the search bar does not always return cluster details; it can return instance group details. There is no such command as gcloud cluster details."
You are viewing the details of a cluster in Cloud Console and want to see how many vCPUs are available in the cluster. Where would you look for that information?;Node Pools section of the Cluster Details page;Labels section of the Cluster Details page;Summary line of the Cluster Listing page;A and C;;D;You can find the number of vCPUs on the cluster listing in the Total Cores column or on the Details page in the Node Pool section in the size parameter, making option D correct. The Labels section does not have vCPU information.
You have been assigned to help diagnose performance problems with applications running on several Kubernetes clusters. The first thing you want to do is understand, at a high level, the characteristics of the clusters. Which command should you use?;gcloud container list;gcloud container clusters list;gcloud clusters list;None of the above;;B ;The correct command includes gcloud container to describe the service, clusters to indicate the resource you are referring to, and list to indicate the command, which makes option B the correct answer. Options A and C are not valid commands.
When you first try to use the kubectl command, you get an error message indicating that the resource cannot be found or you cannot connect to the cluster. What command would you use to try to eliminate the error?;gcloud container clusters access;gdcloud container clusters get-credentials;gcloud auth container;gcloud auth container clusters;;B;It is likely you do not have access privileges to the cluster. The gdcloud container clusters get-credentials command is the correct command to configure kubectl to use GCP credentials for the cluster, so option B is the right option. Options A, C, and D are invalid commands.
An engineer recently joined your team and is not aware of your team’s standards for creating clusters and other Kubernetes objects. In particular, the engineer has not properly labeled several clusters. You want to modify the labels on the cluster from Cloud Console. How would you do it?;Click the Connect button.;Click the Deploy menu option.;Click the Edit menu option.;Type the new labels in the Labels section.;;C;Clicking the Edit button allows you to change, add, or remove labels, so option C is the correct answer. The Connect button is on the cluster listing page, and the Deploy button is for creating new deployments. There is no way to enter labels under the Labels section when displaying details.
You receive a page in the middle of the night informing you that several services running on a Kubernetes cluster have high latency when responding to API requests. You review monitoring data and determine that there are not enough resources in the cluster to keep up with the load. You decide to add six more VMs to the cluster. What parameters will you need to specify when you issue the cluster resize command?;Cluster size;Cluster name;Node pool name;All of the above;;D;When resizing, the gcloud container clusters resize command requires the name of the cluster and the node pool to modify. The size is required to specify how many nodes should be running. Therefore, option D is correct.
You want to modify the number of pods in a cluster. What is the best way to do that?;Modify pods directly;Modify deployments;Modify node pools directly;Modify nodes;;B;"Pods are used to implement replicas of a deployment. It is a best practice to modify the deployments, which are configured with a specification of the number of replicas that should always run, so option B is the correct answer. Option A is incorrect; you should not modify pods directly. Options C and D are incorrect because they do not change the number of pods running an application."
You want to see a list of deployments. Which option from the Kubernetes Engine navigation menu would you select?;Clusters;Storage;Workloads;Deployments;;C;Deployments are listed under Workloads, making option C the correct answer. The Cluster option shows details about clusters but does not have details on deployments. Storage shows information about persistent volumes and storage classes. Deployments is not an option.
What actions are available from the Actions menu when viewing deployment details?;Scale and Autoscale only;Autoscale, Expose, and Rolling Update;Add, Modify, and Delete;None of the above;;B;There are four actions available for deployments (Autoscale, Expose, Rolling Update, and Scale), so option B is correct. Add, Modify, and Delete are not options.
What is the command to list deployments from the command line?;gcloud container clusters list-deployments;gcloud container clusters list;kubectl get deployments;kubectl deployments list;;C;Since deployments are managed by Kubernetes and not GCP, we need to use a kubectl command and not a gcloud command, which makes option C correct. Option D is incorrect because it follows the gcloud command structure, not the kubectl command structure. The kubectl command has the verb, like get, before the resource type, like deployments, for example.
What parameters of a deployment can be set in the Create Deployment page in Cloud Console?;Container image;Cluster name;Application name;All of the above;;D;"You can specify container image, cluster name, and application name along with the labels, initial command, and namespace; therefore, option D is the correct answer."
Where can you view a list of services when using Cloud Console?;In the Deployment Details page;In the Container Details page;In the Cluster Details page;None of the above;;A;"The Deployment Details page includes services, so option A is the correct answer. Containers are used to implement services; service details are not available there. The Clusters Detail page does not contain information on services running in the cluster."
What kubectl command is used to add a service?;run;start;initiate;deploy;;A;kubectl run is the command used to start a deployment. It takes a name for the deployment, an image, and a port specification. The other options are not valid kubectl commands.
You are supporting machine learning engineers who are testing a series of classifiers. They have five classifiers, called ml-classifier-1, ml-classifier-2, etc. They have found that mlclassifier-3 is not functioning as expected and they would like it removed from the cluster. What would you do to delete a service called ml-classifier-3?;Run the command kubectl delete service ml-classifier-3.;Run the command kubectl delete ml-classifier-3.;Run the command gcloud service delete ml-classifier-3.;Run the command gcloud container service delete ml-classifier-3.;;A;Option A shows the correct command, which is kubectl delete service mlclassifier-3. Option B is missing the service term. Options C and D cannot be correct because services are managed by Kubernetes, not GCP.
What service is responsible for managing container images?;Kubernetes Engine;Compute Engine;Container Registry;Container Engine;;C;The Container Registry is the service for managing images that can be used in other services, including Kubernetes Engine and Compute Engine, making option C correct. Both Compute Engine and Kubernetes Engine use images but do not manage them. There is no service called Container Engine.
What command is used to list container images in the command line?;gcloud container images list;gcloud container list images;kubectl list container images;kubectl container list images;;A;Images are managed by GCP, so the correct command will be a gcloud command, so option A is the correct answer. Option B is incorrect because the verb is placed before the resource. Options C and D are incorrect because kubectl is for managing Kubernetes resources, not GCP resources like container images.
A data warehouse designer wants to deploy an extraction, transformation, and load process to Kubernetes. The designer provided you with a list of libraries that should be installed, including drivers for GPUs. You have a number of container images that you think may meet the requirements. How could you get a detailed description of each of those containers?;Run the command gcloud container images list details.;Run the command gcloud container images describe.;Run the command gcloud image describe.;Run the command gcloud container describe.;;B;The correct command is gcloud container images describe, which makes option B the right answer. describe is the gcloud verb or operation for showing the details of an object. All other options are invalid commands.
You have just created a deployment and want applications outside the cluster to have access to the services provided by the deployment. What do you need to do to the service?;Give it a public IP address.;Issue a kubectl expose deployment command.;Issue a gcloud expose deployment command.;Nothing, making it accessible must be done at the cluster level.;;B;The kubectl expose deployment command makes a service accessible, so option B is the correct answer. IP addresses are assigned to VMs, not services. The command gcloud does not manage Kubernetes services, so option C is incorrect. Option D is incorrect because making a service accessible is not a cluster-level task.
You have deployed an application to a Kubernetes cluster that processes sensor data from a fleet of delivery vehicles. The volume of incoming data depends on the number of vehicles making deliveries. The number of vehicles making deliveries is dependent on the number of customer orders. Customer orders are high during daytime hours, holiday seasons, and when major advertising campaigns are run. You want to make sure you have enough nodes running to handle the load, but you want to keep your costs down. How should you configure your Kubernetes cluster?;Deploy as many nodes as your budget allows.;Enable autoscaling.;Monitor CPU, disk, and network utilization and add nodes as necessary.;Write a script to run gcloud commands to add and remove nodes when peaks usually start and end, respectively.;;B;Autoscaling is the most cost-effective and least burdensome way to respond to changes in demand for a service, so option B is the correct answer. Option A may run nodes even when they are not needed. Option C is manually intensive and requires human intervention. Option D reduces human intervention but does not account for unexpected spikes or lulls in demand.
When using Kubernetes Engine, which of the following might a cloud engineer need to configure?;Nodes, pods, services, and clusters only;Nodes, pods, services, clusters, and container images;Nodes, pods, clusters, and container images only;Pods, services, clusters, and container images only;;B;Cloud engineers working with Kubernetes will need to be familiar with working with clusters, nodes, pods, and container images. They will also need to be familiar with deployment. Option B is the correct answer because the other options are all missing an important component of Kubernetes that cloud engineers will have to manage.
You have designed a microservice that you want to deploy to production. Before it can be deployed, you have to review how you will manage the service lifecycle. The architect is particularly concerned about how you will deploy updates to the service with minimal disruption. What aspect of App Engine components would you use to minimize disruptions during updates to the service?;Services;Versions;Instance groups;Instances;;B;Versions support migration. An app can have multiple versions, and by deploying with the --migrate parameter, you can migrate traffic to the new version, so option B is the correct answer. Services are a higher-level abstraction and represent the functionality of a microservice. An app may have multiple services, but they serve different purposes. Instances execute code in a version. Instances may be added and removed as needed, but they will run only one version of a service. Instance groups are part of Compute Engine and are not an App Engine component.
You’ve just released an application running in App Engine Standard. You notice that there are peak demand periods in which you need up to 12 instances, but most of the time 5 instances are sufficient. What is the best way to ensure that you have enough instances to meet demand without spending more than you have to?;Configure your app for autoscaling and specify max instances of 12 and min instances of 5.;Configure your app for basic scaling and specify max instances of 12 and min instances of 5.;Create a cron job to add instances just prior to peak periods and remove instances after the peak period is over.;Configure your app for instance detection and do not specify a max or minimum number of instances.;;A;"Autoscaling enables setting a maximum and minimum number of instances, which makes option A correct. Basic scaling does not support maximum and minimum instances. Option C is not recommended because it is difficult to predict when load will peak and even if the schedule is predictable today, it may change over time. Option D is wrong; there is no instance detection option."
In the hierarchy of App Engine components, what is the lowest-level component?;Application;Instance;Version;Service;;B;Application is the top-level component, so option B is the correct answer. Applications have one or more services. Services have one or more versions. Versions are executed on one or more instances when the application is running.
What command should you use to deploy an App Engine app from the command line?;gcloud components app deploy;gcloud app deploy;gcloud components instances deploy;gcloud app instance deploy;;B;"The correct command is gcloud app deploy, which is option B. Options A and C are incorrect because gcloud components commands are used to install gcloud commands for working with parts of App Engine, such as the Python runtime environment. Option D is incorrect; you do not need to specify instance in the command."
You have deployed a Django 1.5 Python application to App Engine. This version of Django requires Python 3. For some reason, App Engine is trying to run the application using Python 2. What file would you check and possibly modify to ensure that Python 3 is used with this application?;app.config;app.yaml;services.yaml;deploy.yaml;;B;The app.yaml file is used to configure an App Engine application, which makes option B correct. The other options are not files used to configure App Engine.
You have several App Engine apps you plan to deploy from your project. What have you failed to account for in this design?;App Engine only supports one app per project.;App Engine only supports two apps per project.;App Engine apps exist outside of projects.;Nothing, this is a common pattern.;;A;A project can support only one App Engine app, so option A is the right answer. If you’d like to run other applications, they will need to be placed in their own projects.
The latest version of your microservice code has been approved by your manager, but the product owner does not want the new features released until a press release is published. You’d like to get the code out but not expose it to customers. What is the best way to get the code out as soon as possible without exposing it to customers?;Deploy with gcloud app deploy --no-traffic.;Write a cron job to deploy after the press release is published.;Deploy with gcloud app deploy --no-promote.;Deploy as normal after the press release is published.;;C;The correct answer is option C because the correct parameter is --no-promote. Option A uses no-traffic, which is not a valid parameter to the gcloud app deploy command. Option B does not get the code out and could release the code too early if there is a delay in getting the press release out. Option D does not meet the requirements of getting the code out as soon as possible.
You have just deployed an app that hosts services that provide the current time in any time zone. The project containing the code is called current-time-zone, the service providing the user interface is called time-zone-ui, and the service performing the calculation is called time-zone-calculate. What is the URL where a user could find your service?;current-time-zone.appengine.com;current-time-zone.appspot.com;time-zone-ui.appspot.com;time-zone-calculate.appspot.com;;B;App Engine applications are accessible from URLs that consist of the project name followed by appspot.com, so option B is correct. Option A is incorrect because the domain is not appengine.com. Options C and D are incorrect because the names of services are not used to reference the application as a whole.
You are concerned that as users make connections to your application, the performance will degrade. You want to make sure that more instances are added to your App Engine application when there are more than 20 concurrent requests. What parameter would you specify in app.yaml?;max_concurrent_requests;target_throughput_utilization;max_instances;max_pending_latency;;A;max_concurrent_requests lets you specify the maximum number of concurrent requests before another instance is started, which makes option A correct. target_ throughput_utilization functions similarly but uses a 0.05 to 0.95 scale to specify maximum throughput utilization. max_instances specifies the maximum number of instances but not the criteria for adding instances. max_pending_latency is based on the time a request waits, not the number of requests.
What parameters can be configured with basic scaling?;max_instances and min_instances;idle_timeout and min_instances;idle_timeout and max_instances;idle_timeout and target_throughput_utilization;;C;Basic scaling only allows for idle time and maximum instances, so option C is the right answer. min_instances is not supported. target_throughput_utilization is an autoscaling parameter, not a basic scaling parameter.
The runtime parameter in app.yaml is used to specify what?;The script to execute;The URL to access the application;The language runtime environment;The maximum time an application can run;;C;The runtime parameter specifies the language environment to execute in, which makes option C correct. The script to execute is specified by the script parameter. The URL to access the application is based on the project name and the domain appspot.com. There is no parameter for specifying the maximum time an application can run.
What are the two kinds of instances available in App Engine Standard?;Resident and dynamic;Persistent and dynamic;Stable and dynamic;Resident and nonresident;;A;Resident instances are used with manual scaling while dynamic instances are used with autoscaling and basic scaling, so option A is the correct answer. There are no persistent, stable, or nonresident types of App Engine instances.
You work for a startup, and costs are a major concern. You are willing to take a slight performance hit if it will save you money. How should you configure the scaling for your apps running in App Engine?;Use dynamic instances by specifying autoscaling or basic scaling.;Use resident instances by specifying autoscaling or basic scaling.;Use dynamic instances by specifying manual scaling.;Use resident instances by specifying manual scaling.;;A;Using dynamic instances by specifying autoscaling or basic scaling will automatically adjust the number of instances in use based on load, so option A is correct. Option B is incorrect because autoscaling and basic scaling only create dynamic instances. Options C and D are incorrect because manual scaling will not adjust instances automatically, so you may continue to run more instances than needed at some points.
A team of developers has created an optimized version of a service. This should run 30 percent faster in most cases. They want to roll it out to all users immediately, but you are concerned that the substantial changes need to be released slowly in case there are significant bugs. What can you do to allocate some users to the new version without exposing all users to it?;Issue the command gcloud app services set-traffic.;Issue the command gcloud instances services set-traffic.;Issue the command gcloud app set-traffic.;Change the target IP address of the service for some clients.;;A;The correct answer is gcloud app services set-traffic. Option B is incorrect because the term instances is not needed. Option C is incorrect because it does not specify the term services. Option D is incorrect because that would require changes on the client’s part.
What parameter to gcloud app services set-traffic is used to specify the method to use when splitting traffic?; --split; --split-by; --traffic-split;  --split-method;;A;--split-traffic is the parameter used to specify the method for splitting traffic, which makes option A correct. Valid options are cookie, ip, and random. All other options are not valid parameters to the gcloud app services set-traffic command.
What parameter to gcloud app services set-traffic is used to specify the percentage of traffic that should go to each instance?; --split-by; --splits; --split-percent; --percent-split;;B;--split is the parameter for specifying a list of instances and the percent of traffic they should receive, so option B is the right answer. The other options are not valid parameters for the gcloud app services set-traffic command.
You have released a new version of a service. You have been waiting for approval from the product manager to start sending traffic to the new version. You get approval to route traffic to the new version. What parameter to gcloud app services set-traffic is used to specify that traffic should be moved to a newer version of the app?; --move-to-new; --migrate-to-new; --migrate; --move;;C;--migrate is the parameter for specifying that traffic should be moved or migrated to the newer instance, which makes option C the correct answer. The other options are not valid parameters for the gcloud app services set-traffic command.
The status of what components can be viewed in the App Engine console?;Services only;Versions only;Instances and versions;Services, versions, and instances;;D;From the App Engine console you can view the list of services and versions as well as information about the utilization of each instance.
What are valid methods for splitting traffic?;By IP address only;By HTTP cookie only;Randomly and by IP address only;By IP address, HTTP cookies, and randomly;;D;All three methods listed, IP address, HTTP cookie, and random splitting, are allowed methods for splitting traffic.
What is the name of the cookie used by App Engine when cookie-based splitting is used?;GOOGID;GOOGAPPUID;APPUID;UIDAPP;;B;The cookie used for splitting in App Engine is called GOOGAPPUID, which makes option B the correct answer. Options A, C, and D are not valid names.
A product manager is proposing a new application that will require several backend services, three business logic services, and access to relational databases. Each service will provide a single function, and it will require several of these services to complete a business task. Service execution time is dependent on the size of input and is expected to take up to 30 minutes in some cases. Which GCP product is a good serverless option for running this related service?;Cloud Functions;Compute Engine;App Engine;Cloud Storage;;C;App Engine is designed to support multiple tightly coupled services comprising an application, making option C the correct answer. This is unlike Cloud Functions, which is designed to support single-purpose functions that operate independently and in response to isolated events in the Google Cloud and complete within a specified period of time. Compute Engine is not a serverless option. Cloud Storage is not a computing product.
You have been asked to deploy a cloud function to reformat image files as soon as they are uploaded to Cloud Storage. You notice after a few hours that about 10 percent of the files are not processed correctly. After reviewing the files that failed, you realize they are all substantially larger than average. What could be the cause of the failures?;There is a syntax error in the function code.;The wrong runtime was selected.;The timeout is too low to allow enough time to process large files.;There is a permissions error on the Cloud Storage bucket containing the files.;;C;A timeout period that is too low would explain why the smaller files are processed in time but the largest are not, which makes option C the right answer. If only 10 percent of the files are failing, then it is not a syntax error or the wrong runtime selected, as in options A and B. Those errors would affect all files, not just the largest ones. Similarly, if there was a permission problem with the Cloud Storage bucket, it would affect all files.
When an action occurs in GCP, such as a file being written to Cloud Storage or a message being added to a Cloud Pub/Sub topic, that action is called what?;An incident;An event;A trigger;A log entry;;B;"Those actions are known as events in Google Cloud terminology; thus, option B is the correct answer. An incident may be a security or performance-related occurrence, but those are unrelated to the expected and standardized actions that constitute events. A trigger is a declaration that a certain function should execute when an event occurs. A log entry is related to applications recording data about significant events. Log entries are helpful for monitoring and compliance, but in themselves are not event-related actions."
All of the following generate events that can be triggered using Cloud Functions, except which one?;Cloud Storage;Cloud Pub/Sub;SSL;Firebase;;C;The correct answer is option C because SSL is a secure protocol for remotely accessing servers. It is used, for example, to access instances in Compute Engine. It does not have events that can be triggered using Cloud Functions. The three GCP products listed do generate events that can have triggers associated with them.
Which runtimes are supported in Cloud Functions?;Node.js 5, Node.js 6, and Node.js 8;Node.js 8, Python, and Go;Node.js 6, Node.js 8, and Python;;;C;Cloud Functions supports three runtimes: Node.js 6, Node.js 8, and Python. Go and Node.js 5 are not supported runtimes.
An HTTP trigger can be invoked by making a request using which of the following?;GET only;POST and GET only;DELETE, POST, and GET;DELETE, POST, REVISE, and GET;;D;HTTP requests using GET, POST, DELETE, PUT, and OPTIONS can invoke an HTTP trigger in Cloud Functions, so option C is the right answer.
What types of events are available to Cloud Functions working with Cloud Storage?;Upload or finalize and delete only;Upload or finalize, delete, and list only;Upload or finalize, delete, and metadata update only;Upload or finalize, delete, metadata update, and archive;;D;"The correct answer, option D, shows the four events supported in Cloud Storage. 
 google.storage.object.finalize 
 google.storage.object.delete 
 google.storage.object.archive 
 google.storage.object.metadataUpdate"
You are tasked with designing a function to execute in Cloud Functions. The function will need more than the default amount of memory and should be applied only when a finalize event occurs after a file is uploaded to Cloud Storage. The function should only apply its logic to files with a standard image file type. Which of the following required features cannot be specified in a parameter and must be implemented in the function code?;Cloud function name;Memory allocated for the function;File type to apply the function to;Event type;;C;There is no option to specify the file type to apply the function to, so option C is correct. You can, however, specify the bucket to which the function is applied. You could only save files or the types you want processed in that bucket, or you could have your function check file type and then execute the rest of the function or not, based on type. All the other options listed are parameters to a Cloud Storage function.
How much memory can be allocated to a Cloud Function?;128MB to 256MB;128MB to 512MB;128MB to 1GB;128MB to 2GB;;D;Cloud Functions can have between 128MB and 2GB of memory allocated, which makes option D the correct answer. The default is 256MB.
How long can a cloud function run by default before timing out?;30 seconds;1 minute;9 minutes;20 minutes;;B;By default Cloud Functions can run for up to 1 minute before timing out, so option B is correct. You can, however, set the timeout parameter for a cloud function for periods of up to 9 minutes before timing out.
You want to use the command line to manage Cloud Functions that will be written in Python. In addition to running the gcloud components update command, what command should you run to ensure you can work with Python functions?;gcloud component install;gcloud components install beta;gcloud components install python;gcloud functions install beta;;B;Python Cloud Functions is currently in beta. The standard set of gcloud commands does not include commands for alpha or beta release features by default. You will need to explicitly install beta features using the gcloud components install beta command, so option B is the right answer. Option A will install standard gcloud commands. Options C and D are not valid gcloud commands.
You want to create a cloud function to transform audio files into different formats. The audio files will be uploaded into Cloud Storage. You want to start transformations as soon as the files finish uploading. Which trigger would you specify in the cloud function to cause it to execute after the file is uploaded?;google.storage.object.finalize;google.storage.object.upload;google.storage.object.archive;google.storage.object.metadataUpdate;;A;The correct trigger in option A is google.storage.object.finalize, which occurs after a file is uploaded. Option B is not a valid trigger name. Option C triggers when a file is archived, not uploaded. Option D is triggered when some metadata attribute changes, but not necessarily only after a file uploads.
You are defining a cloud function to write a record to a database when a file in Cloud Storage is archived. What parameters will you have to set when creating that function?;runtime only;trigger-resource only;runtime, trigger-resource, trigger-event only;runtime, trigger-resource, trigger-event, file-type;;C;The three parameters are runtime, trigger-resource, and trigger-event, as listed in option C. All must be set, so options A and B are incorrect. file-type is not a parameter to creating a cloud function on Cloud Storage, so option D is incorrect.
You’d like to stop using a cloud function and delete it from your project. Which command would you use from the command line to delete a cloud function?;gcloud functions delete;gcloud components function delete;gcloud components delete;gcloud delete functions;;A;The correct answer is option A, gcloud functions delete. Option B references components, which is incorrect. You do need to reference components when installing or updating gcloud commands but not when deleting a cloud function, so options B and C are incorrect. Option D is incorrect because the GCP entity type, in this case functions, comes before the name of the operation, in this case delete, in a gcloud command.
You have been asked to deploy a cloud function to work with Cloud Pub/Sub. As you review the Python code, you notice a reference to a Python function called base64.b64decode. Why would a decode function be required in a Pub/Sub cloud function?;It’s not required and should not be there.;Messages in Pub/Sub topics are encoded to allow binary data to be used in places where text data is expected. Messages need to be decoded to access the data in the message.;It is required to add padding characters to the end of the message to make all messages the same length.;The decode function maps data from a dictionary data structure to a list data structure.;;B;"Messages are stored in a text format, base64, so that binary data can be stored in the message in a text format, so option B is correct. Option A is incorrect; it is needed to map from a binary encoding to a standard text encoding. Option C is incorrect because the function does not pad with extra characters to make them the same length. Option D is incorrect; it does not change dictionary data types into list data types."
Which of these commands will deploy a Python cloud function called pub_sub_function_test?;gcloud functions deploy pub_sub_function_test;gcloud functions deploy pub_sub_function_test --runtime python37;gcloud functions deploy pub_sub_function_test --runtime python37 --trigger-topic gcp-ace-exam-test-topic;gcloud functions deploy pub_sub_function_test --runtime python --trigger-topic gcp-ace-exam-test-topic;;C;"Option C is correct because it includes the name of the function, the runtime environment, and the name of the Pub/Sub topic. Option A is incorrect because it’s missing both the runtime and the topic. Option B is incorrect because it is missing the topic. Option D is incorrect because the runtime specification is incorrect; you have to specify python37 and not python as the runtime."
When specifying a Cloud Storage cloud function, you have to specify an event type, such as finalize, delete, or archive. When specifying a Cloud Pub/Sub cloud function, you do not have to specify an event type. Why is this the case?;Cloud Pub/Sub does not have triggers for event types.;Cloud Pub/Sub has triggers on only one event type, when a message is published.;Cloud Pub/Sub determines the correct event type by analyzing the function code;"The statement in the question is incorrect; you do have to specify an event type with Cloud Pub/Sub functions.";;B;"There is only one type of event that is triggered in Cloud Pub/Sub, and that is when a message is published, which is option B. Option A is incorrect; Cloud Pub/Sub has one event type that can have a trigger. Option C is incorrect; Cloud Pub/Sub does not analyze the code to determine when it should be run. Option D is incorrect; you do not have to specify an event type with Cloud Pub/Sub functions."
Your company has a web application that allows job seekers to upload résumé files. Some files are in Microsoft Word, some are PDFs, and others are text files. You would like to store all résumés as PDFs. How could you do this in a way that minimizes the time between upload and conversion and with minimal amounts of coding?;Write an App Engine application with multiple services to convert all documents to PDF.;Implement a Cloud Function on Cloud Storage to execute on a finalize event. The function checks the file type, and if it is not PDF, the function calls a PDF converter function and writes the PDF version to the bucket that has the original.;Add the names of all files to a Cloud Pub/Sub topic and have a batch job run at regular intervals to convert the original files to PDF.;Implement a Cloud Function on Cloud Pub/Sub to execute on a finalize event. The function checks the file type, and if it is not PDF, the function calls a PDF converter function and writes the PDF version to the bucket that has the original.;;B;The correct answer is option B because it uses a Cloud Storage finalize event to trigger conversion if needed. There is minimal delay between the time the file is uploaded and when it is converted. Option A is a possibility but would require more coding than option B. Option C is not a good option because files are not converted until the batch job runs. Option D is incorrect because you cannot create a cloud function for Cloud Pub/Sub using a finalize event. That event is for Cloud Storage, not Cloud Pub/Sub.
What are options for uploading code to a cloud function?;Inline editor;Zip upload;Cloud source repository;All of the above;;D;All of the options are available along with zip from Cloud Storage.
What type of trigger allows developers to use HTTP POST, GET, and PUT calls to invoke a cloud function?;HTTP;Webhook;Cloud HTTP;None of the above;;A;The HTTP trigger allows for the use of POST, GET, and PUT calls, so option A is the correct answer. Webhook and Cloud HTTP are not valid trigger types. Option D is incorrect because option A is the correct answer.
You are tasked with defining lifecycle configurations on buckets in Cloud Storage. You need to consider all possible options for transitioning from one storage class to another. All of the following transitions are allowed except for one. Which one is that?;Nearline to coldline;Regional to nearline;Multiregional to coldline;Regional to multiregional;;D;Once a bucket is created as either regional or multiregional, it cannot be changed to the other, so option D is correct. Nearline to coldline and regional to nearline are both allowed, as is multiregional to coldline.
Your manager has asked for your help in reducing Cloud Storage charges. You know that some of the files stored in Cloud Storage are rarely accessed. What kind of storage would you recommend for those files?;Nearline;Regional;Coldline;Multiregional;;C;The goal is to reduce cost, so you would want to use the least costly storage option. Coldline has the lowest per-gigabyte charge at $0.07/GB/month, so option C is correct. Nearline is the next lowest followed by regional. Multiregional has the highest per-gigabyte charge. Both nearline and coldline have access charges, but those are not considered in this question.
You are working with a startup developing analytics software for IoT data. You have to be able to ingest large volumes of data consistently and store it for several months. The startup has several applications that will need to query this data. Volumes are expected to grow to petabyte volumes. Which database should you use?;Cloud Spanner;Bigtable;BigQuery;Datastore;;B;Bigtable is a wide-column database that can ingest large volumes of data consistently, so option B is correct. It also supports low-millisecond latency, making it a good choice for supporting querying. Cloud Spanner is a global relational database that is not suitable for high-speed ingestion of large volumes of data. Datastore is an object data model and not a good fit for IoT or other time series data. BigQuery is an analytics database and not designed for ingestion of large volumes of data in short periods of time.
A software developer on your team is asking for your help improving the query performance of a database application. The developer is using a Cloud SQL MySQL, Second Generation instance. Which options would you recommend?;Memorystore and SSD persistent disks;Memorystore and HDD persistent disks;Datastore and SSD persistent disks;Datastore and HDD persistent disks;;A;Option A is correct because Memorystore is a managed Redis cache. The cache can be used to store the results of queries. Follow-on queries that reference the data stored in the cache can read it from the cache, which is much faster than reading from persistent disks. SSDs have significantly lower latency than hard disk drives and should be used for performance-sensitive applications like databases. Options B and D are incorrect because HDD persistent disks do give the best performance with respect to IOPS. Options C and D are incorrect because Datastore is a managed NoSQL database and would not have any impact on SQL query performance.
You are creating a set of persistent disks to store data for exploratory data analysis. The disks will be mounted on a virtual machine in the us-west2-a zone. The data is historical data retrieved from Cloud Storage. The data analysts do not need peak performance and are more concerned about cost than performance. The data will be stored in a local relational database. Which type of storage would you recommend?;SSDs;HDDs;Datastore;Bigtable;;B;"HDDs are the better choice for persistent disks for a local database when performance is not the primary concern and you are trying to keep costs down, so option B is correct. Option A is wrong because SSDs are more expensive and the users do not need the lowest latency available. Options C and D are wrong; both of those are other databases that would not be used to store data in a local relational database."
Which of the following statements about Cloud Storage is not true?;Cloud Storage buckets can have retention periods.;Lifecycle configurations can be used to change storage class from regional to multiregional.;Cloud Storage does not provide block-level access to data within files stored in buckets.;Cloud Storage is designed for high durability.;;B;"Lifecycle configurations can change storage class from regional to nearline or coldline. Once a bucket is created as regional or multiregional, it cannot be changed to the other, so option B is the right answer. Option A is true; you can set retention periods when creating a bucket. Option C is true; Cloud Storage does not provide file system–like access to internal data blocks. Option D is true because Cloud Storage is highly durable."
When using versioning on a bucket, what is the latest version of the object called?;Live version;Top version;Active version;Safe version;;A;"The most recent version of an object is called the live version, so option A is correct. Options B and C are incorrect; top and active are not terms used to refer to versions. Option D is incorrect because option A is correct."
A product manager has asked for your advice on which database services might be options for a new application. Transactions and support for tabular data are important. Ideally, the database would support common query tools. What databases would you recommend the product manager consider?;BigQuery and Spanner;Cloud SQL and Spanner;Cloud SQL and Bigtable;Bigtable and Spanner;;B;Both Cloud SQL and Spanner are relational databases and are well suited for transactionprocessing applications, so option B is right. Option A is incorrect because BigQuery is relational, but it is designed for data warehousing and analytics, not transaction processing. Options C and D are incorrect because Bigtable a wide-column NoSQL database, not a relational database.
The Cloud SQL service provides fully managed relational databases. What two types of databases are available in Cloud SQL?;SQL Server and MySQL;SQL Server and PostgreSQL;PostgreSQL and MySQL;MySQL and Oracle;;C;Both MySQL and PostgreSQL are Cloud SQL options so Option C is correct. Options A and B are incorrect, SQL Server is not a Cloud SQL option. Option D is incorrect because Oracle is not a Cloud SQL option. You could choose to run SQL Server or Oracle on your instances but you would have to manage them, unlike Cloud SQL managed databases
Which of the following Cloud Spanner configurations would have the highest hourly cost?;Located in us-central1;Located in nam3;Located in us-west1-a;Located in nam-eur-asia1;;D;"The multiregional and multi-super-regional location of nam-eur-aisa1 is the most expensive, which makes option D the right answer. Option A is a region that costs less than the multi-super-regional nam-eur-asia1. Option C is incorrect; that is a zone, and Spanner is configured to regions or super regions. Option B is incorrect; it is only a single super region, which cost less than deploying to multiple super regions."
Which of the following are database services that do not require you to specify configuration information for VMs?;BigQuery only;Datastore only;Firebase and Datastore;BigQuery, Datastore, and Firebase;;D;BigQuery, Datastore, and Firebase are all fully managed services that do not require you to specify configuration information for VMs, which makes option D correct. Cloud SQL and Bigtable require you to specify some configuration information for VMs.
What kind of data model is used by Datastore?;Relational;Document;Wide-column;Graph;;B;Datastore is a document database, which makes option B correct. Cloud SQL and Spanner are relational databases. Bigtable is a wide-column database. Google does not offer a managed graph database.
You have been tasked with creating a data warehouse for your company. It must support tens of petabytes of data and use SQL for a query language. Which managed database service would you choose?;BigQuery;Bigtable;Cloud SQL;SQL Server;;A;"BigQuery is a managed service designed for data warehouses and analytics. It uses standard SQL for querying, which makes option A the right answer. Bigtable can support the volume of data described, but it does not use SQL as a query language. Cloud SQL is not the best option to scale to tens of petabytes. SQL Server is a relational database from Microsoft; it is not a GCP-managed database service."
A team of mobile developers is developing a new application. It will require synchronizing data between mobile devices and a backend database. Which database service would you recommend?;BigQuery;Firestore;Spanner;Bigtable;;B;Firestore is a document database that has mobile supporting features, like data synchronization, so option B is the right answer. BigQuery is for analytics, not mobile or transactional applications. Spanner is a global relational database but does not have mobile-specific features. Bigtable could be used with mobile devices, but it does not have mobile-specific features like synchronization.
A product manager is considering a new set of features for an application that will require additional storage. What features of storage would you suggest the product manager consider?;Read and write patterns only;Cost only;Consistency and cost only;None, they are all relevant considerations.;;D;In addition to read and write patterns, cost, and consistency, you should consider transaction support and latency, which makes option D correct.
What is the maximum size of a Memorystore cache?;100GB;300GB;400GB;50GB;;B;Option B is correct because Memorystore can be configured to use between 1GB and 300GB of memory.
Once a bucket has its storage class set to coldline, what are other storage classes it can transition to?;Regional;Nearline;Multi-regional;None of the above;;D;"Once a bucket is set to coldline, it cannot be changed to another storage class; thus, option D is correct. Regional and multiregional can change to nearline and coldline. Nearline buckets can change to coldline."
Before you can start storing data in BigQuery, what must you create?;A data set;A bucket;A persistent disk;An entity;;A;To use BigQuery to store data, you must have a data set to store it, which makes option A the right answer. Buckets are used by Cloud Storage, not BigQuery. You do not manage persistent disks when using BigQuery. An entity is a data structure in Datastore, not BigQuery.
What features can you configure when running a Second Generation MySQL database in Cloud SQL?;Machine type;Maintenance windows;Failover replicas;All of the above;;D;With a second-generation instance, you can configure the MySQL version, connectivity, machine type, automatic backups, failover replicas, database flags, maintenance windows, and labels, so option D is correct.
A colleague is wondering why some storage charges are so high. They explain that they have moved all their storage to nearline and coldline storage. They routinely access most of the object on any given day. What is one possible reason the storage costs are higher than expected?;Nearline and coldline incur access charges.;Transfer charges.;Multiregional coldline is more expensive.;Regional coldline is more expensive.;;A;Access charges are used with nearline and coldline storage, which makes option A correct. There is no transfer charge involved. Options C and D do not refer to actual storage classes.
Cloud SQL is a fully managed relational database service, but database administrators still have to perform some tasks. Which of the following tasks do Cloud SQL users need to perform?;Applying security patches;Performing regularly scheduled backups;Creating databases;Tuning the operating system to optimize Cloud SQL performance;;C;"Creating databases is the responsibility of database administrators or other users of Cloud SQL, so option C is correct. Google applies security patches and performs other maintenance, so option A is incorrect. GCP performs regularly scheduled backups, so option B is incorrect. Database administrators need to schedule backups, but GCP makes sure they are performed on schedule. Cloud SQL users can’t SSH into a Cloud SQL server, so they can’t tune the operating system. That’s not a problem; Google takes care of that."
Which of the following commands is used to create a backup of a Cloud SQL database?;gcloud sql backups create;gsutil sql backups create;gcloud sql create backups;gcloud sql backups export;;A;"Cloud SQL is controlled using the gcloud command; the sequence of terms in gcloud commands is gcloud followed by the service, in this case SQL; followed by a resource, in this case backups, and a command or verb, in this case create. Option A is the correct answer. Option B is incorrect because gsutil is used to work with Cloud Storage, not Cloud SQL. Option C is wrong because the order of terms is incorrect; backups comes before create. Option D is incorrect because the command or verb should be create."
Which of the following commands will run an automatic backup at 3:00 a.m. on an instance called ace-exam-mysql?;gcloud sql instances patch ace-exam-mysql --backup-start-time 03:00;gcloud sql databases patch ace-exam-mysql –-backup-start-time 03:00;cbt sql instances patch ace-exam-mysql -–backup-start-time 03:00;bq gcloud sql instances patch ace-exam-mysql -–backup-start-time 03:00;;A;"Option A is the correct answer. The base command is gcloud sql instances patch, which is followed by the instance name and a start time passed to the –-backup-start-time parameter. Option B is incorrect because databases is not the correct resource to reference; instances is. Option C uses the cbt command, which is for use with Bigtable, so it is incorrect. Similarly, Option D is incorrect because it uses the bq command, which is used to manage BigQuery resources."
What is the query language used by Datastore?;SQL;MDX;GQL;DataFrames;;C;"Datastore uses a SQL-like query language called GQL, so option C is correct. Option A is incorrect; SQL is not used with this database. Option B is incorrect; MDX is a query language for online analytic processing (OLAP) systems. Option D is incorrect because DataFrames is a data structure used in Spark."
What is the correct command-line structure to export data from Datastore?;gcloud datastore export '[NAMESPACE]' gs://[BUCKET_NAME];gcloud datastore export gs://[BUCKET_NAME];gcloud datastore export --namespaces='[NAMESPACE]' gs://[BUCKET_NAME];gcloud datastore dump --namespaces='[NAMESPACE]' gs://[BUCKET_NAME];;C;Option C is the correct command. It has the correct base command, gcloud datastore export, followed by the --namespaces parameter and the name of a Cloud Storage bucket to hold the export file. Option A is incorrect because the --namespaces parameter name is missing. Option B is incorrect because it is missing a namespace. Option D is incorrect because it uses the command or verb dump instead of export.
When you enter a query into the BigQuery query form, BigQuery analyzes the query and displays an estimate of what metric?;Time required to enter the query;Cost of the query;Amount of data scanned;Number of bytes passed between servers in the BigQuery cluster;;C;"Option C is correct; BigQuery displays an estimate of the amount of data scanned. This is important because BigQuery charges for data scanned in queries. Option A is incorrect; knowing how long it took you to enter a query is not helpful. Option B is incorrect; you need to use the scanned data estimate with the Pricing Calculator to get an estimate cost. Option D is incorrect; you do not create clusters in BigQuery as you do with Bigtable and Dataproc. Network I/O data is not displayed."
You want to get an estimate of the volume of data scanned by BigQuery from the command line. Which option shows the command structure you should use?;gcloud BigQuery query estimate [SQL_QUERY];bq ––location=[LOCATION] query --use_legacy_sql=false ––dry_run [SQL_QUERY];gsutil ––location=[LOCATION] query --use_legacy_sql=false ––dry_run [SQL_QUERY];cbt BigQuery query estimate [SQL_QUERY];;B;"Option B shows the correct bq command structure, which includes location and the ––dry_run option. This option calculates an estimate without actually running the query. Options A and C are incorrect because they use the wrong command; gcloud and gsutil are not used with BigQuery. Option D is also wrong. cbt is a tool for working with Bigtable, not BigQuery. Be careful not to confuse the two because their names are similar."
You are using Cloud Console and want to check on some jobs running in BigQuery. You navigate to the BigQuery part of the console. Which menu item would you click to view jobs?;Job History.;Active Jobs.;My Jobs.;"You can’t view job status in the console; you have to use bq on the command line.";;A;"Option A is correct; the menu option is Job History. Options B and C are incorrect; there is no Active Jobs or My Jobs option. Job History shows active jobs, completed jobs, and jobs that generated errors. Option D is incorrect; you can get job status in the console."
You want to estimate the cost of running a BigQuery query. What two services within Google Cloud Platform will you need to use?;BigQuery and Billing;Billing and Pricing Calculator;BigQuery and Pricing Calculator;Billing and Pricing Calculator;;C;"BigQuery provides an estimate of the amount of data scanned, and the Pricing Calculator gives a cost estimate for scanning that volume of data. Options A, B, and C are incorrect; the Billing service tracks charges incurred. It is not used to estimate future or potential charges."
You have just created a Cloud Spanner instance. You have been tasked with creating a way to store data about a product catalog. What is the next step after creating a Cloud Spanner instance that you would perform to enable you to load data?;Run gcloud spanner update-security-patches.;Create a database within the instance.;Create tables to hold the data.;Use the Cloud Spanner console to import data into tables created with the instance.;;B;"Option B is correct; the next step is to create a database within the instance. Once a database is created, tables can be created, and data can be loaded into tables. Option A is incorrect; Cloud Spanner is a managed database, so you do not need to apply security patches. Option C is incorrect because you can’t create tables without first having created a database. Option D is incorrect; no tables are created that you could import data into when an instance is created."
You have created a Cloud Spanner instance and database. According to Google best practices, how often should you update VM packages using apt-get?;Every 24 hours.;Every 7 days;Every 30 days.;Never, Cloud Spanner is a managed service.;;D;Option D is correct because there is no need to apply patches to the underlying compute resources when using Cloud Spanner. because Google manages resources used by Cloud Spanner. Updating packages is a good practice when using VMs, for example, with Compute Engine, but it is not necessary with a managed service.
Your software team is developing a distributed application and wants to send messages from one application to another. Once the consuming application reads a message, it should be deleted. You want your system to be robust to failure, so messages should be available for at least three days before they are discarded. Which GCP service is best designed to support this use case?;Bigtable;Dataproc;Cloud Pub/Sub;Cloud Spanner;;C;"This use case is well suited to Pub/Sub, so option C is correct. It involves sending messages to the topic, and the subscription model is a good fit. Pub/Sub has a retention period to support the three-day retention period. Option A is incorrect; Bigtable is designed for storing large volumes of data. Dataproc is for processing and analyzing data, not passing it between systems. Cloud Spanner is a global relational database. You could design an application to meet this use case, but it would require substantial development and be costly to run."
Your manager asks you to set up a bare-bones Pub/Sub system as a sandbox for new developers to learn about messaging systems. What are the two resources within Pub/Sub you will need to create?;Topics and tables;Topics and databases;Topics and subscriptions;Tables and subscriptions;;C;"Pub/Sub works with topics, which receive and hold messages, and subscriptions, which make messages available to consuming applications; therefore, option C is correct. Option A is incorrect; tables are data structures in relational databases, not message queues. Similarly, option B is wrong because databases exist in instances of database management systems, not messaging systems. Option D is wrong because tables are not a resource in messaging systems."
Your company is launching an IoT service and will receive large volumes of streaming data. You have to store this data in Bigtable. You want to explore the Bigtable environment from the command line. What command would you run to ensure you have command-line tools installed?;apt-get install bigtable-tools;apt-get install cbt;gcloud components install cbt;gcloud components install bigtable-tools;;C;"The correct command is gcloud components install cbt to install the Bigtable command-line tool, so option C is correct. Options A and B are incorrect; apt-get is used to install packages on some Linux systems but is not specific to GCP. Option D is incorrect; there is no such command as bigtable-tools."
You need to create a table called iot-ingest-data in Bigtable. What command would you use?;cbt createtable iot-ingest-data;gcloud bigtable tables create ace-exam-bt-table;gcloud bigtable create tables ace-exam-bt-table;gcloud create ace-exam-bt-table;;A;You would need to use a cbt command, which is the command-line tool for working with Bigtable, so option A is correct. All other options reference gcloud and are therefore incorrect.
Cloud Dataproc is a managed service for which two big data platforms?;Spark and Cassandra;Spark and Hadoop;Hadoop and Cassandra;Spark and TensorFlow;;B;Cloud Dataproc is a managed service for Spark and Hadoop, so option B is correct. Cassandra is a big data distributed database but is not offered as a managed service by Google, so options A and C are incorrect. Option D is incorrect because TensorFlow is a deep learning platform not included in Dataproc.
Your department has been asked to analyze large batches of data every night. The jobs will run for about three to four hours. You want to shut down resources as soon as the analysis is done, so you decide to write a script to create a Dataproc cluster every night at midnight. What command would you use to create a cluster called spark-nightly-analysis in the us-west2-a zone?;bq dataproc clusters create spark-nightly-analysis ––zone us-west2-a;gcloud dataproc clusters create spark-nightly-analysis ––zone us-west2-a;gcloud dataproc clusters spark-nightly-analysis ––zone us-west2-a;None of the above;;B;The correct command is gcloud dataproc clusters create followed by the name of the cluster and the a --zone parameter. Option B is correct. Option A is incorrect because bq is the command-line tool for BigQuery, not Dataproc. Option C is a gcloud command missing a verb or command, so it is incorrect. Option D is wrong because option B is the correct answer.
You have a number of buckets containing old data that is hardly ever used. You don’t want to delete it, but you want to minimize the cost of storing it. You decide to change the storage class to coldline for each of those buckets. What is the command structure that you would use?;gcloud rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT];gsutil rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT];cbt rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT];bq rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT];;B;gsutil is the correct command, so option B is correct. Option A is incorrect because gcloud commands are not used to manage Cloud Storage. Similarly, options C and D are incorrect because they use commands for Bigtable and BigQuery, respectively.
You want to rename an object stored in a bucket. What command structure would you use?;gsutil cp gs://[BUCKET_NAME]/[OLD_OBJECT_NAME] gs://[BUCKET_NAME]/ [NEW_OBJECT_NAME];gsutil mv gs://[BUCKET_NAME]/[OLD_OBJECT_NAME] gs://[BUCKET_NAME]/ [NEW_OBJECT_NAME];gsutil mv gs://[OLD_OBJECT_NAME] gs://[NEW_OBJECT_NAME];gcloud mv gs://[OLD_OBJECT_NAME] gs://[NEW_OBJECT_NAME];;B;The command in option B correctly renames an object from an old name to a new name. Option A is incorrect because it uses a cp command instead of mv. Option C does not include bucket names, so it is incorrect. Option D uses gcloud, but gsutil is the command-line tool for working with Cloud Storage.
An executive in your company emails you asking about creating a recommendation system that will help sell more products. The executive has heard there are some GCP solutions that may be good fits for this problem. What GCP service would you recommend the executive look into?;Cloud Dataproc, especially Spark and its machine learning library;Cloud Dataproc, especially Hadoop;Cloud Spanner, which is a global relational database that can hold a lot of data;Cloud SQL, because SQL is a powerful query language;;A;Dataproc with Spark and its machine learning library are ideal for this use case, so option A is correct. Option B suggests Hadoop, but it is not a good choice for machine learning applications. Option C is incorrect because Spanner is designed as a global relational database with support for transaction processing systems, not analytic and machine learning systems. Option D is incorrect. SQL is a powerful query language, but it does not support the kinds of machine learning algorithms needed to solve the proposed problem.
Which of the following commands is used to create buckets in Cloud Storage?;gcloud storage buckets create;gsutil storage buckets create;gsutil mb;gcloud mb;;C;gsutil is the command-line utility for working with Cloud Storage. It is one of the few GCP services that does not use gcloud. (BigQuery and Bigtable are others.) Option C is the correct answer because mb, short for “make bucket,” is the verb that follows gsutil to create a bucket. Options A and D are wrong because they use gcloud instead of gsutil. Option B is wrong because it uses gsutil with a command syntax used by gcloud.
You need to copy files from your local device to a bucket in Cloud Storage. What command would you use? Assume you have Cloud SDK installed on your local computer.;gsutil copy;gsutil cp;gcloud cp;gcloud storage objects copy;;B;"The correct answer is option B; gsutil is the command to copy files to Cloud Storage. Option A is incorrect; the verb is cp, not copy. Options C and D are wrong because gsutil, not gcloud, is the command-line utility for working with Cloud Storage."
You are migrating a large number of files from a local storage system to Cloud Storage. You want to use the Cloud Console instead of writing a script. Which of the following Cloud Storage operations can you perform in the console?;Upload files only;Upload folders only;Upload files and folders;Compare local files with files in the bucket using the diff command;;C;From the console, you can upload both files and folders. Options A and B are incorrect because they are missing an operation that can be performed in the console. Option D is incorrect because there is no diff operation in Cloud Console.
A software developer asks for your help exporting data from a Cloud SQL database. The developer tells you which database to export and which bucket to store the export file in, but hasn’t mentioned which file format should be used for the export file. What are the options for the export file format?;CSV and XML;CSV and JSON;JSON and SQL;CSV and SQL;;D;When exporting a database from Cloud SQL, the export file format options are CSV and SQL, which makes option D correct. Option A is incorrect because XML is not an option. Options B and C are incorrect because JSON is not an option.
A database administrator has asked for an export of a MySQL database in Cloud SQL. The database administrator will load the data into another relational database and would to do it with the least amount of work. Specifically, the loading method should not require the database administrator to define a schema. What file format would you recommend for this task?;SQL;CSV;XML;JSON;;A;Option A, SQL format, exports a database as a series of SQL data definition commands. These commands can be executed in another relational database without having to first create a schema. Option B could be used, but that would require mapping columns to columns in a schema that was created before loading the CSV, and the database administrator would like to avoid that. Options C and D are incorrect because they are not export file format options.
Which command will export a MySQL database called ace-exam-mysql1 to a file called ace-exam-mysql-export.sql in a bucket named ace-exam-buckete1?;gcloud storage export sql ace-exam-mysql1 gs://ace-exam-buckete1/aceexam-mysql-export.sql \ ––database=mysql;gcloud sql export ace-exam-mysql1 gs://ace-exam-buckete1/ace-exammysql-export.sql \ ––database=mysql;gcloud sql export sql ace-exam-mysql1 gs://ace-exam-buckete1/ace-exammysql-export.sql \ ––database=mysql;gcloud sql export sql ace-exam-mysql1 gs://ace-exam-mysql-export.sql/ ace-exam-buckete1/ \ ––database=mysql;;C;Option C is the correct command, gcloud sql export sql, indicating that the service is Cloud SQL, the operation is export, and the export file format is SQL. The filename and target bucket are correctly formed. Option A is incorrect because it references gcloud storage, not gcloud sql. Option B is incorrect because it is missing an export file format parameter. Option D is incorrect because the bucket name and filename are in the wrong order.
As part of a compliance regimen, your team is required to back up data from your Datastore database to an object storage system. Your data is stored in the default namespace. What command would you use to export the default namespace from Datastore to a bucket called ace-exam-bucket1?;"gcloud datastore export ––namespaces=""(default)"" gs://ace-exam-bucket1";"gcloud datastore export ––namespaces=""(default)"" ace-exam-bucket1";"gcloud datastore dump ––namespaces=""(default)"" gs://ace-exam-bucket1";"gcloud datastore dump ––namespaces=""(default)"" ace-exam-bucket1";;A;Option A uses the correct command, which is gcloud datastore export followed by a namespace and a bucket name. Option B is incorrect because the bucket name is missing gs://. Options C and D are incorrect because they use the command dump instead of export. The bucket name in option D is missing gs://.
As required by your company’s policy, you need to back up your Datastore database at least once per day. An auditor is questioning whether or not Datastore export is sufficient. You explain that the Datastore export command produces what outputs?;A single entity file;A metadata file;A metadata file and a folder with the data;A metadata file, an entity file, and a folder with the data;;C;"The export process creates a metadata file with information about the data exported and a folder that has the data itself, so option C is correct. Option A is incorrect because export does not produce a single file; it produces a metadata file and a folder with the data. Option B is incorrect because it does not include the data folder. Option D is incorrect because the correct answer is option C."
Which of the following file formats is not an option for an export file when exporting from BigQuery?;CSV;XML;Avro;JSON;;B;Option B is correct because XML is not an option in BigQuery’s export process. All other options are available.
Which of the following file formats is not supported when importing data into BigQuery?;CSV;Parquet;Avro;YAML;;D;"Option D is correct because YAML is not a file storage format; it used for specifying configuration data. Options A, B, and C are all supported import file types."
You have received a large data set from an Internet of Things (IoT) system. You want to use BigQuery to analyze the data. What command-line command would you use to make data available for analysis in BigQuery?;bq load ––autodetect ––source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_SOURCE];bq import ––autodetect ––source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_SOURCE];gloud BigQuery load ––autodetect ––source_format=[FORMAT] [DATASET]. [TABLE] [PATH_TO_SOURCE];gcloud BigQuery load ––autodetect ––source_format=[FORMAT] [DATASET]. [TABLE] [PATH_TO_SOURCE];;A;The correct command is bq load in option A. The autodetect and source_format parameters and path to source are correctly specified in all options. Option B is incorrect because it uses the term import instead of load. Options C and D are incorrect because they use gcloud instead of bq.
You have set up a Cloud Spanner process to export data to Cloud Storage. You notice that each time the process runs you incur charges for another GCP service, which you think is related to the export process. What other GCP service might be incurring charges during the Cloud Spanner export?;Dataproc;Dataflow;Datastore;bq;;B;"The correct answer is B because Dataflow is a pipeline service for processing streaming and batch data that implements workflows used by Cloud Spanner. Option A is incorrect; Dataproc is a managed Hadoop and Spark service, which is used for data analysis. Option C is incorrect; Datastore is a NoSQL database. Option D is incorrect because bq is used with BigQuery only."
As a developer on a project using Bigtable for an IoT application, you will need to export data from Bigtable to make some data available for analysis with another tool. What would you use to export the data, assuming you want to minimize the amount of effort required on your part?;A Java program designed for importing and exporting data from Bigtable;gcloud bigtable table export;bq bigtable table export;An import tool provided by the analysis tool;;A;"Bigtable data is exported using a compiled Java program, so option A is correct. Option B is incorrect; there is no gcloud Bigtable command. Option C is incorrect; bq is not used with Bigtable. Option D is incorrect because it does not export data from Bigtable."
You have just exported from a Dataproc cluster. What have you exported?;Data in Spark DataFrames;All tables in the Spark database;Configuration data about the cluster;All tables in the Hadoop database;;C;"Exporting from Dataproc exports data about the cluster configuration, which makes option C correct. Option A is incorrect; data in DataFrames is not exported. Option B is incorrect; Spark does not have tables for persistently storing data like relational databases. Option D is incorrect; no data from Hadoop is exported."
A team of data scientists has requested access to data stored in Bigtable so that they can train machine learning models. They explain that Bigtable does not have the features required to build machine learning models. Which of the following GCP services are they most likely to use to build machine learning models?;Datastore;Dataflow;Dataproc;DataAnalyze;;C;"The correct answer is option C; the service Dataproc supports Apache Spark, which has libraries for machine learning. Options A and B are incorrect, neither is an analysis or machine learning service. Option D, DataAnalyze, is not an actual service."
The correct command to create a Pub/Sub topic is which of the following?;gcloud pubsub topics create;gcloud pubsub create topics;bq pubsub create topics;cbt pubsub topics create;;A;"The correct command in option A uses gcloud followed by the service, in this case pubsub, followed by the resource, in this case topics; and finally the verb, in this case create. Option B is incorrect because the last two terms are out of order. Options C and D are incorrect because they do not use gcloud. bq is the command-line tool for BigQuery. cbt is the command-line tool for Bigtable."
Which of the following commands will create a subscription on the topic ace-exam-topic1?;gcloud pubsub create ––topic=ace-exam-topic1 ace-exam-sub1;gcloud pubsub subscriptions create ––topic=ace-exam-topic1;gcloud pubsub subscriptions create ––topic=ace-exam-topic1 ace-exam-sub1;gsutil pubsub subscriptions create ––topic=ace-exam-topic1 ace-exam-sub1;;C;The correct answer, option C, uses gcloud pubsub subscriptions create followed by the topic and the name of the subscription. Option A is incorrect because it is missing the term subscriptions. Option B is incorrect because it is missing the name of the subscription. Option D is incorrect because it uses gsutil instead of gcloud.
What is one of the direct advantages of using a message queue in distributed systems?;It increases security.;It decouples services, so if one lags, it does not cause other services to lag.;It supports more programming languages.;It stores messages until they are read by default.;;B;"Using a message queue between services decouples the services, so if one lags it does not cause other services to lag, which makes option B correct. Option A is incorrect because adding a message queue does not directly mitigate any security risks that might exist in the distributed system, such as overly permissive permissions. Option C is incorrect; adding a queue is not directly related to programming languages. Option D is incorrect; by default, message queues have a retention period."
To ensure you have installed beta gcloud commands, which command should you run?;gcloud components beta install;gcloud components install beta;gcloud commands install beta;gcloud commands beta install;;B;The correct answer is B, gcloud components followed by install and then beta. Option A is incorrect because beta and install are in the wrong order. Options C and D are wrong because commands is used instead of components.
What parameter is used to tell BigQuery to automatically detect the schema of a file on import?; --autodetect; --autoschema; --detectschema; --dry_run;;A;The correct parameter name is autodetect, which is option A. Options B and C are not actually valid bq parameters. Option D is a valid parameter, but it returns the estimated size of data scanned to when executing a query.
The compression options deflate and snappy are available for what file types when exporting from BigQuery?;Avro;CSV;XML;Thrift;;A;Avro supports Deflate and Snappy compression. CSV supports Gzip and no compression. XML and Thrift are not export file type options.
Virtual private clouds have a ______ scope.;Zonal;Regional;Super-regional;Global;;D;Virtual private clouds are global, so option D is correct. By default, they have subnets in all regions. Resources in any region can be accessed through the VPC. Options A, B, and C are all incorrect.
You have been tasked with defining CIDR ranges to use with a project. The project includes 2 VPCs with several subnets in each VPC. How many CIDR ranges will you need to define?;One for each VPC;One for each subnet;One for each region;One for each zone;;B;IP ranges are assigned to subnets, so option B is correct. Each subnet is assigned an IP range for its exclusive use. IP ranges are assigned network structures, not zones and regions. VPCs can have multiple subnets but each subnet has its own address range.
The legal department needs to isolate its resources on its own VPC. You want to have network provide routing to any other service available on the global network. The VPC network has not learned global routes. What parameter may have been missed when creating the VPC subnets?;DNS server policy;Dynamic routing;Static routing policy;Systemic routing policy;;B;"Option B is correct; dynamic routing is the parameter that specifies whether routes are learned regionally or globally. Option A is incorrect; DNS is a name resolution service and is not involved with routing. Option C is incorrect; there is no static routing policy parameter. Option D is incorrect because global routing is not an actual option."
The command to create a VPC from the command line is:;gcloud compute networks create;gcloud networks vpc create;gsutil networks vpc create;gcloud compute create networks;;A;"The correct answer is gcloud compute networks create, which is option A. Option B is incorrect; networks vpc is not a correct part of the command. Option C is incorrect because gsutil is the command used to work with Cloud Storage. Option D is incorrect because there is no such thing."
You have created several subnets. Most of them are sending logs to Stackdriver. One subnet is not sending logs. What option may have been misconfigured when creating the subnet that is not forwarding logs?;Flow Logs;Private IP Access;Stackdriver Logging;Variable-Length Subnet Masking;;A;The Flow Log option of the create vpc command determines whether logs are sent to Stackdriver, so option A is correct. Option B, Private IP Access, determines whether an external IP address is needed by a VM to use Google services. Option C is incorrect because Stackdriver Logging is the service, not a parameter used when creating a subnet. Option D is incorrect because variable-length subnet masking has to do with CIDR addresses, not logging.
At what levels of the resource hierarchy can a shared VPC be created?;Folders and resources;Organizations and project;Organizations and folders;Folders and subnets;;C;"Shared VPCs can be created at the organization or folder level of the resource hierarchy, so option C is correct. Options A and B are incorrect; shared VPCs are not created at the resource or project levels. Option D is incorrect; shared VPCs are not applied at subnets, which are resources in the resource hierarchy."
You are using Cloud Console to create a VM that you want to exist in a custom subnet you just created. What section of the Create Instance form would you use to specify the custom subnet?;Networking tab of the Management, Security, Disks, Networking, Sole Tenancy section;Management tab of the Management, Security, Disks, Networking, Sole Tenancy section;Sole Tenancy tab of Management, Security, Disks, Networking, Sole Tenancy;Sole Tenancy tab of Management, Security, Disks, Networking;;A;The correct answer is the Networking tab of the Management, Security, Disks, Networking, Sole Tenancy section of the form, which makes option A correct. The Management tab is not about subnet configurations. Option D is incorrect because it does not lead to Sole Tenancy options.
You want to implement interproject communication between VPCs. Which feature of VPCs would you use to implement this?;VPC peering;Interproject peering;VPN;Interconnect;;A;"VPC is used for interproject communications. Option B is incorrect; there is no interproject peering. Options C and D are incorrect; they have to do with linking on-premise networks with networks in GCP."
You want to limit traffic to a set of instances. You decide to set a specific network tag on each instance. What part of a firewall rule can reference the network tag to determine the set of instances affected by the rule?;Action;Target;Priority;Direction;;B;"The target can be all instances in a network, instances with particular network tags, or instances using a specific service account, so option B is correct. Option A is incorrect; action is either allow or deny. Option C is incorrect; priority determines which of all the matching rules is applied. Option D is incorrect; it specifies whether the rule is applied to incoming or outgoing traffic."
What part of a firewall rule determines whether a rule applies to incoming or outgoing traffic?;Action;Target;Priority;Direction;;D;"Direction specifies whether the rule is applied to incoming or outgoing traffic, which makes option D the right answer. Option A is incorrect; action is either allow or deny. Option B is incorrect; target specifies the set of instances that the rule applies to. Option C is incorrect; priority determines which of all matching rules is applied."
You want to define a CIDR range that applies to all destination addresses. What IP address would you specify?;0.0.0.0/0;10.0.0.0/8;172.16.0.0/12;192.168.0.0/16;;A;The 0.0.0.0/0 matches all IP addresses, so option A is correct. Option B represents a block of 16,777,214 addresses. Option C represents a block of 1,048,574 addresses. Option D represents a block of 65,534. You can experiment with CIDR block options using a CIDR calculator such as the one at www.subnet-calculator.com/cidr.php.
You are using gcloud to create a firewall rule. Which command would you use?;gcloud network firewall-rules create;gcloud compute firewall-rules create;gcloud network rules create;gcloud compute rules create;;B;The product you are working with is compute and the resource you are creating is a firewall rule, so option B is correct. Options A and C references network instead of compute. Option D references rules instead of firewall-rules.
You are using gcloud to create a firewall rule. Which parameter would you use to specify the subnet it should apply to?; --subnet; --network; --destination; --source-ranges;;B;"The correct parameter is network, which makes option B correct. Option A is incorrect; subnet is not a parameter to gcloud to create a firewall. Option C is incorrect; destination is not a valid parameter. Option D is incorrect; source-ranges is for specifying sources of network traffic the rule applies to."
An application development team is deploying a set of specialized service endpoints and wants to limit traffic so that only traffic going to one of the endpoints is allowed through by firewall rules. The service endpoints will accept any UDP traffic and each endpoint will use a port in the range of 20000–30000. Which of the following commands would you use?;gcloud compute firewall-rules create fwr1 --allow=udp:20000-30000 --direction=ingress;gcloud network firewall-rules create fwr1 --allow=udp:20000-30000 --direction=ingress;gcloud compute firewall-rules create fwr1 --allow=udp;gcloud compute firewall-rules create fwr1 --direction=ingress;;A;The rule in option A uses the correct gcloud command and specifies the allow and direction parameters. Option B is incorrect because it references gcloud network instead of gcloud compute. Option C is incorrect because it does not specify the port range. Option D is incorrect because it does not specify the protocol or port range.
You have a rule to allow inbound traffic to a VM. You want it to apply only if there is not another rule that would deny that traffic. What priority should you give this rule?;0;1;1000;65535;;D;Option D is correct because it is the largest number allowed in the range of values for priorities. The larger the number, the lower the priority. Having the lowest priority will ensure that other rules that match will apply.
You want to create a VPN using Cloud Console. What section of Cloud Console should you use?;Compute Engine;App Engine;Hybrid Connectivity;IAM & Admin;;C;The VPC create option is available in the Hybrid Connectivity section, so option C is correct. Compute Engine, App Engine, and IAM & Admin do not have features related to VPNs.
You are using Cloud Console to create a VPN. You want to configure the GCP end of the VPN. What section of the Create VPN form would you use?;Tunnels;Routing Options;Google Compute Engine VPN;IKE Version;;C;"The Google Compute Engine VPN is where you specify information about the Google Cloud end of the VPN connection, so option C is correct. You specify name, description, network, region, and IP address. Option A is incorrect because tunnels are about the connections between the cloud and the remote network. Option B is incorrect; Routing Options is about how to configure routers. Option D is incorrect; IKE Version is about exchanging secret keys."
You want the router on a tunnel you are creating to learn routes from all GCP regions on the network. What feature of GCP routing would you enable?;Global dynamic routing;Regional routing;VPC;Firewall rules;;A;"Option A is correct because global dynamic routing is used to learn all routes on a network. Option B is incorrect; regional routing would learn only routes in a region. Options C and D are incorrect because they are not used to configure routing options."
When you create a cloud router, what kind of unique identifier do you need to assign for the BGP protocol?;IP address;ASN;Dynamic load routing ID;None of the above;;B;"The autonomous system number (ASN) is a number used to identify a cloud router on a network, so option B is correct. IP addresses are not unique identifiers for the BGP protocol. Option C is incorrect; there is no dynamic load routing ID. Option D is incorrect because option B is correct."
You are using gcloud to create a VPN. Which command would you use?;gcloud compute target-vpn-gateways only;gcloud compute forwarding-rule and gcloud compute target-vpn-gateways only;gcloud compute vpn-tunnels only;gcloud compute forwarding-rule, gcloud compute target-vpn-gateways, and gcloud compute vpn-tunnels;;D;When using gcloud to create a VPN, you need to create forwarding rules, tunnels, and gateways, so all the gcloud commands listed would be used.
What record type is used to specify the IPv4 address of a domain?;AAAA;A;NS;SOA;;B;"The A record is used to map a domain name to an IPv4 address, so option B is correct. Option A is incorrect because the AAAA record is used for IPv6 addresses. Option C is incorrect; NS is a name server record. Option D is incorrect; SOA is a start of authority record."
The CEO of your startup just read a news report about a company that was attacked by something called cache poisoning. The CEO wants to implement additional security measures to reduce the risk of DNS spoofing and cache poisoning. What would you recommend?;Using DNSSEC;Adding SOA records;Adding CNAME records;Deleting CNAME records;;A;"DNSSEC is a secure protocol designed to prevent spoofing and cache poisoning, so option A is correct. Options B and C are incorrect because SOA and CNAME records contain data about the DNS record; they are not an additional security measure. Option D is incorrect because deleting a CNAME record does not improve security."
What do the TTL parameters specify in a DNS record?;Time a record can exist in a cache before it should be queried again;Time a client has to respond to a request for DNS information;Time allowed to create a CNAME record;Time before a human has to manually verify the information in the DNS record;;A;"The TTL parameters specify the time a record can be in a cache before the data should be queried again, so option A is correct. Option B is incorrect; this time period is not related to timeouts. Option C is incorrect; the TTLs are not related to time restriction on data change operations. Option D is not correct; there is no manual review required."
What command is used to create a DNS zone in the command line?;gsutil dns managed-zones create;gcloud beta dns managed-zones create;gcloud beta managed-zones create;gcloud beta dns create managed zones;;B;The correct answer, Option B, is gcloud beta dns managed-zones create. Option A is incorrect, it uses the gsutil command which is used to work with Cloud Storage. Option C is incorrect, it is missing the term dns. Option D is incorrect, the ordering of terms is incorrect.
What parameter is used to make a DNS zone private?;#NOM?;#NOM?;#NOM?;#NOM?;;B;"The visibility parameter is the parameter that can be set to private, so option B is correct. Option A is not a valid parameter. Option C is incorrect; private is not a parameter. Similarly, option D is incorrect; status is not a valid parameter for making a DNS zone private."
Which load balancers provide global load balancing?;HTTP(S) only;SSL Proxy and TCP Proxy only;HTTP(S), SSL Proxy, and TCP Proxy;Internal TCP/UDP, HTTP(S), SSL Proxy, and TCP Proxy;;C;The three global load balancers are HTTP(S), SSL Proxy, and TCP Proxy, so option C is correct. Options A and B are missing at least one global load balancer. Option D is incorrect because Internal TCP/UD is a regional load balancer.
Which regional load balancer allows for load balancing based on IP protocol, address, and port?;HTTP(S);SSL Proxy;TCP Proxy;Network TCP/UDP;;D;Network TCP/UDP enables balancing based on IP protocol, address, and port, so option D is correct. Options A, B, and C are all global load balancers, not regional ones.
You are configuring a load balancer and want to implement private load balancing. Which option would you select?;Only Between My VMs;Enable Private;Disable Public;Local Only;;A;In the console there is an option to select between From Internet To My VMs and Only Between My VMs. This is the option to indicate private or public, so option A is correct. Options B, C, and D are all fictitious parameters.
What two components need to be configured when creating a TCP Proxy load balancer?;Frontend and forwarding rule;Frontend and backend;Forwarding rule and backend only;Backend and forwarding rule only;;B;"TCP Proxy load balancers require you to configure both the frontend and backendthe , so option B is correct. Options A and D are incorrect because they are missing one component. Option C is incorrect; forwarding rules are the one component specified with network load balancing. There is no component known as a traffic rule."
A health check is used to check what resources?;Load balancer;VMs;Storage buckets;Persistent disks;;B;"Health checks monitor the health of VMs used with load balancers, so option B is correct. Option A is incorrect, nearline storage is a type of Cloud Storage. Option C and D are incorrect; storage devices or buckets are not health checked."
Where do you specify the ports on a TCP Proxy load balancer that should have their traffic forwarded?;Backend;Frontend;Network Services section;VPC;;B;"You specify ports to forward when configuring the frontend, so option B is correct. The backend is where you configure how traffic is routed to VMs. Option C is incorrect; Network Services is a high-level area of the console. Option D is incorrect; VPCs are not where you specify load balancer configurations."
What command is used to create a network load balancer at the command line?;gcloud compute forwarding-rules create;gcloud network forwarding-rules create;gcloud compute create forwarding-rules;gcloud network create forwarding-rules;;A;"The correct answer, option A, is gcloud compute forwarding-rules create. Option B is incorrect; the service should be compute, not network. Option C is incorrect; create comes after forwarding-rules. Option D is incorrect because it has the wrong service, and the verb is in the wrong position."
A team is setting up a web service for internal use. They want to use the same IP address for the foreseeable future. What type of IP address would you assign?;Internal;External;Static;Ephemeral;;C;"Static addresses are assigned until they are released, so option C is correct. Options A and B are incorrect because internal and external addresses determine whether traffic is routed into and out of the subnet. External addresses can have traffic reach them from the Internet; internal addresses cannot. Option D is incorrect; ephemeral addresses are released when a VM shuts down or is deleted."
You are starting up a VM to experiment with a new Python data science library. You’ll SSH via the server name into the VM, use the Python interpreter interactively for a while and then shut down the machine. What type of IP address would you assign to this VM?;Ephemeral;Static;Permanent;IPv8;;A;"An ephemeral address is sufficient, since resources outside the subnet will not need to reach the VM and you can SSH into the VM from the console, so option A is correct. Option B is incorrect because there is no need to assign a permanent address, which would then have to be released. Option C is incorrect; there is no Permanent type. Option D is incorrect; there is no IPv8 address."
You have created a subnet called sn1 using 192.168.0.0 with 65,534 addresses. You realize that you will not need that many addresses, and you’d like to reduce that number to 254. Which of the following commands would you use?;gcloud compute networks subnets expand-ip-range sn1 --prefix-length=24;gcloud compute networks subnets expand-ip-range sn1 --prefix-length=-8;gcloud compute networks subnets expand-ip-range sn1 --size=256;There is no command to reduce the number of IP addresses available.;;D;"You cannot reduce the number of addresses using any of the commands, so option D is correct. Option A is incorrect because the prefix length specified in the expand-ip-range command must be a number less than the current length. If there are 65,534 addresses, then the prefix length is 16. Option B is incorrect for the same reason, and the prefix length cannot be a negative number. Option C is incorrect; there is no ––size parameter."
You have created a subnet called sn1 using 192.168.0.0. You want it to have 14 addresses. What prefix length would you use?;32;28;20;16;;B;The prefix length specifies the length in bits of the subnet mask. The remaining bits of the IP address are used for device addresses. Since there are 32 bits in an IP address, you subtract the length of the mask to get the number of bits used to represent the address. 16 is equal to 24 , so you need 4 bits to represent 14 addresses. 32-4 is 28, so option B is the correct answer. Option A would leave 1 address, option C would provide 4,094 addresses, and option D would provide 65,534.
You want all your network traffic to route over the Google network and not traverse the public Internet. What level of network service should you choose?;Standard;Google-only;Premium;Non-Internet;;C;"Premium is the network service level that routes all traffic over the Google network, so option C is correct. Option A is incorrect; the Standard tier may use the public Internet when routing traffic. Options B and D are incorrect; there are no service tiers called Google-only or non-Internet."
You have a website hosted on a Compute Engine VM. Users can access the website using the domain name you provided. You do some maintenance work on the VM and stop the server and restart it. Now users cannot access the website. No other changes have occurred on the subnet. What might be the cause of the problem?;The restart caused a change in the DNS record.;You used an ephemeral instead of a static IP address.;You do not have enough addresses available on your subnet.;Your subnet has changed.;;B;"Stopping and starting a VM will release ephemeral IP addresses, so option B is correct. Use a static IP address to have the same IP address across reboots. Option A is incorrect; rebooting a VM does not change a DNS record. Option C is incorrect because if you had enough addresses to get an address when you first started the VM and you then released that IP address, there should be at least one IP address assuming no other devices are added to the subnet. Option D is incorrect because no other changes, including changes to the subnet, were made."
You are deploying a distributed system. Messages will be passed between Compute Engine VMs using a reliable UDP protocol. All VMs are in the same region. You want to use the load balancer that best fits these requirements. Which kind of load balancer would you use?;Internal TCP/UDP;TCP Proxy;SSL Proxy;HTTP(S);;A;Internal TCP/UDP is a good option. It is a regional load balancer that supports UDP, so option A is correct. Options B, C, and D are all global load balancers. Option B supports TCP, not UDP. Option D supports HTTP and HTTPS, not UDP.
You want to use Cloud Console to review the records in a DNS entry. What section of Cloud Console would you navigate to?;Compute Engine;Network Services;Kubernetes Engine;Hybrid Connectivity;;B;"Network Services is the section of Cloud Console that has the Cloud DNS console, so option B is correct. Option A is incorrect; Compute Engine does not have DNS management forms. Neither does option C, Kubernetes Engine. Option D is related to networking, but the services in Hybrid Connectivity are for services such as VPNs."
What are the categories of Cloud Launcher solutions?;Data sets only;Operating systems only;Developer tools and operating systems only;Data sets, operating systems, and developer tools;;D;Categories of solutions include all of the categories mentioned, so option D is correct. Others include Kubernetes Apps, API & Services, and Databases.
What is the other name of Cloud Launcher?;Cloud Deployment Manager;Marketplace;Cloud Tools;Cloud Solutions: Third Party;;B;The Cloud Launcher is also known as Marketplace, so option B is correct. Option A is incorrect because the Cloud Deployment Manager is used to create deployment templates. Options C and D are fictional names of services.
Where do you navigate to launch a Cloud Launcher solution?;Overview page of the solution;Main Cloud Launcher page;Network Services;None of the above;;A;"You launch a solution by clicking the Launch on Compute Engine link in the overview page, so option A is correct. Option B is incorrect; the main page has summary information about the products. Option C is incorrect; Network Services is unrelated to this topic. Option D is incorrect because option A is the correct answer."
You want to quickly identify the set of operating systems available in Cloud Launcher. Which of these steps would help with that?;Use Google Search to search the Web for a listing.;Use filters in Cloud Launcher.;Scroll through the list of solutions displayed on the start page of Cloud Launcher.;It is not possible to filter to operating systems.;;B;Cloud Launcher has a set of predefined filters, including filtering by operating system, so option B is correct. Option A may eventually lead to the correct information, but it is not efficient. Option D is incorrect because it is impractical for such a simple task.
You want to use Cloud Launcher to deploy a WordPress site. You notice there is more than one WordPress option. Why is that?;It’s a mistake. Submit a ticket to Google support.;Multiple vendors may offer the same application.;It’s a mistake. Submit a ticket to the vendors.;You will never see such an option.;;B;"Multiple vendors may offer configurations for the same applications, so option B is correct. This gives users the opportunity to choose the one best suited to their requirements. Options A and C are incorrect; this is a feature of Cloud Launcher. Option D is incorrect because option B is the correct answer."
You have used Cloud Launcher to deploy a WordPress site and would now like to deploy a database. You notice that the configuration form for the databases is different from the form used with WordPress. Why is that?;It’s a mistake. Submit a ticket to Google support.;You’ve navigated to a different subform of Cloud Launcher.;Configuration properties are based on the application you are deploying and will be different depending on what application you are deploying.;This cannot happen.;;C;"Cloud Launcher will display configuration options appropriate for the application you are deploying, so option C is correct. For example, when deploying WordPress, you will have the option of deploying an administration tool for PHP. Option A is incorrect; this is a feature of Cloud Launcher. Option B is incorrect; you are not necessarily on the wrong form. Option D is incorrect; this is a feature of Cloud Launcher."
You have been asked by your manager to deploy a WordPress site. You expect heavy traffic, and your manager wants to make sure the VM hosting the WordPress site has enough resources. Which resources can you configure when launching a WordPress site using Cloud Launcher?;Machine type;Disk type;Disk size;All of the above;;D;You can change the configuration of any of the items listed, so option D is correct. You can also specify firewall rules to allow both HTTP and HTTPS traffic or change the zone in which the VM runs.
You would like to define as code the configuration of a set of application resources. The GCP service for creating resources using a configuration file made up of resource specifications defined in YAML syntax is called what?;Compute Engine;Deployment Manager;Marketplace Manager;Marketplace Deployer;;B;Deployment Manager is the name of the service for creating application resources using a YAML configuration file, so option B is correct. Option A is incorrect, although you could use scripts with gcloud commands to deploy resources in Compute Engine. Options C and D are incorrect because those are fictitious names of products.
What file format is used to define Deployment Manager configuration files?;XML;JSON;CSV;YAML;;D;"Configuration files are defined in YAML syntax, so option D is correct. Options A, B, and C are all incorrect; configuration files are defined in YAML."
A Deployment Manager configuration file starts with what term?;Deploy;Resources;Properties;YAML;;B;Configuration files define resources and start with the term resources, so option B is correct. Options A, B, and C are all incorrect. Those terms do not start the configuration file.
Which of the following are used to define a resource in a Cloud Deployment Manager configuration file?;type only;properties only;name and type only;type, properties, and name;;D;All three, type, properties, and name, are used when defining resources in a Cloud Deployment Manager configuration file, so option D is correct.
What properties may be set when defining a disk on a VM?;A device name only;A Boolean indicating a boot disk and a Boolean indicating autodelete;A Boolean indicating autodelete only;A device name, a Boolean indicating a boot disk, and a Boolean indicating autodelete;;D;"All three can be set; specifically, the keys are deviceName, boot, and autodelete. Option D is correct."
You need to look up what images are available in the zone in which you want to deploy a VM. What command would you use?;gcloud compute images list;gcloud images list;gsutil compute images list;gcloud compute list images;;A;"Option A is the correct command. Option B is incorrect; it is missing the term compute. Option C is incorrect; gsutil is the command for working with Cloud Storage. Option D is incorrect because the terms list and images are in the wrong order."
You want to use a template file with Deployment Manager. You expect the file to be complicated. What language would you use?;Jinja2;Ruby;Go;Python;;D;"Google recommends using Python for complicated templates, so option D is correct. Option A is incorrect because Jinja2 is recommended only for simple templates. Options B and C are incorrect; neither language is supported for templates."
What command launches a deployment?;gcloud deployment-manager deployments create;gcloud cloud-launcher deployments create;gcloud deployment-manager deployments launch;gcloud cloud-launcher deployments launch;;A;"The correct answer is gcloud deployment-manager deployments create, so option A is correct. Options B and D are incorrect; the service is not called cloud-launcher in the command. Option C is incorrect; launch is not a valid verb for this command."
A DevOps engineer is noticing a spike in CPU utilization on your servers. You explain you have just launched a deployment. You’d like to show the DevOps engineer the details of a deployment you just launched. What command would you use?;gcloud cloud-launcher deployments describe;gcloud deployment-manage deployments list;gcloud deployment-manager deployments describe;gcloud cloud-launcher deployments list;;C;"The correct answer is gcloud deployment-manager deployments describe, so option C is correct. Options A and D are incorrect; cloud-launcher is not the name of the service. Option B is incorrect; list displays a brief summary of each deployment. describe displays a detailed description."
If you expand the More link in the Networking section when deploying a Cloud Launcher solution, what will you be able to configure?;IP addresses;Billing;Access controls;Custom machine type;;A;You will be able to configure IP addresses, so option A is correct. You cannot configure billing or access controls in Deployment Manager, so options B and C are incorrect. You can configure the machine type, but that is not the More section of Networking.
What are the license types referenced in Cloud Launcher?;Free only;Free and Paid only;Free and Bring your own license (BYOL) only;Free, paid, and bring your own license;;D;The correct answer is option D because free, paid, and BYOL are all license options used in Cloud Launcher.
Which license type will add charges to your GCP bill when using Cloud Launcher with this type of license?;Free;Paid;BYOL;Chargeback;;B;The paid license types include payment for the license in your GCP charges, so option B is correct. The free license type does not incur charges. The BYOL license type requires you to work with the software vendor to get and pay for a license. There is no such license type as chargeback, so option D is incorrect.
You are deploying a Cloud Launcher application that includes a LAMP stack. What software will this deploy?;Apache server and Linux only;Linux only;MySQL and Apache only;Apache, MySQL, Linux, and PHP;;D;LAMP is short for Linux, Apache, MySQL, and PHP. All are included when installing LAMP solutions, so option D is correct.
What does IAM stand for?;Identity and Authorization Management;Identity and Access Management;Identity and Auditing Management;Individual Access Management;;B;"IAM stands for Identity and Access Management, so option B is correct. Option A is incorrect; the A does not stand for authorization, although that is related. Option C is incorrect; the A does not stand for auditing, although that is related. Option D is incorrect. IAM also works with groups, not just individuals."
When you navigate to IAM & Admin in Cloud Console, what appears in the main body of the page?;Members and roles assigned;Roles only;Members only;Roles and permissions assigned;;A;"Members and their roles are listed, so option A is correct. Options B and C are incorrect because they are missing the other main piece of information provided in the listing. Option D is incorrect; permissions are not displayed on that page."
Why are primitive roles classified in a category in addition to IAM?;They are part of IAM.;They were created before IAM.;They were created after IAM.;They are not related to access control.;;B;"Primitive roles were created before IAM and provided coarse-grained access controls, so option B is correct. Option A is incorrect; they are used for access control. Option C is incorrect; IAM is the newer form of access control. Option D is incorrect; they do provide access control functionality."
A developer intern is confused about what roles are used for. You describe IAM roles as a collection of what?;Identities;Permissions;Access control lists;Audit logs;;B;"Roles are used to group permissions that can then be assigned to identities, so option B is correct. Option A is incorrect; roles do not have identities, but identities can be granted roles. Option C is incorrect; roles do not use access control lists. Option D is incorrect; roles do not include audit logs. Logs are collected and managed by Stackdriver Logging."
You want to list roles assigned to users in a project called ace-exam-project. What gcloud command would you use?;gcloud iam get-iam-policy ace-exam-project;gcloud projects list ace-exam-project;gcloud projects get-iam-policy ace-exam-project;gcloud iam list ace-exam-project;;C;"The correct answer is gcloud projects get-iam-policy ace-exam-project, so option C is correct. Option A is incorrect because the resource should be projects and not iam. Option B is incorrect; list does not provide detailed descriptions. Option D is incorrect because iam and list are incorrectly referenced."
You are working in the form displayed after clicking the Add link in the IAM form of IAM & Admin in Cloud Console. There is a parameter called New Members. What items would you enter in that parameter?;Individual users only;Individual users or groups;Roles or individual users;Roles or groups;;B;"New members can be users, indicated by their email addresses, or groups, so option B is correct. Option A is incorrect; it does not include groups. Options C and D are incorrect because roles are not added there."
You have been assigned the App Engine Deployer role. What operations can you perform?;Write new versions of an application only;Read application configuration and settings only;Read application configuration and settings and write new configurations;Read application configuration and settings and write new versions;;D;Deployers can read application configurations and settings and write new application versions, so option D is correct. Option A is incorrect because it is missing the ability to read configurations and settings. Option B is incorrect because it is missing writing new versions. Option C is incorrect because it references writing new configurations.
You want to list permissions in a role using Cloud Console. Where would you go to see that?;"IAM & Admin; select Roles. All permissions will be displayed.";"IAM & Admin; select Roles. Check the box next to a role to display the permissions in that role.";"IAM & Admin; select Audit Logs.";"IAM & Admin; select Service Accounts and then Roles.";;B;"The correct steps are navigating to IAM & Admin, selecting Roles, and then checking the box next to a role, so option B is correct. Option A is incorrect; all roles are not displayed automatically. Option C is incorrect; audit logs do not display permissions. Option D is incorrect; there is no Roles option in Service Accounts."
You are meeting with an autidor to discuss security practices in the cloud. The auditor asks how you implement several best practices. You describe how IAM predefined roles help to implement which security best practice(s)?;Least privilege;Separation of duties;Defense in depth;Options A and B;;D;Predefined roles help implement both least privilege and separation of duties, so option D is correct. Predefined roles do not implement defense in depth by themselves but could be used with other security controls to implement defense in depth.
What launch stages are available when creating custom roles?;Alpha and beta only;General availability only;Disabled only;Alpha, beta, general availability, and disabled;;D;The four launch stages available are alpha, beta, general availability, and disabled, so option D is correct.
The gcloud command to create a custom role is what?;gcloud project roles create;gcloud iam roles create;gcloud project create roles;gcloud iam create roles;;B;The correct answer, option B, is gcloud iam roles create. Option A is incorrect because it references project instead of iam. Option C is incorrect because it references project instead of iam, and the terms create and roles are out of order. Option D is incorrect because the terms create and roles are out of order.
A DevOps engineer is confused about the purpose of scopes. Scopes are access controls that are applied to what kind of resources?;Storage buckets;VM instances;Persistent disks;Subnets;;B;"Scopes are permissions granted to VM instances, so option B is correct. Scopes in combination with IAM roles assigned to service accounts assigned to the VM instance determine what operations the VM instance can perform. Options A and C are incorrect; scopes do not apply to storage resources. Option D is incorrect; scopes do not apply to subnets."
A scope is identified using what kind of identifier?;A randomly generated ID;A URL beginning with https://www.googleserviceaccounts/;A URL beginning with https://www.googleapis.com/auth/;A URL beginning with https://www.googleapis.com/auth/PROJECT_ID];;C;"Scope identifiers start with https://www.googleapis.com/auth/ and are followed by a scope-specific name, such as devstorage.read_only or logging.write, so option C is correct. Option A is incorrect; scope IDs are not randomly generated. Option B is incorrect; the domain name is not googleserviceaccounts. Option D is incorrect; scopes are not linked directly to projects."
A VM instance is trying to read from a Cloud Storage bucket. Reading the bucket is allowed by IAM roles granted to the service account of the VM. Reading buckets is denied by the scopes assigned to the VM. What will happen if the VM tries to read from the bucket?;The application performing the read will skip over the read operation.;The read will execute because the most permissive permission is allowed.;The read will not execute because both scopes and IAM roles are applied to determine what operations can be performed.;The read operation will succeed, but a message will be logged to Stackdriver Logging.;;C;"Both scopes and IAM roles assigned to service accounts must allow an operation for it to succeed, so option C is correct. Option A is incorrect; access controls do not affect the flow of control in applications unless explicitly coded for that. Option B is incorrect; the most permissive permission is not used. Option D is incorrect; the operation will not succeed."
What are the options for setting scopes in a VM?;Allow Default Access and Allow Full Access only;Allow Default Access, Allow Full Access, and Set Access for Each API;Allow Full Access or Set Access For Each API only;Allow Default Access and Set Access For Each API only;;B;"The options for setting scopes are: Allow Default Access, Allow Full Access, and Set Access For Each API, so option B is correct. Option A is incorrect; it is missing Set Access For Each API. Option C is incorrect; it is missing Allow Default Access. Option D is incorrect; it is missing Allow Full Access."
What gcloud command would you use to set scopes?;gcloud compute instances set-scopes;gcloud compute instances set-service-account;gcloud compute service-accounts set-scopes;gcloud compute service-accounts define-scopes;;B;"The correct command is gcloud compute instances set-service-account, so option B is correct. Option A is incorrect; there is no set-scopes command verb. Option C is incorrect; the command verb is not set-scopes. Option D is incorrect; there is no command verb define-scopes."
What gcloud command would you use to assign a service account when creating a VM?;gcloud compute instances create [INSTANCE_NAME] --service-account [SERVICE_ACCOUNT_EMAIL];gcloud compute instances create-service-account [INSTANCE_NAME][SERVICE_ACCOUNT_EMAIL];gcloud compute instances define-service-account [INSTANCE_NAME][SERVICE_ACCOUNT_EMAIL];gcloud compute create instances-service-account [INSTANCE_NAME][SERVICE_ACCOUNT_EMAIL];;A;"You can assign a service account when creating a VM using the create command. Option B is incorrect; there is no create-service-account command verb. Option C is incorrect; there is no define-service-account command verb. Option D is incorrect; there is no instances-service-account command; also, create should come at the end of the command."
What GCP service is used to view audit logs?;Compute Engine;Cloud Storage;Stackdriver Logging;Custom logging;;C;"Stackdriver Logging collects, stores, and displays log messages, so option C is correct. Option A is incorrect; Compute Engine does not manage logs. Option B is incorrect; Cloud Storage is not used to view logs, although log files can be stored there. Option D is incorrect; custom logging solutions are not GCP services."
What options are available for filtering log messages when viewing audit logs?;Period time and log level only;Resource, type of log, log level, and period of time only;Resource and period of time only;Type of log only;;B;Logs can be filtered by resource, type of logs, log level, and period of time only, so option B is correct. Options A, C, and D are incorrect because they are missing at least one option.
An auditor needs to review audit logs. You assign read-only permission to a custom role you create for auditors. What security best practice are you following?;Defense in depth;Least privilege;Separation of duties;Vulnerability scanning;;B;"This is an example of assigning the least privilege required to perform a task, so option B is correct. Option A is incorrect; defense in depth combines multiple security controls. Option C is incorrect because it is having different people perform sensitive tasks. Option D is incorrect; vulnerability scanning is a security measure applied to applications that helps reveal potential vulnerabilities in an application that an attacker could exploit."
What Stackdriver service is used to generate alerts when the CPU utilization of a VM exceeds 80 percent?;Logging;Monitoring;Cloud Trace;Cloud Debug;;B;"The Monitoring service is used to set a threshold on metrics and generate alerts when a metric exceeds the threshold for a specified period of time, so option B is correct. Option A is incorrect; Logging is for collecting logged events. Option C is incorrect; Cloud Trace is for application tracing. Option D is incorrect; Debug is used to debug applications."
You have just created a virtual machine, and you’d like Stackdriver Monitoring to alert you via email whenever the CPU average utilization exceeds 75 percent for 5 minutes. What do you need to do to the VM to have this happen?;Install a Stackdriver workspace;Install the Stackdriver monitoring agent on the VM;Edit the VM configuration in Cloud Console and check the Monitor With Stackdriver checkbox;Set a notification channel;;B;"You must install the monitoring agent on the VM. The agent will collect data and send it to Stackdriver, so option B is correct. Option A is incorrect because a Workspace is not installed on a VM; it is created in Stackdriver. Option C is incorrect; there is no Monitor With Stackdriver check box in the VM configuration form. Option D is incorrect because you set notification channels in Stackdriver, not on a VM."
Stackdriver can be used to monitor resources where?;In Google Cloud Platform only;In Google Cloud Platform and Amazon Web Services only;In Google Cloud Platform and on premises data centers;In Google Cloud Platform, Amazon Web Services, and on premises data centers;;D;Stackdriver can monitor resources in GCP, AWS, and in on-premise data centers, so option D is correct. Options A through C are incorrect because they do not include two other correct options.
Grouping a set of metrics that arrive in a period of time into regular-sized buckets is called what?;Aggregation;Alignment;Minimization;Consolidation;;B;"Aligning is the process of separating data points into regular buckets, so option B is correct. Option A is incorrect; aggregation is used to combine data points using a statistic, such as mean. Options C and D are incorrect; they are not processes related to processing streams of metric data."
You have created a condition of CPU utilization, and you want to receive notifications. Which of the following are options?;Email only;PagerDuty only;Hipchat and PagerDuty;Email, PagerDuty, and Hipchat;;D;All three options are valid notification channels in Stackdriver Monitoring, so option D is correct. PagerDuty and HipChat are popular DevOps tools.
When you create a policy to notify you of a potential problem with your infrastructure, you can specify optional documentation. Why would you bother putting documentation in that form?;It is saved to Cloud Storage for future use.;It can help you or a colleague understand the purpose of the policy.;It can contain information that would help someone diagnose and correct the problem.;Options B and C.;;D;"The documentation is useful for documenting the purpose of the policy and for providing guidance for solving the problem, so option D is correct. Option A is incorrect; where a policy is stored is irrelevant to its usefulness. Options B and C alone are partially correct, but option D is a better answer."
What is alert fatigue, and why is it a problem?;Too many alert notifications are sent for events that do not require human intervention, and eventually DevOps engineers begin to pay less attention to notifications.;Too many alerts put unnecessary load on your systems.;Too few alerts leave DevOps engineers uncertain of the state of your applications and infrastructure.;Too many false alerts;;A;Alert fatigue is a state caused by too many alert notifications being sent for events that do not require human intervention, so option A is correct. This creates the risk that eventually DevOps engineers will begin to pay less attention to notifications. Option B is incorrect, although it is conceivable that too many alerts could adversely impact performance, but that is not likely. Option C is a potential problem, too, but that is not alert fatigue. Option D is incorrect because too many true alerts contribute to alert fatigue.
How long is log data stored in Stackdriver Logging?;7 days;15 days;30 days;60 days;;C;Stackdriver Logging stores log entries for 30 days, so option C is correct.
You need to store log entries for a longer period of time than Stackdriver Logging retains them. What is the best option for preserving log data?;"There is no option; once the data retention period passes, Stackdriver Logging deletes the data.";Create a log sink and export the log data using Stackdriver Logging’s export functionality.;Write a Python script to use the Stackdriver API to write the data to Cloud Storage.;Write a Python script to use the Stackdriver API to write the data to BigQuery.;;B;"The best option is to use Stackdriver Logging’s export functionality to write log data to a log sink, so option B is correct. Option A is incorrect; there is a way to export data. Options C and D are incorrect because writing a custom script would take more time to develop and maintain than using Logging’s export functionality."
Which of the following are options for logging sinks?;Cloud Storage bucket only;BigQuery dataset and Cloud Storage bucket only;Cloud Pub/Sub topic only;Cloud Storage bucket, BigQuery dataset, and Cloud Pub/Sub topic;;D;All three, Cloud Storage buckets, BigQuery data sets, and Cloud Pub/Sub topics, are available as sinks for logging exports, so option D is correct.
Which of the following can be used to filter log entries when viewing logs in Stackdriver Logging?;Label or text search only;Resource type and log type only;Time and resource type only;Label or text search, resource type, log type, and time;;D;All of the options listed can be used to filter, so option D is correct. Log level is another option as well.
Which of the following is not a standard log level that can be used to filter log viewings?;Critical;Halted;Warning;Info;;B;The correct answer, option B, is halted. There is no such standard log level status. Statuses include Critical, Error, Warning, Info, and Debug.
You are viewing log entries and spot one that looks suspicious. You are not familiar with the kind of log entry, and you want to see the complete details of the log entry as quickly as possible. What would you do?;Drill down one by one into each structure in the log entry.;Click Expand all to show all details.;Write a Python script to reformat the log entry.;Click the Show Detail link next to the log entry.;;B;"The fastest way to see the details is to expand all levels of structured data in the entry, so option B is correct. Option A would show the details, but it is not the fastest way. Option C is more time-consuming than using the functionality built into Stackdriver Logging. Option D is incorrect; there is no such link."
What Stackdriver service is best for identifying where bottlenecks exist in your application?;Monitoring;Logging;Trace;Debug;;C;"Cloud Trace is a distributed tracing application that provides details on how long different parts of code run, so option C is correct. Option A is incorrect; monitoring is used to notify DevOps engineers when resources are not functioning as expected. Option B is incorrect; Logging is for collecting, storing, and viewing log data, and although log entries might help diagnose bottlenecks, it is not specifically designed for that. Option D is incorrect; Debug is used to generate snapshots and inject logpoints."
There is a bug in a microservice. You have reviewed application outputs but cannot identify the problem. You decide you need to step through the code. What Stackdriver service would you use to give you insight into the status of the services at particular points in execution?;Monitoring;Logging;Trace;Debug;;D;"Debug is used to generate snapshots that provide a view of the status of an application at a particular point in its execution, so option D is correct. Option A is incorrect; monitoring is used to notify DevOps engineers when resources are not functioning as expected. Option B is incorrect; Logging is for collecting, storing, and viewing log data. Option C is incorrect because Cloud Trace is a distributed tracing application that provides details on how long different parts of code run."
You believe there may be a problem with BigQuery in the us-central zone. Where would you go to check the status of the BigQuery service for the quickest access to details?;Email Google Cloud Support.;Check https://status.cloud.google.com/.;Check https://bigquery.status.cloud.google.com/.;Call Google tech support.;;B;The Google Cloud Status Dashboard at https://status.cloud.google.com/ has information on the status of GCP services, so option B is correct. Options A and B might lead to information, but they would take longer. Option C is not a link to a source of information on BigQuery.
You would like to estimate the cost of GCP resources you will be using. Which services would require you to have information on the virtual machines you will be using?;Compute Engine and BigQuery;Compute Engine and Kubernetes Engine;BigQuery and Kubernetes Engine;BigQuery and Cloud Pub/Sub;;B;Both Compute Engine and Kubernetes Engine will require details about the VMs’ configurations, so option B is correct. The other options are incorrect because BigQuery and Cloud Pub/Sub are serverless services.
You are generating an estimate of the cost of using BigQuery. One of the parameters is Query Pricing. You have to specify a value in TB units. What is the value you are specifying?;The amount of data stored in BigQuery;The amount of data returned by the query;The amount of data scanned by the query;The amount of data in Cloud Storage bucket;;C;"Query pricing in BigQuery is based on the amount of data scanned, so option C is correct. Option A is incorrect; the amount of data storage is specified in the Storage Pricing section. Option B is incorrect; query pricing is not based on the volume of data returned. Option D is incorrect because this is not related to Cloud Storage. Option D is incorrect because option C is correct."
Why do you need to specify the operating system to be used when estimating the cost of a VM?;All operating systems are charged a fixed rate.;Some operating systems incur a cost.;"It’s not necessary; it is only included for documentation.";To estimate the cost of Bring Your Own License configurations.;;B;"Some operating systems, like Microsoft Windows Server, require a license, so option B is correct. Google sometimes has arrangements with vendors to collect fees for using proprietary software. Option A is incorrect; there is no fixed rate charge for operating systems. Option C is incorrect; the information is sometimes needed to compute charges. Option D is incorrect because if you Bring Your Own License, there is no additional license charge."
You want to create a custom metric for use in Stackdriver Monitoring but do not want to use the low-level Stackdriver API. What is an alternative open source option for working with custom metrics?;Prometheus;OpenCensus;Grafana;Nagios;;B;"OpenCensus is a library for developing custom metrics that can be used with Stackdriver Logging, so option B is correct. Option A is incorrect; Prometheus is an open source monitoring tool, but it is not used to define custom metrics in Stackdriver Monitoring. Option C is incorrect; Grafana is a visualization tools for Prometheus. Option D is incorrect; Nagios is an open source monitoring and alerting service, but it is not used for defining custom metrics in Stackdriver Logging."
"Company A has developed a web application that uses Docker. As a developer, you use Google Container Registry to manage your container images. Now want to get a list of metadata detailing the existing
Which command do you use? container image.";gcloud container show images;gcloud container list images;gcloud container metadata list;gcloud container images list;;D;"Option 4 is the correct answer. Run the command `gcloud container images list --repository [repository name]` to list the metadata about existing container images in the specified repository. The repository you specify must be hosted by Google Container Registry.
Option 1 is incorrect. gcloud container show images is not a valid command. Run list instead of show.
Option 2 is incorrect. gcloud container list images is not a valid command. list must be written after images.
Option 3 is incorrect. gcloud container metadata list is not a valid command. You can display the metadata for an image by writing images list instead of metadata.
[reference] https://cloud.google.com/sdk/gcloud/reference/container/images/list"
"You have created a new project in Google Cloud for a new application development project.
What role is automatically assigned when creating a new project?";roles/viewer;roles/editor;roles/publisher;roles/owner;;D;"Option 4 is the correct answer. Roles / owner is automatically assigned when you create a new project. This is a role that grants project owner rights. The user with this role has all editorial rights to the project as the owner of the project. In addition, you have the authority to manage the project and the permissions and roles of all resources within the project, as well as the authority to set billing information for the project. Basically, the user who created the project can use the project as the project owner, but it is also possible to additionally grant permissions to other users.
Option 1 is incorrect. roles / viewer is the authority as a viewer. Grants the required permissions for state-independent read-only actions, such as viewing existing resources and data (but not modifiable).
Option 2 is incorrect. roles / editor is the authority as an editor. You can perform actions that change state in addition to all viewer permissions.
Option 3 is incorrect. There is no role called roles / publisher.
[reference] https://cloud.google.com/resource-manager/docs/access-control-proj"
"A developer is a health-tech venture company developing a health management app hosted on Google Cloud. This application uploads 3TB of user's health data to cloud storage every day. This data is collected as one file and is uploaded daily for each user. Depending on the number of users, the number of files can reach hundreds of thousands, and the file name is in a format in which a user's ID is assigned as a date. The problem is that the upload speed of these files is slower than expected.
What is the most likely cause of this?";Cloud Storage fast uploads have been disabled.;Multipart upload is not used.;You may have created a hotspot when writting data to Cloud Storage.;Since encryption is enabled, the load on the upload process has increased;;C;"Option 3 is the correct answer. The data stored in the storage is written to the server in the cloud storage system based on the file name. In this case, similar file names are being written to the same server. Uploading a large number of files with similar names, such as assigning a user ID as a date, will concentrate the workload on a single server and reduce performance. This is called a hotspot. When writing data to Cloud Storage, you may be concentrating on the same server, causing hotspots.
Option 1 is incorrect. There is no fast upload feature in Cloud Storage.
Option 2 is incorrect. Multipart upload is an upload function used when uploading a large file. This makes it easier to upload data with large file sizes, but it is not a reason for upload delays for data with normal file sizes.
Option 4 is incorrect. One of the possible causes is the overhead of the encryption process. However, this is less of an impact than the concentration of workload on a single server due to filename issues, so it is not the main cause.
[reference] https://cloud.google.com/storage/docs/best-practices"
"Your organization plans to migrate its financial transaction monitoring application to Google Cloud. Auditors need to view the data and run reports in BigQuery, but they are not allowed to perform transactions in the application.
You are leading the migration and want the simplest solution that will require the least amount of maintenance. What should you do?";Assign roles/bigquery.dataViewer to the individual auditors.;Create a group for auditors and assign roles/viewer to them.;Create a groupe for auditors, and assign roles/bigquery.dataViewer to them.;Assigne a custom role to each auditor that allows view-only access to BigQuery.;;C;"Option 3 is correct because it uses a predefined role to provide view access to BigQuery for the group of auditors. Auditors can be added or deleted from the group if job responsibilities change.
Option 1 is not correct because Google recommended practice is to assign IAM roles to groups, not individuals. Groups are easier to manage individual users and they provide high level visibility into roles and permissions.
Option 2 is not correct because it uses a basic role to give auditors view access to all resources on the project.
Option 4 is not correct because using a predefined role can accomplish the goal and requires less maintenance.
[reference] https://cloud.google.com/iam/docs/understanding-roles"
"You are implementing infrastructure management using Google Cloud. To comply with company security regulations, you need to manage your data encryption keys.
What is the initial encryption support required to perform management using Cloud Key Management Services?";Grant a role that can use the Google Cloud Key Management Service(KMS) API.;Enable the Google Cloud Key Management Service(KMS) API and apply it to your project.;Enable the Google Cloud Key Management Service(KMS) API and set up billing.;Enable the Google Cloud Key Management Service(KLS) API and set it in your project.;;C;"Option 3 is the correct answer. Cloud Key Management Service is a single centralized cloud service. You can use it to create, import, and manage encryption keys and perform encryption operations. To set up encryption with Cloud Key Management Service, enable the Google Cloud Key Management Service (KMS) API and implement billing settings. This allows you to perform encryption. Clients can access the Cloud Key Management Service via the REST API.
[reference]  https://cloud.google.com/kms/docs/accessing-the-api"
Development engineers are developing applications that utilize App Engine. This engineer has been assigned the App Engine Deployer role as a development privilege. What does this privilege allow?;Allows reading the application configuration/settings and removing the version.;Allows reading the application configuration/settings and deployment.;Allows creating a new version of your application and changing its configuration and settings.;Allows updating a new version of your application.;;A;"Option 1 is the correct answer. roles / appengine.deployer has the following execution permissions:
--List all services / versions / instances
--View all settings for application / service / version / instance
--Display runtime indicators such as resource usage / load information / error information
--Create, list  and Delete version Therefore, by granting roles / appengine.deployer, you can read the application configuration and setting information and remove the version.
Option 2 is incorrect. The roles / appengine.deployer permissions do not have permission to deploy the new version. In order to deploy the new version, you also need to grant roles such as service account user.
Option 3 is incorrect. You cannot change the application's configuration or settings with the roles / appengine.deployer permissions.
Option 4 is incorrect. You cannot update the  version of your application with the roles / appengine.deployer permissions.
[reference] https://cloud.google.com/appengine/docs/standard/php-gen2/roles
"
"You are using the Compute Engine to set up the VM. It is necessary to set firewall rules for access control.
Select the wrong setting element for the firewall rule.";VPC ID;protocol;Action when matching;Traffic direction;;A;"Option 1 is the correct answer. There is no VPC ID parameter in the firewall rule. Firewall rules can set the direction of traffic, the action to take when matched, the protocol, and so on. This allows communication of a specific protocol to a specific target.
Each firewall rule consists of the following configuration components:
■ Direction of connection Ingress (inward) rules apply to inbound connections from the specified source to your Google Cloud target. Egress (outward) rules apply to connections from the target to the specified destination.
■ Numerical priority Used to determine if the rule applies. Only the highest priority (lowest priority number) rule with other components matching the traffic applies. Low priority, conflicting rules are ignored.
■ Action when matching (allow or deny) The rule determines whether the connection is allowed or blocked.
■ Firewall rule application status You can enable or disable the firewall rule without deleting it.
■ Target Defines the instance to which the rule applies (GKE cluster, App Engine flexible environment instance, and so on).
■ Source filter for up (inward) rules or destination filter for down (outward) rules.
■ Protocols (TCP, UDP, ICMP, etc.) and ports.
■ Boolean log options Log the connections that match the rule in Cloud Logging.
[reference] https://cloud.google.com/vpc/docs/firewalls
"
"You are building a large system hosted on a large number of VMs. A total of 900 IP addresses are required for this system configuration.
Which is the best CIDR that can minimize unused IP addresses?";/21;/22;/23;/24;;B;"Option 2 is the correct answer. / 22 will generate 1,022 available IP addresses. This means that 1000 IP addresses are available and you can minimize unused IP addresses.
Option 1 is incorrect. / 21 will generate 2048 available IP addresses. This is too many IP addresses to minimize unused IP addresses.
Option 3 is incorrect. / 23 will generate 512 available IP addresses, so you will run out of IP addresses.
Option 4 is incorrect. / 24 will generate 256 available IP addresses, so you will run out of IP addresses."
"You have decided to upload 50GB of data to your Nearline Storage bucket over a 1Gbps WAN connection.
What is the best upload method for transferring files quickly?";Use gsutil to enable parallel composite uploads.;Transfer files using the console.;Use gcloud for file transfer to enable parallel composite uploads.;Set up file transfer using the console to enable parallel composite uploads.;;A;"Option 1 is the correct answer. Parallel composite upload is one way to upload large files to Cloud Storage. With this upload method, a file is split into up to 32 chunks and uploaded in parallel to a temporary object. Temporary objects are deleted when the final object is created from these objects.
Python users who download objects must be able to use google-crc32c or crcmod to use parallel composite uploads. Or download the object but gsutil requires the user to be able to use crcmod. Therefore, using gsutil to enable parallel composite uploads is the correct answer.
Options 2 and 4 are incorrect. You cannot enable parallel composite upload in console operation. Command operation by CLI is required.
Option 3 is incorrect. The correct answer is to use the gsutil command instead of the gcloud command to enable parallel composite uploads.
[reference] https://cloud.google.com/storage/docs/uploads-downloads#parallel-composite-uploads"
A load balancer is configured for multiple Compute Engines on Google Cloud to distribute the load using the HTTP (S) protocol. Which load balancer settings route requests to different backend services depending on the URL specified by the user?;port number;IP address;URL map;Route number;;C;"Option 3 is the correct answer. Use a URL map to set up a load balancer to route requests to different backend services depending on the URL specified by the user. URL maps for load balancers allow requests to be routed to different backend services, depending on the URL specified by the user. URL maps can perform L7-based traffic distribution processing based on URLs.  The Google Cloud HTTP (S) load balancer and Traffic Director use Google Cloud configuration resources called ""URL maps"" to route requests to back-end services or back-end buckets. An external HTTP (S) load balancer can use a single URL map to route requests to different destinations based on the rules configured in the URL map.
https://example.com/video
Requests are sent to one backend service.
https://example.com/audio
Request is sent to another backend service.
https://example.com/images
Requests are sent to your Cloud Storage backend bucket. Requests for other host and path combinations are sent to the default backend service. There are also two types of URL map resources: global and region. The type of resource you use depends on your product's load balancing scheme.
Option 1 is incorrect. For the port number, communication permission can be set according to the protocol according to the port number, but routing according to the URL cannot be performed.
Option 2 is incorrect. The IP address is used to specify the traffic distribution of the load balancer, but this alone cannot be used for routing according to the URL.
Option 4 is incorrect. There is no setting that uses the route number in the setting of the load balancer.
[reference]  https://cloud.google.com/load-balancing/docs/url-map"
"You are building an in-house data sharing system that uses Google Cloud cloud storage. You need to choose cost-effective storage and protect your data with encryption. These saved files are rarely used, about once every 30 days on average.
Which is the best storage option?";Cold line storage;Archive storage;Multi-region storage;Nearline storage;;D;"Option 4 is the correct answer. Nearline storage is a low-cost, durable storage service for storing frequently accessed data. It is slightly less available than other storage classes and has a minimum data retention period of 30 days. Nearline storage is the best choice over standard storage if you want to reduce storage costs when storing frequently accessed data.
Option 1 is incorrect. Cold line storage is suitable for objects that are accessed up to once a year. It's more cost-effective than nearline storage, but it's not suitable for frequently accessed data.
Option 2 is incorrect. Archive storage is the lowest cost and most durable storage service for data archiving, online backup and disaster recovery. It's more cost-effective than nearline storage, but it's not suitable for frequently accessed data.
Option 3 is incorrect. Multi-region storage is only available for objects stored in multi-regions or dual regions and is ideal for frequently accessed data (“hot” data) or data that is stored for a short period of time. This is less than optimal as it costs more than nearline storage.
[reference] https://cloud.google.com/storage/docs/storage-classes"
"You development team is developing multiple projects on Google Cloud. As an operations manager, you need to get metadata about each project.
Which command do you use?";gcloud projects describe <PROJECT_ID>;gcloud describe projects <PROJECT_ID>;gcloud list projects <PROJECT_ID>;gcloud projects list <PROJECT_ID>;;A;"Option 1 is the correct answer. You can get the metadata for the specified project by running the command `gcloud projects describe [project ID]`. Run the command with a specific project in gcloud projects. The describe command is an action that retrieves metadata.
Option 2 is incorrect. The command of gcloud describe projects is incorrect. To be correct, it is necessary to describe the commands in the order of gcloud projects describe.
Option 3 is incorrect. The command for gcloud list projects is incorrect. To be correct, write the commands in the order of gcloud projects list.
Option 4 is incorrect. The command `gcloud projects list` is a command to get a list of projects. You cannot get the metadata for a particular project.
[reference] https://cloud.google.com/sdk/gcloud/reference/projects/describe"
"Your company is migrating an on premises archive of files to Google Cloud. The archived files are infrequently used but on average about once every 30 days.
You would like to minimize the cost of storage.
What storage option would you recommend?";Nearline Storage;Multi-regional storage;Coldline Storage;Persistent Disks;;A;"Nearline Storage is a class of Cloud Storage designed for objects that will be accessed at most once every 30 days. Coldline Storage is suitable for objects accessed at most once per year. Multi-regional storage is best suited for objects that should have low latency access from multiple regions. Persistent disks should not be used for archival storage. For more information, see https://cloud.google.com/storage/docs/storage-classes.
[Reference]https://cloud.google.com/billing/docs/how-to/manage-billing-account"
You are developing an application using App Engine. Which command do you run to prepare a gcloud component that contains an extension for Python on AppEngine?;gcloud init app-engine install python;gcloud install components app-engine-python;gcloud components list app-engine-python;gcloud componets install app-engine-python;;D;"Option 4 is the correct answer. You can run the command `gcloud components install app-engine-python` to install the gcloud components that include the AppEngine extension for Python 3. gcloud components are commands used to perform actions such as listing, installing, updating, or removing Google Cloud SDK components. You can use ""install"" to install the specified component and check the installation status. App Engine extensions are required to deploy Python applications on App Engine.
Option 1 is incorrect. This is not a valid command. You need to use gcloud components.
Option 2 is incorrect. This is not a valid command. You need to use gcloud components.
Option 3 is incorrect. gcloud components list app-engine-python can list components. This is just a display, so use install to see if the installation was successful.
[reference] https://cloud.google.com/appengine/docs/standard/python/setting-up-environment"
You need to create a Cloud Storage bucket using command line tools. Which of the following commands do you use?;gsutil create bucket;gsutil mb bucket;gsutil mb;gsutil mk bucket;;C;"Option 3 is the correct answer. gsutil is a command line for accessing and operating cloud storage from the command line. mb is the command to create a bucket. Create a bucket by typing gsutil mb. Cloud Storage has only one namespace, so you can't create a bucket with a name that another user is already using. Run it with a unique gsutil mb bucket name.
Option 1 is incorrect. Create a bucket with the mb command instead of the gsutil create bucket.
Option 2 is incorrect. You must run gsutil mb ""unique bucket name"" instead of gsutil mb bucket.
Option 4 is incorrect. Create a bucket with the mb command instead of the gsutil mk bucket.
[reference]  https://cloud.google.com/storage/docs/gsutil/commands/mb"
"You have resized the non-boot disk persistent disk because the VM running the application is out of disk space.
What settings do you need to make this additional space on the persistent disk available ?";Format and mount the disk.;Repartition and format the disk.;Extend the file system on disk.;Use the additional space on the disk in the file system with the gcloud disk resize command.;;C;"Option 3 is the correct answer. If you want to increase the persistent disk space, resize the disk and resize the single file system instead of repartitioning and formatting the disk. Adding a persistent disk volume size to suit your performance needs does not immediately take advantage of the additional space. After that, you will need to extend the file system on disk.
Resizing the disk may require resizing the file system and partitions.
Boot disk: For VMs that use public images, resizing the VM's boot disk and restarting the VM automatically resizes the root partition and file system. If you are using an image that does not support this feature, you will need to manually resize the root partition and file system.
Non-boot disk: When you resize the disk, you need to expand the file system on the disk to use the added space.
Option 1 is incorrect. This is the response when a new disc is attached. After creating a new disk and attaching it to the VM, you need to format and mount the disk. This will allow the operating system to use the storage capacity.
Option 2 is incorrect. If you want to increase the persistent disk space, resize the disk and resize a single file system instead of repartitioning and formatting the disk.
Option 4 is incorrect. The gcloud disk resize command is not a valid command. for example, If you are using ext4, Use the resize2fs command to extend the file system."
"You are setting up a VM as an engineer in charge of Google Cloud.
Which command is used to create an instance group for a VM?";gcloud compute template create;gcloud create compute instance-templates;gcloud compute instance-templates create;gcloud instance-group instance-templates create;;C;"Option 3 is the correct answer. You can create an instance template by running the command `gcloud compute instance-templates create [new template name]`. The gcloud command-line tool allows you to manage Compute Engine resources using the gcloud compute command group. After that, by writing instance-templates create, it will create instance-templates of the Compute Engine resource.
Option 1 is incorrect. The template is incorrectly written in ""gcloud compute template create"". Enter instance-templates instead of template.
Option 2 is incorrect. Enter the gcloud compute command group instead of gcloud create compute instance-templates.
Option 4 is incorrect. Enter the gcloud compute command group instead of gcloud instance-group instance-templates create.
[reference] https://cloud.google.com/sdk/gcloud/reference/compute/instance-templates/create"
"You have decided to build an application using Ruby on Google Cloud. It is a requirement to optimize costs while minimizing the tasks required to prepare and operate the development environment.
Which service should you use for this application development?";App Engine;Compute Engine;Kubernetes Engine;Cloud Functions;;A;"Option 1 is the correct answer. App Engine is a fully managed serverless platform for developing and hosting large-scale web applications. From your application, you can choose from multiple popular programming languages, libraries, and frameworks, and App Engine provides on-demand server provisioning and scaling for your application instances.
Option 2 is incorrect. Compute Engine is not a fully managed service, so you have to manage the infrastructure yourself, which has more management overhead than App Engine.
Option 3 is incorrect. Kubernetes Engine requires you to set up and manage the Kubernetes environment yourself, and requires skills and settings related to Kubernetes compared to App Engine.
Option 4 is incorrect. Cloud Functions are typically used to respond to Google Cloud events, not to run continuously running applications.
[reference] https://cloud.google.com/appengine/docs/standard"
"As an infrastructure administrator, you have a Google Cloud resource monitoring mechanism in place.
What do you need to do to group a set of metrics that occur over a period of time into a regular size bucket?";Alignment;Aggregation;Consolidation;Integration;;A;"Option 1 is the correct answer. Use ""Alignment"" to group a set of metrics that arrive over a period of time into a regular size bucket. Alignment is one of the processes used when setting metrics for monitoring using the operation suite. You can convert data points to a single data point and group them.
This process takes all the data points within the Alignment period, applies mathematical transformations such as mean, minimum, maximum, and delta, and transforms them into a single data point for each period. This allows you to group a set of metrics that arrive over a period of time into a regular size bucket.
Option 2 is incorrect. Aggregation is the entire process for combining data, which includes Alignment. Time series aggregation is done in two steps. First, each time series in the set is aligned to the same time interval boundary, then the number of time series sets is optionally reduced.
Options 3 and 4 are incorrect. There is no such process.
[reference] https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies#aggregation"
"The company manages users using Google Cloud. You are using Cloud Identity as an administrator to create an identity and have received a domain validation record.
Where do you need to add this domain validation record?";Domain DNS settings;Firewall rule settings;Load balancer policy settings;Virtual netork name resolution option settings;;A;"Option 1 is the correct answer. Google Cloud recommends that domain validation be done through a domain host. At that time, domain ownership is verified by adding the validation record received from Cloud Identity to the DNS settings. Cloud Identity provides validation records to add to your domain's DNS settings. When Cloud Identity verifies the existence of the record, it verifies the ownership of the domain. Confirmation records do not affect websites or emails.
Option 2 is incorrect. There is no need to change firewall rule settings for domain validation.
Option 3 is incorrect. For domain validation, you do not need to change the load balancer policy settings.
Option 4 is incorrect. The virtual network name resolution option is a feature that enables name resolution within a virtual network to enable domain discovery. This has nothing to do with domain validation.
[reference] https://cloud.google.com/identity/docs/verify-domain"
You have been entrusted with the operation management of Google Cloud. Which settings are required to perform VM logging and monitoring using the Operations Suite Logging and Monitoring?;BigQuery dataset settings;Operation Suite Agent Configuration;Cloud Storage bucket settings;Compute Engine storage space settings;;B;"Option 2 is the correct answer. In order to use the Operations Suite to collect metric data from a Compute Engine instance and Cloud Logging to collect logs from a Compute Engine instance, the Operations Suite Agent must be installed on all target VMs.
The Google Cloud Operations suite provides the following agents for collecting metrics on Linux and Windows VM instances:
Ops agent: The primary and recommended agent for collecting telemetry from Compute Engine instances. This agent integrates logging and metrics into a single agent. It provides a YAML-based configuration for collecting logs and metrics, as well as high-throughput logging capabilities.
Former Monitoring Agent: Collect system and application metrics from virtual machine instances and send the collected information to Cloud Monitoring. By default, the old Monitoring agent collected disc, CPU, network, and process metrics. You can configure the agent to monitor third-party applications to get a complete list of agent metrics.
Option 1 is incorrect. You don't need to create a BigQuery dataset for logging or monitoring with the Operations Suite.
Option 3 is incorrect. You don't need to create a Cloud Storage bucket for logging or monitoring with the Operations Suite.
Option 4 is incorrect. You do not need to create Compute Engine storage space for logging and monitoring with the Operations Suite.
[reference] https://cloud.google.com/products/operations"
You are developing an application using a large number of VMs. It is necessary to increase availability by automatically scaling the VM based on CPU utilization. What is the way to scale based on CPU usage or memory usage to manage the scaling of many VMs using Kubernetes Engine?;Create a Google Kubernetes Engine cluster and configure autoscalling based on monitored CPU utilization in your operations suite;Create an instance template and create a Google Kubernetes Engine cluster with autoscaling configured.;Create a Google Kubernetes Engine cluster and set up horizontal pod autoscaling.;Create a Google Kubernetes Engine cluster using a set of third-party tools and configure autoscaling based on monitoring CPU utilization in your operations suite.;;C;"Option 3 is the correct answer. For efficient management of VM scaling configurations, create a Google Kubernetes Engine cluster and enable horizontal pod autoscaling to be based on the CPU and memory usage of the target pod. You can increase or decrease the number of pods.
Kubernetes is an orchestration tool that automatically manages and scales clusters of one or more containers. All Kubernetes objects, which are containerized applications, run on the cluster. You can enable automatic scaling for your cluster, which will perform metric-based scaling. Pods are the smallest unit of application you can run on Kubernetes.
Pod can coexist multiple containers from one container. Pods always run on nodes and run on virtual machines. Kubernetes has the following automatic scaling functions.
Horizontal pod autoscaling:
You can increase or decrease the number of pods based on the CPU and memory usage of the target pods.
Vertical Pod Auto Scaling:
You can automatically increase or decrease the amount of CPU and memory allocated to the container.
Cluster autoscaling:
Scales the number of nodes in the cluster.
[reference] https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler"
You are developing an application using Kubernetes. Where does Kubernetes Engine write application log data by default?;SYSLOG;STDIN;SYSOUT;STDERR;;D;"Option 4 is the correct answer. Kubernetes Engine collects log data written to standard output (STDOUT) and standard error (STDERR) by default. Kubernetes Engine can get the standard output / standard error logs of the container with the kubectl logs command.
Option 1 is incorrect. SYSLOG is a standard client / server protocol that forwards log messages over IP networks and is not relevant here.
Option 2 is incorrect. Kubernetes Engine does not collect input to standard input (STDIN).
Option 3 is incorrect. SYSOUT is not used in Kubernetes Engine.
[reference] https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine"
" You are developing a highly accessed application on Google Cloud. That data layer requires a high-performance relational database.
Which is the best service available as a relational database on Google Cloud?";Cloud SQL and Bigtable;Datastore and Cloud Storage;Spanner and Cloud SQL;Spanner and Bigtable;;C;"Option 3 is the correct answer. In Google Cloud, Spanner and Cloud SQL databases are relational databases with SQL interfaces.
Spanner is a fully managed relational database with unlimited scaling, strong integrity and up to 99.999% availability. Unlimited scaling allows you to take advantage of relational semantics and SQL features. Cloud SQL is a fully managed relational database service that can utilize MySQL, PostgreSQL, and SQL Server. It is possible to build an industry standard SQL database.
Options 1 and 4 are incorrect. Cloud Bigtable is Google's fully managed NoSQL database service.
Option 2 is incorrect. Datastore is a highly scalable NoSQL database for web and mobile apps.
[reference] 
https://cloud.google.com/spanner?hl=JA 
https://cloud.google.com/sql?hl=ja"
"Company A has decided to analyze the data of Cloud Storage. You need to load the data from Cloud Storage into BigQuery.
Which of the BigQuery usage permissions do not you need when doing this?";bigquery.tables.create;bigquery.jobs.create;bigquery.tables.read;bigquery.tables.updateData;;C;"Option 3 is the wrong command and is the correct answer. You don't have the authority bigquery.tables.read. The following three permissions are required to read data into BigQuery.
bigquery.tables.create
bigquery.tables.updateData
bigquery.jobs.create
These permissions are required when reading data into a new table or partition, or when adding or overwriting data to a table or partition.
[reference] https://cloud.google.com/bigquery/docs/batch-loading-data 
https://cloud.google.com/bigquery/docs/access-control"
"Your company uses Docker to develop web applications. You are preparing to deploy your application using a Kubernetes cluster under the name test1 of your replication controller. This application needs to be configured to scale pods from 2 to 8 when CPU usage exceeds 70%.
Which command do you need to use to set up a Kubernetes cluster?";`gcloud kubectl autoscale rc test1 --min=2 --max=8 cpu-percent=70`;`kubectl autoscale rc test1 --min=2 --max=8 cpu-percent=70`;`gcloud containers autoscale rc test1 --min=2 --max=8 cpu-percent=70`; `kubectl apply rc test1 --min=2 --max=8 cpu-percent=70`;;B;"Option 2 is the correct answer. When you execute the command `kubectl autosclae rc [replication controller name] --min = [minimum number of pods] --max = [maximum number of pods] --cpu-percent = [CPU usage threshold]`, the CPU usage rate will increase. Performs scaling to adjust the number of pods between the specified minimum and maximum pods so that CPU utilization falls below the threshold when the specified threshold is exceeded.
You can use the kubectl command line tool to control your Kubernetes cluster. Set autoscaling with autosclae rc. Therefore, by setting max = 8, the number of bots can be increased up to 8 when scaling.
Option 1 is incorrect. Like gcloud kubectl autoscale, you don't need gcloud. Control your Kubernetes cluster with the kubectl command.
Option 3 is incorrect. There is no command called gcloud containers. Control your Kubernetes cluster with the kubectl command.
Option 4 is incorrect. Use autosclae rc instead of kubectl apply to set autoscaling.
[reference] https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#autoscale"
"Objects stored in Cloud Storage should be migrated from regional storage to nearline storage after 30 days.
What kind of settings do you need?";Create a lifecycle rule to specify that the object should be moved to nearline storage after 30 days.;Set the JSON file to move the object to nearline storage after 30 days.;The migrateObjectAfter property of the saved object specifies that the objet should be moved to nearline storage after 30 days.;Create an object rule to specify that the object should be moved to nearline storage after 30 days.;;A;"Option 1 is the correct answer. Lifecycle rules in Cloud Storage allow you to schedule data to be migrated or deleted from other storage systems. This allows you to assign lifecycle management configurations to buckets. If the object meets the rule criteria you set, Cloud Storage automatically performs the specified operation on the object. In this scenario, the life cycle rule automates the operation of automatically moving an object to nearline storage 30 days after it is uploaded by specifying that the object should move from current storage to nearline storage after 30 days.
Option 2 is incorrect. Set lifecycle rules instead of JSON file definitions.
Option 3 is incorrect. Set lifecycle rules, not object property settings.
Option 4 is incorrect. Set life cycle rules, not object rule settings.
[reference] https://cloud.google.com/storage/docs/lifecycle"
As an infrastructure manager, you have a monitoring system in place and you plan to use the operation suite of Google Cloud. Which of the following monitoring targets is not covered by the Operation Suite?;IBM cloud;On-premises data center;AWS;Google Cloud;;A;"Option 1 is the correct answer. The Google Cloud Operations Suite can monitor resources not only in Google Cloud, but also in AWS and on-premises data centers. However, IBM Cloud is not monitored. The Google Cloud Operations Suite is a basic monitoring tool that allows you to monitor and troubleshoot application performance in your Google Cloud environment. Cloud Logging automatically captures Google Cloud audit logs and platform logs. Cloud Monitoring allows you to view all Google Cloud metrics for free and integrates with various providers for monitoring other than Google Cloud.
Option 2 is incorrect. The Google Cloud Operations Suite can be applied to your on-premises environment.
Option 3 is incorrect. The Google Cloud Operations Suite can be applied to AWS.
Option 4 is incorrect. The Operations Suite is a basic feature of Google Cloud.
[reference]
https://cloud.google.com/stackdriver/docs"
"Your company has decided to move from an on-premises environment to the cloud. At that time, we will use Google Cloud. As the person responsible for migration, you are designing your organizational hierarchy.
Which organizational components are not available on Google Cloud?";project;Bucket;folder;Organization;;B;"Option 2 bucket is correct because it is not a component used for organizational hierarchies or configurations. Organizations, folders, and projects are components used to manage the Google Cloud organizational hierarchy. On the other hand, buckets, directories, and subdirectories are used to organize storage.
Option 1 is incorrect. The Google Cloud project is the basis for creating, enabling, and using all Google Cloud services, including managing APIs, enabling billing, adding and removing collaborators, and managing permissions on Google Cloud resources.
Option 3 is incorrect. Folders are nodes in the Cloud Platform resource hierarchy. Within the folder, there are projects and other folders. Folders allow you to group projects on organizational nodes in your hierarchy. For example, your organization may have multiple departments, each using its own set of Google Cloud resources. Folders allow you to group these resources by department.
Option 4 is incorrect. Your organization is the root node of your Google Cloud resource hierarchy and the hierarchical supernode of your project.
[reference] https://cloud.google.com/resource-manager/docs/creating-managing-organization"
"You are developing a serverless application using Cloud Functions. It is necessary to utilize event processing in Cloud Functions, but which event processing is available?
Please select the most correct combination from the following.";HTTP, Cloud Storage, Cloud Pub/Sub, Firebase, Cloud Logging;HTTP, Cloud Storage, Cloud Pub/Sub, Firebase;HTTP, Cloud Storage, Cloud Pub/Sub;Cloud Pub/Sub, Firebase, Operations Suite Log;;A;"Option 1 is the correct answer. Cloud Functions can run serverless applications from events. At that time, Cloud Functions supports all five events: Cloud Storage, HTTP, Cloud Pub / Sub, Firebase, and Cloud Logging. For example, changing data in the database, adding files to the storage system, creating a new virtual machine instance, etc. are treated as events. Cloud Functions supports the following provider events:
・ HTTP
・ Cloud Storage
・ Cloud Pub / Sub
・ Cloud Firestore
・ Firebase (Realtime Database, Storage, Analytics, Auth)
・ Cloud Logging-
Options 2, 3 and 4 are incorrect. All of these elements are supported by Cloud Functions, but they are under-targeted compared to Option 1.
[reference]
https://cloud.google.com/functions/docs/concepts/events-triggers
https://cloud.google.com/functions/"
"You must create a custom role with one or more permissions at the organization or project level and apply it to your users.
Which launch steps are available when creating a custom role?";DISABLED;Only General availability (GA);Only Alpha version;Only Beta;Options 1 to 4;E;"Option 5 is the correct answer. Identity and Access Management (IAM) allows you to set permissions using roles that are already defined, as well as allow users to create and manage their own custom roles. Managing roles includes changing, disabling, listing, deleting, and canceling deletions of roles. When creating a custom role, you can select the boot stage from all release stages of disabled, GA (general availability), alpha, and beta.
Custom roles have a release stage. This is stored in the role's stage property. The most common release stages for active custom roles are alpha, beta, and GA. These release stages are for informational purposes only. This allows you to track the widespread use of each role.
[reference] https://cloud.google.com/iam/docs/understanding-custom-roles?"
You are developing an application with a redundant configuration by launching instances in two zones in the same region. However, it seems that the load balancer's distributed processing for the target pool is not working well. Which of the following is the most likely cause?;Data transmission for the target pool is not enabled.;The target pool is not activated.;The target pool firewall does not allow a particular protocol.;Health checks are not enabled for the target pool.;;D;"Option 4 is the correct answer. If your load balancer uses a target pool for load balancing, traffic will only be routed to instances with good health checks. Therefore, if health checks are not enabled, distributed processing will not run properly.
External TCP / UDP Network Load Balancing uses either a backend service or a target pool to define a group of backend instances that receive inbound traffic. At that time, we use health checks to distribute traffic to healthy targets.
Option 1 is incorrect. Since traffic can be forwarded to one instance, it is likely that traffic forwarding to the target pool itself is enabled.
Option 2 is incorrect. The target pool is considered active because traffic can be forwarded to one instance.
Option 3 is incorrect. Since traffic can be forwarded to one instance, the firewall settings for the target pool should be fine. 
[reference] https://cloud.google.com/load-balancing/docs/target-pools"
Which command should you use to display information about the dataset named testdata in the test1 project?;bq list --format=prettyjsontest1:testdata;bq show --format=prettyjsontest1:testdata;bq load --format=prettyjsontest1:testdata;bq ls --format=prettyjsontest1:testdata;;B;"Option 2 is the correct answer. You can get the information for the specified dataset by running the command `bq show --format = prettyjson [project ID]: [dataset name]`. The bq command is a command line tool for working with BigQuery. When working with BigQuery, type bq and then an action command such as show. The show command is a command that displays information about BigQuery datasets and tables. Therefore, if you want to specify a dataset named testdata for the test1 project, run --format = prettyjson [project ID]: [dataset name] `.
Option 1 is incorrect. The list command does not exist in the bq command.
Option 3 is incorrect. The load command is a command for importing CSV data as table data.
Option 4 is incorrect. The ls command is a command to display a list of projects, datasets, and tables. 
[reference] https://cloud.google.com/bigquery/docs/dataset-metadata#bq"
We are building a mechanism to store client-side development and server-side development data using NoSQL Cloud Databse on Google Cloud. Which of the following is not a Cloud Firestore data storage option?;Firestore in Datastore mode;Firestore in native mode;Native mode Datastore;;;C;"Option 3 is the correct answer. Cloud Firestore can use Google's flexible and scalable NoSQL cloud database to store or synchronize data for client-side and server-side development. When creating a new Firestore database, choose a database mode from the following two options:

■ Firestore in Datastore mode This mode uses the behavior of the Datastore system and removes Datastore restrictions because it accesses the Firestore storage layer.
■ Firestore in native mode This is the mode that will be the next major version that will be the rebranding of Datastore. It's a NoSQL document database built to make the most of Datastore and Firebase Realtime Database, with automatic scaling and high performance, to make application development more comfortable. Therefore, Datastore native mode is not available as an option.
[reference] https://cloud.google.com/datastore/docs/firestore-or-datastore"
The company uses Google Cloud to maintain its internal infrastructure. As an administrator, you have created multiple organizations for use by multiple departments. You have a separate VPC configured between each organization, but you need to connect the individual VPC networks to each other. Which of the following is the appropriate VPC-to-VPC connection method?;Set up CPC network peering.;Set up a shared VPC.;Set up a VPN.;Set a leased line.;;A;"Option 1 is the correct answer. In order to connect VPCs in different organizations, you need to use VPC network peering. VPC network peering allows workloads in different VPC networks to communicate internally.
If you have multiple network management domains in your organization, you can take advantage of VPC network peering to use internal IP addresses to communicate between different VPCs between your organizations. When servicing other organizations, VPC network peering makes the service available to other organizations using their internal IP address. If we can provide services across organizations, we will be able to provide services to other companies. It is also useful if you own or manage multiple organizations.
Option 2 is incorrect. VPC shares are only available within a single organization. You may need to change your firewall rules, but that's not enough. VPN is used to connect your Google Cloud network to your on-premises network.
Option 3 is incorrect. VPN is used to connect the Google Cloud network to the on-premises environment.
Option 4 is incorrect. The leased line is used to connect the Google Cloud network to the on-premises environment.
[reference]  https://cloud.google.com/vpc/docs/vpc-peering"
"A data scientist at your company is building applications that process large amounts of data with machine learning on Google Cloud. The data for analysis is stored in Bigtable.
What permissions does the data scientists need to read data from Bigtable, while giving no permissions other than reading data?";roles/bigtable.reader;roles/bigtable.owner;roles/bigtable.viewer;roles/bigtable.user;;A;"Option 1 is the correct answer. It is necessary to grant the minimum privilege required to the data scientist according to the principle of least privilege. Therefore, to be able to read data from Bigtable, set roles / bigtable.reader so that no other permissions are granted. roles / bigtable.reader only provides read permission for the data stored in the table, not write permission or change database settings.
Option 2 is incorrect. There is no authority called roles / bigtable.owner. Option 3 is incorrect. roles / bigtable.viewer provides read permission for database metadata. This privilege is not sufficient for data scientists because it cannot read data.
Option 4 is incorrect. roles / bigtable.user provides read / write permissions to the data stored in the table. Data scientists do not need write permission for data.
[reference] https://cloud.google.com/bigtable/docs/access-control"
"You are managing a Google Cloud project as an infrastructure administrator. At the direction of my boss, you have must submit a list of roles defined in the project.
Which command do you need to use?";gsutil iam roles sow --project=[PROJECT-ID];gcloud roles show --project=[PROJECT-ID];gcloud iam roles list --project=[PROJECT-ID];gsutil roles list --project=[PROJECT-ID];;C;"Option 3 is the correct answer. You can get the list of roles defined in the specified project by running the command `gcloud iam roles list --project = [project ID]`. You can perform actions related to roles with the command gcloud iam roles. On top of that, list is an action command that displays a list of roles.
Option 1 is incorrect. gsutil is a command line tool for working with Cloud Storage. You need to perform actions related to roles with a command called gcloud iam roles.
Option 2 is incorrect. gcloud roles is not a valid command. You need to perform actions related to roles with a command called gcloud iam roles.
Option 4 is incorrect. gsutil roles list is not a valid command. You need to perform actions related to roles with a command called gcloud iam roles.
[reference] https://cloud.google.com/sdk/gcloud/reference/iam/roles/list"
"To make a running Windows server redundant, you need to load balance traffic from other resources in the same VPC. All traffic is TCP with an IPv4 address.
Which load balancer should you use?";External HTTPS load balancer;Internal TCP/UDP load balancer;TCP Proxy load balancer;External TCP/UDP load balancer;;B;"Option 2 is the correct answer. Use a TCP / UDP load balancer to distribute TCP traffic with IPv4 addresses. Also, it is the internal load balancer that distributes traffic from other resources in the same VPC. The internal TCP / UDP load balancer is used for internal traffic that is not from the internet. It also distributes traffic to instances inside Google Cloud.
Option 1 is incorrect. The external HTTPS load balancer is implemented in Google Front End (GFE). GFE is globally distributed and works with Google's global network using the control plane. This is incorrect because it cannot handle TCP traffic.
Option 3 is incorrect. A TCP proxy load balancer is a reverse proxy load balancer that distributes TCP traffic sent from the Internet to virtual machine (VM) instances in your Google Cloud VPC network. This time it is necessary to load balance the internal traffic.
Option 4 is incorrect. A External TCP / UDP load balancer distributes the load on traffic from all clients on the Internet.
[reference] 
https://cloud.google.com/load-balancing/docs/load-balancing-overview
https://cloud.google.com/load-balancing/docs/choosing-load-balancer"
"Your company is migrating its infrastructure to Google Cloud. We first created a virtual private cloud (VPC) in automatic mode and automatically created subnets in each region.
How is the CIDR block assigned to each region's subnet determined?";Each region is automatically assigned a range of internal IP addresses.;For each region, specify the range of internal IP addresses in CIDR.;For each region, specify the range of external IP addresses in CIDR;Each region is automatically assigned a range of external IP addresses.;;A;"Option 1 is the correct answer. Automatic Mode When you create a VPC network, one subnet from each region is automatically created in the network. The auto-created subnet uses a predefined IP range (10.128.0.0/9 CIDR block). Therefore, each region is automatically assigned a range of internal IP addresses.
Option 2 is incorrect. The range to be assigned does not need to be specified by the user and is set automatically. Other than auto-created subnets, you can manually add subnets to your Automode VPC network within the selected region using an IP range other than 10.128.0.0/9.
Option 3 is incorrect. It is assigned an internal IP address, not an external IP address.
Option 4 is incorrect. It is assigned an internal IP address, not an external IP address.
[reference] https://cloud.google.com/vpc/docs/vpc?hl=ja#subnet-ranges"
"Healthtech Venture is developing a health management app hosted on Google Cloud. The engineer in charge is setting up a local development environment for Google Cloud.
Which gcloud command should you use to authorize access to the resource?";gcloud auth login;gcloud login;gcloud login start;gcloud login config;;A;"Option 1 is the correct answer. By executing `gcloud auth login`, the authentication page will be displayed in the browser, and you can log in with an account that uses Google Cloud. This allows you to authenticate to access your local development environment.
Option 2 is incorrect. gcloud login is not a valid command. Run gcloud auth login.
Option 3 is incorrect. gcloud login start is not a valid command. Run gcloud auth login.
Option 4 is incorrect. gcloud login config is not a valid command. Run gcloud auth login.
[Reference] https://cloud.google.com/sdk/docs/initializing"
"You use Google Cloud to manage your internal storage. You want to rename multiple files in my bucket.
Which command should you use for this?";gsutil move;gsutil mv;gstil rb;gsutil cp;;B;"Option 2 is the correct answer. You can use the `gsutil mv gs: // [bucket name] / [current object] gs: // [bucket name] / [new object name]` command to modify the objects in the bucket. gsutil is a command line tool that allows you to access Cloud Storage from the command line. Then write a command that specifies the operation. gsutil mv can also rename files in Google Cloud Storage.
Option 4 is incorrect. gsutil cp copies the file instead of renaming it.
Options 1 and 3 are not valid commands.
[reference] https://cloud.google.com/storage/docs/gsutil/commands/mv"
"You are designing a microservices architecture. It is necessary to link applications with various functions, and you will use a mechanism to notify messages between applications. In this scenario, you need to put the data in the queue.
Which service do you use for this?";Cloud Pub/Sub;Google Cloud Messaging;Cloud Tasks;Cloud Dataflow;;A;"Option 1 is the correct answer. Cloud Pub / Sub is an asynchronous messaging service that separates the service that handles events from the service that generates events. Services can communicate asynchronously with a latency of around 100 milliseconds. Since you can select the push notification method or queue method in Cloud Pub / Sub, you can take data into the queue like this time and use it for cooperation that is processed from another component.
Option 2 is incorrect. Google Cloud Messaging is now obsolete and is a service integrated into Firebase Cloud Messaging (FCM), which is incorrect. You can take advantage of this to facilitate messaging between mobile and server applications. Option 3 is incorrect. Cloud Tasks organizes and manages asynchronous execution requests with features such as scheduling, deduplication, configurable retry policies, and version redirection. Option 4 is incorrect. Cloud Dataflow is a stream and batch processing service, not a queuing service. 
[reference] https://cloud.google.com/pubsub/docs/overview"
"You company leverages legacy applications hosted on clusters deployed on-premises. The nodes in the cluster are using different nodes.
If you're migrating this cluster to Google Cloud, which Compute Engine resource is the best place to migrate?";Unmanaged instance Group;Managed instance group;Service account;Ressource group;;A;"Option 1 is the correct answer. Unmanaged instance groups allow load balancing configurations across a set of VMs that you manage yourself. Therefore, when setting when the nodes in the on-premises cluster are different, it is possible to implement a distributed configuration using different nodes by using an unmanaged instance group. An unmanaged instance group is a collection of virtual machines (VMs) that reside in a single zone, VPC network, or subnet. Unmanaged instance groups are useful for grouping VMs that require individual configuration settings and tuning.
Option 2 is incorrect. Managed instance groups cannot run between clusters. Managed instance groups can operate applications on multiple identical VMs. Leverage automated MIG services such as auto-scaling, auto-repair, region (multi-zone) deployment, and auto-update for scalable and highly available workload processing. It is convenient to manage it automatically, but it is not suitable for user-specific configuration for this scenario.
Option 3 is incorrect. Service accounts are special accounts used by applications and virtual machine (VM) instances, not users. The application uses the service account to make authorized API calls. It cannot be used for configurations such as setting a VM using this.
Option 4 is incorrect. Resource group is a function to manage a series of resources as a group in Cloud Monitoring. It cannot be used for configurations such as setting a VM using this.
[reference] https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances"
"Your company is migrating a server hosted in an on-premises data center to Google Cloud. You create a set of virtual machines with Compute Engine as the migration destination.
Which IP address is assigned to the virtual machine by default?";Static external IP address;Ephemeral internal IP address;Static internal IP address;Global static external IP address;;B;"Option 2 is the correct answer. This time, the migration destination is assumed to be a virtual machine (VM). Unless a separate external IP address is assigned to the virtual machine, the ephemeral internal IP address and ephemeral external IP address are assigned by default. You can use internal and external IP addresses for Google Cloud resources. Instances use these addresses to communicate with other Google Cloud resources and external systems. 
■ Internal IP address A private IP address used only inside the virtual network in Google Cloud. It is used for communication between resources in the virtual network, but not for communication with the Internet. 
■ External IP address If you need to communicate with the Internet, resources on another network, or resources other than Compute Engine, you can assign an external IP address to your instance or forwarding rule. A VM instance can have one primary internal IP address that is unique to your VPC network. You can assign a specific internal IP address when you create a VM instance. Alternatively, you can reserve a static internal IP address for your project and assign that address to the resource. If you do not specify an address, Compute Engine automatically assigns it. In either case, the address must belong to the IP range of the subnet.
Option 1 is incorrect. The static external IP address is an additional external IP address to be set. It will be assigned to the project for a long time until it is explicitly released, and will remain connected to the resource until it is explicitly disconnected.
Option 3 is incorrect. The static internal IP address is an additional internal IP address to be set. It is assigned to the project for a long time until it is explicitly released, and remains connected to the resource until it is explicitly disconnected from the resource. Therefore, the user must perform attach and release operations.
Option 4 is incorrect. Global static external IP address Global load balancing Only available in the global forwarding rules used for. You cannot assign a global IP address to a region or zone resource, such as a Compute Engine instance. IP addresses can also be divided into regions and global IPs.
· Region IP address with one or more network interfaces VM instance Or you can use it with a region load balancer.
-The global IP address can be used for the global load balancer.
[reference] https://cloud.google.com/vpc/docs/ip-addresses"
"You are setting up a redundant configuration with multiple VMs plus a load balancer. You must terminate the client SSL session on the load balancer to ensure communication security.
Which load balancer do you choose?";External TCP proxy load balancer;External SSL proxy load balancer;Internal TCP/UDP load balancer;HTTP(S) Load Balancer;;B;"Option 2 is the correct answer. External SSL Proxy Load Balancing is a reverse proxy load balancer that distributes SSL traffic sent from the Internet to virtual machine (VM) instances in your Google Cloud VPC network.  External SSL Proxy Load Balancing is a reverse proxy load balancer that distributes SSL traffic coming from the internet to virtual machine (VM) instances in your Google Cloud VPC network.
When using External SSL Proxy Load Balancing for your SSL traffic, user SSL (TLS) connections are terminated at the load balancing layer, and then proxied to the closest available backend instances by using either SSL (recommended) or TCP. For the types of backends that are supported, see Backends.
Option 1 is incorrect. External TCP Proxy Load Balancing is a reverse proxy load balancer that distributes TCP traffic sent from the Internet to virtual machine (VM) instances in your Google Cloud VPC network.
Option 3 is incorrect. Internal TCP / UDP load balancing is a region load balancer built on the Andromeda network virtualization stack.
Option 4 is incorrect. It is the HTTPS load balancer that allows the load balancer to terminate the client SSL session to ensure communication security.  External HTTP (S) load balancing distributes HTTP and HTTPS traffic to the backend hosted by Compute Engine and Google Kubernetes Engine (GKE). Terminate HTTP (S) traffic as close to the user as possible. In Standard Tier, load balancing is handled on a region-by-region basis.
[reference]  https://cloud.google.com/load-balancing/docs/choosing-load-balancer"
You decide to use an instance template to launch multiple instances at the same time.Which of the following is an incorrect element of an instance template?;Boot disk image or container image;Cloud Storage bucket definition;Machine type;label;;B;"Option 2 is the correct answer. The Cloud Storage bucket definition is not included in the instance template. Instance templates are resources that you can use to create virtual machine (VM) instances and managed instance groups (MIGs). Templates define machine types, boot disk or container images, labels, and other instance properties. After that, you can create multiple VMs with instance templates defined and manage them in a shared manner. Instance templates allow you to save the configuration of a VM instance and create a VM or group of VMs based on the defined configuration.
Options 1, 3 and 4 are incorrect. Machine types, boot disk images or container images, zones, and labels are all virtual machine configuration parameters or attributes. Therefore, these are included in the instance group configuration that creates the virtual machine.
[reference] https://cloud.google.com/compute/docs/instance-templates"
" Your company uses Google Cloud projects in multiple departments, and there are currently three projects. You would like to report daily and monthly service cost estimates for the next year using standard query syntax for each project.
How do you set this?";Export the invoices to a Cloud Storage bucket and perform analysis with Cloud Bigtable.;Export the invoices to a Cloud Storage bucket and perform time window-based SQL query analysis.;Export the invoices to a BigQuery dataset and perform time window-based SQL query analysis;Export the invoices to a BigQuery dataset and perform analysis with CloudBigtable.;;C;"Option 3 is the correct answer. By exporting the invoice to a BigQuery dataset, you can perform time window-based SQL query analysis.
Cloud Billing can automatically export detailed Google Cloud billing data (usage, cost forecasts, pricing data, etc.) to the BigQuery dataset you specify by using the export feature throughout the day. This allows BigQuery to access Cloud Billing data for in-depth analysis. 
[reference]  https://cloud.google.com/billing/docs/how-to/export-data-bigquery"
"As a system administrator, you are using the Operations Suite Logging to configure Google Cloud logging. Logs should be retained for a longer period than the normal log entry retention period.
What is the best way to control the transfer of logs?";Transfer the log data to the sink using the export function of the operation suite Logging.;Transfer log data to log storage using the Operations Suite API.;Transfer log data to log storage using the sink function of the operation suite Logging.;Transfer to the sink of the operation suite Logging.;;C;"Option 3 is the correct answer. The sink controls how Cloud Logging routes the logs. You can use a sink to transfer some or all of your logs to a supported destination. Reasons for controlling how logs are transferred include the following: - Although unlikely to be read, to maintain logs for compliance purposes. - To organize the logs in the bucket in a useful format. - To use big data analysis tools for logs. - To stream logs to other applications, other repositories, or third parties.
Regardless of how the predefined sinks handle log entries, you can create your own sink and forward some or all of the log to a selectable destination, or log it so that it is not saved by Cloud Logging. Can be excluded. Options 1 and 4 are incorrect. Instead of using the export features of the Operations Suite Logging to export log data to the sink, the sink is the ability to control log transfers.
Option 2 is incorrect. Instead of using the Operations Suite API, use the sink to export the log data to the log sink."
"Google Cloud engineers have decided to create a new project for new application development.
Which permission is required for the access administrator?";resourcemanager.create.projects;resourcemanager.projects.create;resourcemanager.projects.write;resourcemanager.write.projects;;B;"Option 2 is the correct answer. The permission required to create a project for new application development is resourcemanager.projects.create. This gives you the privileges you need for project management, such as ID / access management. To request the creation of a new project, you need resourcemanager.projects.create with IAM permissions. Permissions are identified by the specified ResourceId and must include both the identity and type, such as organization.
Options 1, 3 and 4 are incorrect. The other options aren't really predefined IAM roles.
[reference]  https://cloud.google.com/resource-manager/reference/rest/v1/projects/create"
What type of cluster do you use to build the most available applications on your Kubernetes cluster?;Multi-virtual network cluster;Zone cluster;Multi-subnet cluster;Regional cluster;;D;"Option 4 is the correct answer. Use region clusters to build the most highly available applications. Redundant replicas of the control plane allow region clusters to deploy clusters in multiple zones to increase the availability of the Kubernetes API. Regional clusters have replicas of the control plane. GKE offers two types of clusters: region clusters and zone clusters.

■ Zone clusters All cluster control planes and nodes run in the single compute zone specified when the cluster was created. 

■ In a regional cluster, the control plane and nodes are duplicated in multiple zones within a single region.

Options 1 and 3 are incorrect. There are no configurations of multi-virtual network clusters or multi-subnet clusters.

Option 2 is incorrect. Zone clusters are less available than regional clusters because they rely on a single zone.

[reference]  
https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster"
"As a Google Cloud administrator, you are building a data sharing system that stores the necessary personal information for a high level of security. Your server must be protected from being compromised by kernel-level malware.
What kind of virtual machine should you use?";Preemptible virtual machine;Shielded VM;Single tenant VM;Single tenant node;;B;"Option 2 is the correct answer. Shielded VMs allow you to configure special, highly secure virtual machines. This virtual machine ensures security by providing verifiable integrity of the VM instance. This feature allows you to verify that the configuration and data inside your instance has not been tampered with by boot-level or kernel-level malware or rootkits.
Option 1 is incorrect. Preemptible virtual machines are short-lived, affordable compute instances suitable for batch jobs and fault-tolerant workloads. This is used when choosing cost optimization. There is no special response to kernel-level malware.
Options 3 and 4 are incorrect. Single-tenant VMs and single-tenant nodes are used as a normal VM configuration. It cannot provide verifiable integrity for Compute Engine VM instances like Shielded VMs.
[reference]  https://cloud.google.com/security/shielded-cloud/shielded-vm"
"You have decided to create a Kubernetes Engine cluster and build an execution environment for machine learning predictive models. The machine learning process time requires more GPU and memory than the process of executing a predictive model.
How do you configure your cluster to meet this requirement?";Configure a cluster with two zones, making sure that one zone constitutes more nodes than the other.;Use multiplu pods, some which are configured for more GPU and memory.;Configure a cluster with two pods. Then set one node with more CPU and memory than the other.;Deploy a cluster in multiple zones using multiple pods.;;B;"Option 2 is the correct answer. By preparing pods for learning and prediction respectively, and requesting more GPU and memory for the learning one, the resources of the node can be used effectively.
A pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker) and the shared resources of those containers.
Pods include:
Shared storage (volume)
Networking (cluster-specific IP address)
Information about how each container works, such as the image version of the container and the ports used. In other words, it is possible to define a pod as one machine and divide the process. Therefore, you can adjust the performance by setting the CPU and GPU allocation for the pod.
Option 1 is incorrect. You can increase redundancy by configuring a cluster with two zones, but it is not required in this scenario.
Option 3 is incorrect. There is no need to configure a cluster with two pods. Configuring two clusters results in an architecture that is too verbose and cost-effective. You need to have separate learning and predictive pods to separate the processes.
Option 4 is incorrect. You can achieve this requirement by configuring with multiple bots. You don't even have to deploy the cluster in multiple zones."
You need to set up a configuration for multiple Google Cloud projects. Which command do you use?;gcloud projects configurations create;gcloud config configurations set;gcloud config configurations create;gcloud config set project project ID;;C;"Option 3 is the correct answer. You can set up multiple Google Cloud project configurations with the `gcloud config configurations create` command. The gcloud config command allows you to view and edit Cloud SDK properties. You can edit the project configuration by using gcloud config configurations.
Option 1 is incorrect. The command gcloud projects configurations is not a valid command. Use the gcloud config configurations command.
Option 2 is incorrect. You can set up multiple Google Cloud project configurations with the gcloud config configurations create` command instead of the gcloud config configurations set` command.
Option 4 is incorrect. Create a specific project with the gcloud config set project project name after the gcloud config configurations create` command. gcloud config set project Not the project ID.
[reference]   https://cloud.google.com/sdk/gcloud/reference/config/configurations/create"
As a system administrator, you're set up to optimally manage your Google Cloud resources.Which of the following elements is not available as a part of the resource hierarchy?;Configuration file;project;Organization;folder;;A;"Option 1 is the correct answer. The configuration file is not part of the resource hierarchy. The resource hierarchy has an organization at the top, folders under it, and projects are organized within the folders.
Option 2 is incorrect. A Google Cloud project is a unit that manages APIs, activates billing, adds and removes collaborators, manages permissions on Google Cloud resources, and more. This is the basis for creating, enabling, and using all Google Cloud services.
Option 3 is incorrect. Your organization is the root node at the top of the Google Cloud resource hierarchy. It becomes a hierarchical supernode of the project.Option 4 is incorrect. Folders are nodes in the Cloud Platform resource hierarchy that are set up under your organization. There are projects and other folders inside the folder. Folders allow you to group projects on organizational nodes in your hierarchy. For example, your organization may have multiple departments, each using its own set of Google Cloud resources.
[reference]  https://cloud.google.com/resource-manager/docs/creating-managing-organization"
You are setting up the Compute Engine and have deployed one tenant node. In what way are the VMs running on that node limited?;Only VMs from the same organization can run on the node.;Only VMs in the same zone can run on the node.;Only VMs in the same region can run on the node.;Only VMs from the same project can run on the node.;;D;"Option 4 is the correct answer. In this scenario, the fact that one tenant node is deployed is a limitation when starting a VM using a single tenant node. On a single tenant node, only VMs from the same project will run on that node. You can physically separate VMs from VMs in other projects by configuring a single tenant node. VMs running on a single tenant node can use the same Compute Engine features as other VMs, such as transparency scheduling and block storage, but with separate hardware. For full control of the VMs on the physical server, each single tenant node maintains a one-to-one mapping with the physical server that supports the node. Because the host hardware is not shared with other projects, security and compliance requirements can be met for other workloads and workloads that require physical isolation from the VM.
Option 1 is incorrect. Limited to VMs in the same project, not in the same organization.
Option 2 is incorrect. Limited to VMs in the same project, not in the same zone.
Option 3 is incorrect. Limited to VMs in the same project, not in the same region.
[reference]https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes"
You need to deploy your application to a Kubernetes Engine cluster using a manifest file called test1.yaml. Which command do you use for this?;kubectl apply -f test1.yaml;kubectl deploy -f test1.yaml;kubectl create -f test1.yaml;kubectl deployment -f test1.yaml;;A;"Option 1 is the correct answer. Execute the kubectl apply command to apply or update resources from a file or standard output. You can apply the contents of the manifest file to Kubernetes resources by running `kubectl apply -f [manifest file name]`.
Option 2 is incorrect. The kubectl deploy command is not a valid command. Use kubectl apply.
Option 3 is incorrect. The kubectl create command -f FILENAME [flags] creates one or more resources from a file or standard output. It is not used to apply or update resources,
Option 4 is incorrect. The kubectl deployment command is not a valid command. Use kubectl apply.
[reference] https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply"
You plan to migrate that data to BigQuery to analyze user behavior data from your existing e-commerce site.Which factor is the cost of BigtQuery calculated based on?;Amount of data scanned by the query;Amount of data stored;The total amount of data scanned and the amount of data stored;Amount of data requested;;C;"Option 3 is the correct answer. BigQuery query analysis fees are based on the amount of data scanned and the amount of data stored. BigQuery calculates analysis fees based on SQL queries, user-defined functions, scripts, and specific data manipulation language (DML) and data definition language (DDL) statements that scan tables. BigQuery pricing consists of analytics usage fees and storage fees.
■ Analysis fee The cost of processing the query. Includes SQL queries, user-defined functions, scripts, and specific data manipulation language (DML) and data definition language (DDL) statements that scan tables.
■ Storage fee The cost of storing the data you load into BigQuery. You will also be charged for certain other operations, such as streaming inserts and the use of the BigQuery Storage API.
[reference]  https://cloud.google.com/bigquery/pricing"
"You are in charge of infrastructure management for Google Cloud. You used the gcloud scp command to upload a file from a local device to a Compute Engine VM, but the copy command fails.
What are the points to check to find out the cause?";Make sure that the ID that uses the resources is correct.;Make sure the firewall rule allows traffic to port 22 to allow SSH connections.;Make sure the developer ID has been added to the VM's admin group.;Grant compute.admin privileges to the ID.;;B;"Option 2 is the correct answer. gcloud compute scp uses the scp command to securely copy files between a virtual machine instance and a local machine. To transfer files using the scp command, you need a firewall rule that allows SSH connections on port 22 in the virtual network where the VM is located. SSH traffic can be communicated by allowing TCP port 22 in the firewall settings set for the virtual network.
Option 1 is incorrect. When connecting via SSH, specify the IP address instead of specifying the resource ID. To transfer files using the scp command, you need to make an SSH connection on port 22.
Option 3 is incorrect. You do not need to add it to the admin group to transfer files using the scp command.
Option 4 is incorrect. You do not need compute.admin privileges to transfer files using the scp command.
[reference]  https://cloud.google.com/compute/docs/instances/transfer-files"
"You're trying to list the roles assigned to a user in a project named udemy-pt.
Which command should you use?";gcloud projects iam list udemy-pt;gcloud projects get-iam-policy udemy-pt;gcloud projects list udemy-pt;gcloud iam get-iam-policy udemy-pt;;B;"Option 2 is the correct answer. You can get a list of IAM policies defined in your project by running the command `gcloud projects get-iam-policy [project ID]`. The gcloud projects command is a command that performs an action on a project. Get a list of IAM policies by specifying the project by writing get-iam-policy [project ID] `.
Option 1 is incorrect. gcloud projects iam list is not a valid command. You should use gcloud projects get-iam-policy.
Option 3 is incorrect. The gcloud projects list command lists the projects that can be accessed by the active account.
Option 4 is incorrect. gcloud iam get-iam-policy is not a valid command. You should use gcloud projects get-iam-policy.
[reference]  https://cloud.google.com/sdk/gcloud/reference/projects/get-iam-policy"
"Applications hosted on Google Cloud are accessed by many users globally. Therefore, we decided to configure a Cloud CDN to set up cache-based delivery processing.
How does this change the process?";Caches allow you to access data faster, wich reduces latency.;Data can be saved separately by keeping the cache.;Keeping the cache makes the backup process more efficient.;Access can be maintained by the cache.;;A;"Option 1 is the correct answer. Here, the merit of cache delivery using Cloud CDN is questioned. By using the CDN cache, you can store content and data processing results that frequently access the cache, resulting in faster access to the data. As a result, latency can be reduced.
Option 2 is incorrect. The cache can hold some data processing results for better access efficiency, but it is not used to store the data in the main database separately.
Option 3 is incorrect. The cache is not a function used for the backup process, so it is not correct.
Option 4 is incorrect. The cache of the WEB browser at the time of WEB access is used to maintain the session, but the cache processing of DB is used to improve the data access efficiency at the time of query execution.
[reference] https://cloud.google.com/cdn/docs"
"Your company has multiple accounts on Google Cloud. Account A is running in the default regions and zones, and Account B is running in non-default regions and zones. You need to launch a new Compute Engine instance in the two accounts using the command line interface.
Which command should you execute?";Use gcloud config configurations create [NAME] to activate the two configurations. Through gcloud active configurations [NAME], it switches accounts when running commands that launch ComputeEngine instances.;Use gcloud config configurations create [NAME] to create two configurations. gcloud config configurations activate [NAME] to activate [NAME] and switch accounts when running commands to launch ComputeEngine instances.;Use gcloud config configurations create [NAME] to create two configurations. From gcloud config activate [NAME], activate [NAME] and switch the account when executing the command to start the Compute Engine instance.;Use gcloud config configurations create [NAME] to activate the two configurations. gcloud activate configurations [NAME] to switch accounts when running commands to launch ComputeEngine instances.;;B;"Option 2 is the correct answer. To launch a new Compute Engine instance with two accounts using the command line interface, configure the following settings:
In order to launch a new Compute Engine instance with two accounts, you need to separate the projects. Therefore, use `gcloud config configurations create [NAME]` to create two project configurations. You can then use gcloud config configurations activate [NAME] to activate [NAME] and switch accounts when running commands that launch ComputeEngine instances. This will launch each new Compute Engine instance from a different account.
[reference]  https://cloud.google.com/sdk/gcloud/reference/config/configurations/create 
https://cloud.google.com/sdk/gcloud/reference/config/configurations/activate"
Which of the following is an inappropriate ID that can be used with Cloud Identity?;Gmail account;Gsuite domain users;Cloud Identity user;Google group;;A;"Option 1 is the correct answer. Cloud Identity is an IDaaS solution for Google Cloud that allows you to centrally manage users and groups that have access to cloud resources. Gmail can be used for registration emails in Cloud Identity, but a separate account registration is required for Google Cloud as a user account. Therefore, you cannot use your Gmail account as it is.
Option 2 is incorrect. Cloud Identity accounts are available as GSuite domain users.
Option 3 is incorrect. Cloud Identity accounts are available as Cloud Identity users.
Option 4 is incorrect. Cloud Identity accounts can use Google Groups.
[reference]  https://cloud.google.com/identity/docs/overview"
"Engineers are using virtual machines for batch processing. This batch process takes about 5 minutes to complete. Even if the process fails, there is no problem if you start another VM and re-execute. You want to minimize VM costs.
Which VM should you configure?";Preemptible VM;Shielded VM;GPU-enabled VM;Hardened VM;;A;"Option 1 is the correct answer. In this scenario, it takes about 5 minutes to complete, and even if the process fails, you need to perform a batch process can allow you to start another VM and re-execute. At the same time, it is also necessary to minimize the VM cost. Therefore, the VM you should choose is a preemptible VM that is cheaper and can process intermittently.
Preemptible VMs are instances that are available at a lower price than regular instances. This is suitable for temporary use such as use during automatic scaling. The disadvantage is that the Compute Engine can stop this instance if other tasks require access to the resource. Preemptable Instances are a feature that takes advantage of Compute Engine's surplus capacity, and availability depends on usage.
Option 2 is incorrect. Shielded VM is a feature that ensures that the VM has not been tampered with by boot-level or kernel-level malware or rootkits and is not relevant here.
Option 3 is incorrect. GPU-enabled VMs are GPU-equipped VMs that are suitable for building machine learning models, but it will not minimize costs. Option 4 is incorrect. There is no VM called Hardened VM.
[reference] https://cloud.google.com/compute/docs/instances/preemptible/"
"You use Google Cloud across multiple departments for your internal infrastructure. Each department has created multiple folders for some projects. Each folder has one or more projects.
Which elements can be shared and set by individual resources in one folder?";IAM Policy;project;Resource name;Metadata ID;;A;"Option 1 is the correct answer. Folders are units available only to Google Workspace and Cloud Identity users who have organizational resources and are used to group resources that share a common IAM policy. A folder can contain multiple folders and resources, but a folder or resource can only have one parent. You can set an IAM policy for a folder and set a common policy for the resources in the folder.
Option 2 is incorrect. Resources in the folder do not share the project. You can generate multiple projects under a folder. Therefore, there can be multiple projects in a folder, which can be different for each resource.
Option 3 is incorrect. Resources in folders do not share resource names.
Option 4 is incorrect. Resources in the folder do not share the metadata ID.
[reference] https://cloud.google.com/resource-manager/docs/creating-managing-folders"
"Your company develops an application on Google Cloud. To improve the security of your application, your engineer with limited permissions needs to create some custom roles.
Which of the following permissions does this engineer need to create a custom role?";roles/ iam.create.roles;roles/ iam.roles.create;roles/ iam.serviceAccountUser.create;roles/ iam.create.serviceAccountUser;;B;"Option 2 is the correct answer. You need the roles / iam.roles.create permission to create a custom role. By default, the owner of a project or organization has this privilege and can create and manage custom roles. Therefore, you need to grant the roles / iam.roles.create permission in order for anyone other than the project or organization owner to create a custom role.
Option 1 is incorrect. There is no authority called roles / iam.create.roles. Use roles / iam.roles.create.
Option 3 is incorrect. roles / iam.serviceAccountUser is the right to operate resources as a service account as a service account.
Option 4 is incorrect. The authority roles / iam.create.serviceAccountUser does not exist.
[reference]
https://cloud.google.com/iam/docs/creating-custom-roles#creating_a_custom_role"
The company uses App Engine to build web applications. You want to leverage your existing App Engine deployment to create a new project for production. How should you set this?;Copy an existing App Engine deployment to a new project and create a DeploymentManager configuration file.;Create a new project and copy the deployed application to the new project.;Launch a new project from an existing App Engine deployment and deploy it for production.;Create a new project and apply an existing App Engine deployment.;;A;"Option 1 is the correct answer. Copy the App Engine deployment, which is the application you are currently developing, to a new project and use it for your production environment. Doing so allows you to duplicate the DeploymentManager configuration file and perform multiple reproducible deployments, such as for production and test environments.
With Google Cloud Deployment Manager, you can easily specify all the resources your application needs in a yaml declarative format. You can also reuse common deployment paradigms such as load balancing and autoscaling instance groups by parameterizing your configuration with Python and Jinja2 templates. Reproducible deployment is achieved by treating the configuration as code.
[reference] https://cloud.google.com/deployment-manager/docs/deployments"
"The company is building an application based on a configuration that connects to CloudSQL on a Linux VM of Compute Engine.
What are the settings for running Cloud SQL from a VM?";Create a VM via the web console, specifying a service account with the appropriate JSON private key.;Download the JSON private key for the service account and create a VM via the web console.;Download the JSON private key for the service account, specify the service account with the appropriate IAM role, and create a VM through the web console.;Create a VM through the web console, specifying a service account with the appropriate IAM role.;;D;"Option 4 is the correct answer. To enable Cloud SQL to be executed from a VM, specify a service account with an IAM role that can execute Cloud SQL, and start and use the VM via the Web console. If you use the Cloud SQL Auth Proxy to connect to your Cloud SQL instance from your Compute Engine instance, you can use the default Compute Engine service account associated with your Compute Engine instance. The service account requires the Cloud SQL client role.
Options 1, 2 and 3 are incorrect. To execute Cloud SQL from a VM, grant permissions with the IAM role. The method of making a connection using the private key directly is not recommended for security.
[reference]
https://cloud.google.com/sql/docs/mysql/roles-and-permissions"
"You are developing an application that utilizes a machine learning model. You need to connect the GPU to the VM to speed up the computation.
What kind of settings do you need for this?";You only need to install the GPU library.;Memory and CPU must be compatible.;The GPU library must be installed and the CPU and GPU must be compatible.;CPU and GPU must be compatible.;;C;"Option 3 is the correct answer. In order to connect a GPU, the GPU library must be installed in the VM running the GPU, and the CPU of the VM must be compatible with the selected GPU.
The Compute Engine has a graphics processing unit (GPU) that you can add to your virtual machine (VM) instances. You can use these GPUs to accelerate certain workloads such as machine learning and data processing running on VMs.
Option 1 is incorrect. In addition to installing the GPU library, it must be CPU compatible.
Option 2 is incorrect. Memory compatibility is irrelevant. Option 4 is incorrect. You need to install the GPU library.
[reference] https://cloud.google.com/compute/docs/gpus"
Select the Google Cloud virtual machine system management operations that you are not allowed to perform.;Configure the file system;Patch operating system software;Change file and directory permissions;Change the settings of the hosted infrastructure.;;D;"Option 4 is the correct answer. The virtual machine's system administration privileges allow all operational actions after the virtual machine is created. Therefore, virtual machine system administration privileges allow you to configure the virtual machine's file system, patch operating system software, and change file and directory permissions.
However, only virtual machine operations are allowed, and management of hosted infrastructure is subject to management on the Google Cloud side. Therefore, the operation of option 4 is not permitted."
"As a Google Cloud administrator, you have defined an IAM role and applied it to your existing project. When launching a new project, you want to use the same IAM role for the new project.
How would you do this?";Copy and duplicate your organization with the geloud iam roles copy command.;"Specify the project as the copy destination in the IAM
console.";Use the gcloud iam roles copy command to specify an organization and identify existing roles to copy, then set them in the new project.;Use the IAM console to specify an organization and identify existing roles to copy, then set them in the new project.;;C;"Option 3 is the correct answer. With the gcloud iam roles copy command, you can specify an organization to identify the roles to copy, and then copy and configure configured roles to other organizations. The command defines detailed settings as follows: This allows you to use the same IAM role for new projects when launching new projects. The commands are:

gcloud iam roles copy [--dest-organization=DEST_ORGANIZATION] [--dest-project=DEST_PROJECT] [--destination=DESTINATION] [--source=SOURCE] [--source-organization=SOURCE_ORGANIZATION] [--source- project = SOURCE_PROJECT] [GCLOUD_WIDE_FLAG…]"
"You used the CLI tool to set the default gcloud parameter using development account A. Next, in order to perform the operation in development account B, you want to switch the account.
Which command should you use?";Run the --gcloud account update command on the command line.;Run the --gsutil account update command on the command line.;Pass the --account parameter on the command line.;Run the --user parameter on the command line.;;C;"Option 3 is the correct answer. You are setting up using two accounts, development account A and development account B. Therefore, you need to specify either account. All gcloud commands allow you to specify a run-time account with the --acount parameter.
Option 1 is incorrect. ‘--gcloud account update‘ is not a valid command.
Option 2 is incorrect. ‘--gsutil account update‘ is not a valid command.
Option 4 is incorrect. ‘You cannot specify a run-time account with the --user parameter. Use the --account parameter.‘
[reference]  https://cloud.google.com/sdk/gcloud/reference"
"A company uses Google Cloud to develop Docker container-based applications. The same settings are required for multiple servers hosting this application.
What is the best node pool needed to configure Kubernetes Engine?";A subset of node instances in a cluster that all have the same configuration;A subset of the set of virtual machines integrated by the Kubernetes Engine;A subset of nodes throughout the cluster;A subset of the cluster that splits the shards in the cluster;;A;"Option 1 is the correct answer. In this scenario, the application requires the same server settings, so it is best to configure a subset of node instances in a cluster that all have the same configuration.
A node pool is a subset of node instances in a cluster that all have the same configuration. Each node in the pool is set to the Kubernetes node label cloud.google.com/gke-nodepool. It is given the name of the node pool as a value. The number of nodes and node type specified when creating the cluster will be the default node pool. You can then add custom node pools of different sizes and types to the cluster. All nodes in a particular node pool will be the same.
[reference] https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools"
"You are setting a snapshot of the persistent disk you are currently using. Your boss asked you to submit a list of the snapshots that are currently being created.
Which command do you need to run?";gcloud compute snapshots describe;gcloud compute snapshots list;gcloud compute snapshots show;gcloud compute snapshots view;;B;"Option 2 is the correct answer. Run the gcloud compute snapshots list command to list Google ComputeEngine snapshots. This allows you to create a list of currently created snapshots.
Option 1 is incorrect. The gcloud compute snapshots describe can display information about specific snapshots, such as creation time, size, and source disk. You can also get the specified snapshot information by running the command `gcloud compute snapshots describe [snapshot name]`.
Option 3 is incorrect. gcloud compute snapshots show is not a valid command
Option 4 is incorrect. gcloud compute snapshots view is not a valid command
[reference] https://cloud.google.com/sdk/gcloud/reference/compute/snapshots/describe"
"Your company decided to use Google Cloud to set up a Docker container-based CI / CD environment. You need to create a Kubernetes cluster with autoscaling enabled for the node.
Which command should you execute?"; kubectl container create clusters [cluster name) --enable-autoscaling--max-nodes=MAX_NODES--min-nodes=MIN NODES;gcloud container create clusters [cluster name]--enable-autoscaling--max-nodes=MAX_NODES--min-nodes=MIN NODES;kubectl container clusters create [cluster name]--enable-autoscaling--max-nodes=MAX NODES --min-nodes=MIN NODES;gcloud container clusters create [cluster name]--enable-autoscaling--max-nodes=MAX NODES --min-nodes=MIN NODES;;D;"Option 4 is the correct answer. You can create Kubernetes clusters by running the command `gcloud container clusters create [cluster name]`. You can set autoscaling for the node pool by enabling the --enable-autoscaling, and the number of nodes when autoscaling by specifying the --min-nodex and --max-nodes parameters respectively. You can set the lower / upper limit of.
Options 1 and 3 are incorrect. The kubectl command is a command for managing Kubernetes resources from within a Kubernetes cluster and cannot be used to create the cluster itself.
Option 2 is incorrect. gcloud container create clusters with is not a valid command order.
[reference] https://cloud.google.com/sdk/gcloud/reference/container/clusters/create"
"You are developing multiple components that require inter-resource coordination. It is necessary to check the role assigned to the resource that hosts the component.
Which command lists the roles assigned to a resource?";gcloud iam grantable-roles;gcloudlist-grantable-roles;gcloud iam list-grantable-roles;gcloud iam role list-grantable-roles;;C;"Option 3 is the correct answer. gcloud is a command line tool for working with IAM. Perform operations related to IAM by using gcloud iam. After that, you can list the roles assigned to the resource by setting list-grantable-roles. Option 1 is incorrect. You can list the roles assigned to a resource by using list-grantable-roles after gcloud iam. gcloud iam grantable-roles is not is not sufficient.
Option 2 is incorrect. The correct command is gcloud iam list-grantable-roles, not gcloud list-grantable-roles.
Option 4 is incorrect. The correct command is gcloud iam list-grantable-roles, not gcloud iam role list-grantable-roles.
[reference]  https://cloud.google.com/sdk/gcloud/reference/iam/list-grantable-roles"
"As a system administrator, you have set up monitoring of Google Cloud resources using the Operations Suite. You need an open source option available to work with custom metrics.
Which of the following can you choose?";OpenCensus;Logging agent in the operations suite;Operation suite Nagios;Operations Suite Cloud Tracer;;A;"Option 1 is the correct answer. OpenCensus is available as an open source option available for working with custom metrics. OpenCensus is a library for developing custom metrics that can be used in the operation suite Logging. OpenCensus is a distributed trace collection tool developed by Google and is available as open source.
Option 2 is incorrect. The Google Cloud Operations Suite provides an agent for collecting logs from Linux and Windows VM instances. The Logging agent streams logs to Logging from popular third-party applications and system software. This is a tool for collecting logs and cannot be used to manipulate custom metrics.
Option 3 is incorrect. Nagios is application software for monitoring open source computer systems and networks. It is not a tool for working with custom metrics.
Option 4 is incorrect. Cloud Trace is a distributed tracing system that collects latency data from your application and displays it in the Google Cloud Console. It tracks the propagation of requests exchanged within your application and provides performance analytics in near real time. It is not a tool for manipulating custom metrics.
[reference]  https://cloud.google.com/monitoring/custom-metrics/open-census"
"Your company stores EC site data in BigQuery and performs daily data analysis. There is concern that the cost of BigQuery will be too high due to the huge amount of data on the EC site. Most queries are run based on the time the data arrived.
It is necessary to reduce the cost of queries without compromising the usefulness of this analytics service. What should you do to reduce the amount of data scanned?";Use a table with the capture time partitioned and specify the. PARTITIONTIME filter when querying.;Use the LIMIT option in the SELECT statement to configure it so that it does not return more than a fixed number of rows.;Distribute the read process by using the readreplica.;Divide the table into multiple parts and assign processing nodes.;;A;"Option 1 is the correct answer. You can partition the table based on the time the data arrived by using a partitioned table with uptake times and specifying the _PARTITIONTIME filter when querying. By doing so, distributed processing becomes possible and query execution processing becomes efficient. A partitioned table is a special table that is divided into segments called partitions, which makes it easier to manage and query data.
You can improve query performance by partitioning a large table into smaller partitions, and you can reduce costs by reducing the number of bytes read by the query.
Option 2 is incorrect. You can reduce the amount of data by using the LIMIT option, but this will limit the number of rows that can be retrieved and may prevent sufficient data retrieval. It is better to refine and analyze the scope of the query by using a partitioned table.
Option 3 is incorrect. By decentralizing the processing using Read Replica, you can decentralize the performance of multiple read processing and improve the performance, but the total amount of data scanned by one query does not change.
Option 4 is incorrect. Dividing the table into multiple pieces and assigning nodes to each does not change the total amount of data scanned.
[reference]  https://cloud.google.com/bigquery/docs/partitioned-tables"
"You are migrating a multi-tiered application hosted in your on-premises data center to Google Cloud. It is a requirement to streamline network management while creating unique projects for each layer.
Which Google Cloud features do you need to do that?";Set up VPC network peering.;Set up a VPN.;Create a shared VPC;Set up a leased line.;;C;"Option 3 is the correct answer. Shared VPCs allow organizations to connect resources from multiple projects to a common VPC network, allowing secure and efficient intercommunication between multiple projects using the network's internal IP.
In a shared VPC, you specify a project as the host project and connect other service projects to the host project. The VPC network in your host project is called a shared VPC network. Eligible resources for service projects can use subnets within a shared VPC network.
Efficient network management is possible by using a shared VPC to centrally manage network resources such as subnets, routes, and firewalls, while delegating management responsibilities such as instance creation and management to the service project administrator. 
Option 1 is incorrect. VPC network peering is a service that connects different VPCs. It is used when a connection between multiple VPCs is required, but this time we need a function to streamline network management.
Option 2 is incorrect. VPN is a service that connects your on-premises network to your VPC.
Option 4 is incorrect. Leased line is a service that connects the on-premises environment and the Google Cloud environment.
[reference]  https://cloud.google.com/vpc/docs/shared-vpc"
The company uses App Engine to run multiple services. When one service ran a complex query, it didn't return the expected data. Which file in your application should be examined first to diagnose this issue?;index.yaml;cron-yaml;app.yaml;config.yaml;;A;"Option 1 is the correct answer. The problem with this scenario is that when the service ran one complex query, it didn't return the expected data. Complex queries require you to look at a configuration file called index.yaml. Indexes for simple queries (such as queries for a single property) are created automatically, but indexes for complex queries can be defined in a configuration file called index.yaml. Therefore, you need to make sure that index.yaml has the required index for your application.
Option 2 is incorrect. cron.yaml is a file for setting a scheduled task (cron job) that operates at a specified time or at regular intervals, and this time it is irrelevant.
Option 3 is incorrect. App Engine app settings are configured in the app.yaml file. This file specifies the correspondence between URL paths and request handlers and static files. The app.yaml file also contains information about your app code, such as the runtime and the latest version of the ID. This has nothing to do with the execution of the query.
Option 4 is incorrect. A file called config.yaml does not exist as a configuration file for App Engine.
[reference]  https://cloud.google.com/appengine/docs/flexible/java/configuring-datastore-indexes-with-index-yaml#index_definitions"
"Your company needs to use the SSH protocol to access the Compute Engine VM.
Which function should be enabled in order to save the SSH host key?";Guest attribute;label;Shielded VM;Single tenant node;;A;"Option 1 is the correct answer. Use the guest attribute to store the SSH host key. Saving the SSH host key as a guest attribute protects the connection from vulnerabilities such as man-in-the-middle (MITM) attacks. If the guest attribute is enabled, the Compute Engine saves the generated host key as a guest attribute on the first boot of the VM, and then Compute Engine uses the saved host key to migrate to the VM. Validate all connections in.
Option 2 is incorrect. Labels are a way to easily group resources that are related to each other and are not used to store host keys.
Option 3 is incorrect. Shielded VM is a function to confirm that the VM has not been tampered with by boot-level or kernel-level malware or rootkits and to enhance security, and it is not used to store the host key.
Option 4 is incorrect. The single tenant node is a feature that physically separates the VM from the VMs of other projects and is not used to store the host key.
[reference] https://cloud.google.com/solutions/connecting-securely/#storing_host_keys_by_enabling_guest_attributes"
The company uses Kubernetes to develop Docker applications. Your boss asked you to analyze the resource usage of each team that uses the Kubernetes Engine cluster. What should be used to distinguish resource usage by team?;Cluster label;Namespace;Resource name;Metadata;;A;"Option 1 is the correct answer. Use cluster labels to analyze resource usage by teams that utilize Kubernetes Engine clusters. Cluster labels are key-value pairs that help you organize your cluster in Google Cloud. You can set a label for each resource and filter resources based on that label. Information about labels is transferred to the billing system, so you can also analyze billing charges by label.
Option 2 is incorrect. The namespace is meant to support the operation of multiple virtual clusters on the same physical cluster and is not relevant here.
Option 3 is incorrect. It is not possible to analyze resource usage per Kubernetes cluster based on resource name. ""Resource"" is a named entity and ""Resource Name"" is its identifier. Each resource must have its own unique resource name. The resource name consists of the ID of the resource itself, the IDs of all parent resources, and its API service name.
Option 4 is incorrect. Metadata is intended to give additional information to resources and cannot be used for per-Kubernetes cluster resource usage analysis.
[reference]  https://cloud.google.com/kubernetes-engine/docs/how-to/creating-managing-labels"
"A network error occurred while running an application hosted on Cloud Storage. You sent the gsutil command to determine the cause, but it failed due to a temporary error.
Which of the following is correct result of executing the command?";Asks the user if re-execution is necessary,;Displays an error message and end the process.;Ends the message and records it in Cloud Shell.;Retries using truncated exponential backoff.;;D;"Option 4 is the correct answer. Cloud Storage retries network operations if a failure occurs by default. It then waits time before each retry. The wait time is calculated using a truncated exponential backoff. Therefore, you will have to retry with a truncated exponential backoff. All other options are incorrect.
[reference] https://cloud.google.com/storage/docs/retry-strategy"
Objects stored in Cloud Storage buckets need to be moved to the cold line after 90 days and deleted after 1 year. How do you do this?;Use gsutil rewrite to set the SetStorageClassaction to 90 days and the Delete action to 365 days.;Use gsutil rewrite to set the SetStorageClassaction to 90 days and the Delete action to 275 days.;In the CloudStorage object lifecycle rule, set the SetStorageClassaction to 90 days and the Delete action to 365 days.;In the life cycle rule of the CloudStorage object, set the SetStorageClassaction to 90 days and the Delete action to 275 days.;;C;"Option 3 is the correct answer. In the life cycle rule of CloudStorage object, you can change the storage class after 90 days by setting SetStorageClassaction to 90 days.
Similarly, for the Delete action, select the Age condition, enter the value 365, select the Matching Storage Class condition, and select the Coldline option in the file you moved to Coldline. , The object will be deleted 365 days after the object was created.
Lifecycle rules specify either the Delete action or the SetStorageClass action. The Delete action deletes an object if it meets all the conditions specified in the lifecycle rule. Specifying the SetStorageClass action modifies the object's storage class if the object meets all the conditions specified in the lifecycle rule.
[reference]  https://cloud.google.com/storage/docs/lifecycle"
"You are managing your company’s first Google Cloud project. Project leads, developers, and internal testers will participate in the project, which includes sensitive information.
You need to ensure that only specific members of the development team have access to sensitive information. You want to assign the appropriate Identity and Access Management (IAM) roles that also require the least amount of maintenance.
What should you do?";Assign a basic role to each user.;Create groups. Assign a basic role to each group, and then assign users to groups;Create groups. Assign a Custom role to each group, including those who should have access to sensitive data. Assign users to groups.;Create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Assign users to groups.;;D;"Option 4 is correct because Predefined roles are fine-grained enough to set permissions for specific roles requiring sensitive data access. This solution also uses groups, which is the recommended practice for managing permissions for individual roles.
Option 1 is not correct for two reasons: The recommended practice is to use groups and not to assign roles to each user. Beyond that, Basic Roles do not have enough granularity to account for access to sensitive data.
Option 2 is not correct because Basic roles do not have enough granularity to account for access to sensitive data.
Option 3 is not correct because creating and maintaining Custom roles will require more maintenance than using Predefined roles.
[reference]
https://cloud.google.com/iam/docs/understanding-roles 
 https://cloud.google.com/iam/docs/understanding-custom-roles"
"Your team's application under development will have a redundant configuration using multiple Compute Engines. You need a managed instance group that automatically adds instances when the load increases significantly. 
Which function is required for this setting?";snap shot;Instance template;VM image;backup;;B;"Option 2 is the correct answer. An instance template is required to configure Compute Engine to use autoscaling to add instances to a managed instance group. Instance templates are resources that you can use to create virtual machine (VM) instances and managed instance groups (MIGs). Configure scaling based on the instance template.
Option 1 is incorrect. Snapshot is not relevant to this case as it is a feature that incrementally backs up data from persistent disks.
Option 3 is incorrect. A VM image is image data for configuring a virtual machine. You can use this to generate a VM, but it cannot be used directly for autoscaling to add instances automatically.
Option 4 is incorrect. Creating a backup does not automatically add an instance to a managed instance group.
[reference] https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances"
"Your company has a Hadoop cluster in an on-premises environment for big data analytics solutions. We have decided to move this solution to Google Cloud. You need to minimize the operational overhead of your Hadoop cluster.
Which of the following Google Cloud services should you use?";Cloud Pub/Sub;Cloud Dataproc;Cloud Dataflow;Cloud Engine;;B;"Option 2 is the correct answer. Cloud Dataproc is a managed Spark / Hadoop service that you can use to migrate your Hadoop cluster Google Cloud. Since it is a managed type, most of the settings and operations are performed on the Google Cloud side. This minimizes the operational overhead of your Hadoop cluster.
Option 1 is incorrect. Cloud Pub / Sub is a message service with queues available that are used to capture and store data until it can be processed. It is not available for Hadoop clusters.
Option 3 is incorrect. Cloud Dataflow is a stream and batch processing service and is not a solution to minimize the operational overhead of Hadoop clusters.
Option 4 is incorrect. Compute Engine is not a managed service and requires infrastructure management, which increases the operational overhead of a Hadoop cluster.
[reference]  https://cloud.google.com/dataproc/docs/how-to"
"It seems that there was unauthorized access to the application hosted on Google Cloud. You need to view the log entries to see what you think is suspicious.
How do you get a fixed log entry in the resource context?";Click View Details next to the log entry.;View log entries one at a time.;Select the pin icon to display the log entry.;Click Expand All.;;C;"Option 3 is the correct answer. To use the Log Explorer to view pinned log entries within the resource context, select the pin icon and then select Pin and Show Resource Log Entries. You can pin log entries and highlight the desired log entries. To pin a log entry, hover over the log entry you want to pin and select the pinned icon.
[reference]  https://cloud.google.com/logging/docs/view/logs-viewer-interface?hl=ja#resource-context"
"Engineers are deploying applications developed using App Engine. This application needs to scale the instances as the number of requests increases, which requires at least two unoccupied instances.
Which scaling type should you use?";Autoscaling configuration with min _idle_instances set to 2; Distributed configuration with min_instances set to 2;Distributed configuration with min_idle_ instances set to 3;Auto scaling configuration with min _instances set to 3;;A;"Option 1 is the correct answer. In this scenario, we need at least two unoccupied instances to scale the instances as the number of requests increases. You can launch two unoccupied instances by setting autoscaling with min_idle_instances set to 2.
In App Engine autoscaling, you can set min_idle_instances to specify the number of running instances in addition to the calculated number of instances. For example, if App Engine calculates that it needs 5 instances to process traffic and sets min_idle_instances to 2, App Engine runs 7 instances. This launches 5 instances calculated based on traffic, as well as 2 waiting instances added by min_idle_instances. 
[reference]
https://cloud.google.com/appengine/docs/standard/python/config/appref https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed"
"You are required to store backup files on Google Cloud to meet your company's disaster recovery plans.
Which storage option should you use?";Archive storage;cold line storage;Nearline storage;Standard storage;;A;"Option 1 is the correct answer. Archive storage is the lowest cost and most durable storage service for data archiving, online backup and disaster recovery. Archive storage is optimal because backup files are backups that are stored over the medium to long term to support disaster recovery plans. Archive storage, unlike other storage classes, does not have availability SLAs, but general availability is comparable to nearline or coldline storage.
Option 2 is incorrect. Cold-line storage is suitable for data that is read or modified only about once a quarter. However, archiving storage is a more cost-effective option if you want to keep your data completely long-term for backup and archiving processes.
Option 3 is incorrect. Nearline storage is suitable for data that is read or modified about once a month, and is not suitable for backing up whether it is accessed once a year.
Option 4 is incorrect. Standard storage is ideal for frequently accessed data (“hot” data) or data that is stored for a short period of time.
[reference]  https://cloud.google.com/storage/docs/storage-classes"
"Your company stores important internal files in Cloud Storage. It is necessary to manage these files properly, but you decided to set life cycle rules to automate this as much as possible.
Which operations can be automated using life cycle rules?";Rename the file;Setting the storage class;Analyzing the storage class on a regular basis.;Moving the file to another folder.;;B;"Option 2 is the correct answer. You can only delete files and change storage classes using lifecycle rules. If the file meets all the conditions specified in the lifecycle rule, the file is deleted or the storage class is changed.
Option 1 is incorrect. You cannot rename files using lifecycle rules.
Option 3 is incorrect. You cannot use lifecycle rules to perform periodic analysis of storage classes.
Option 4 is incorrect. You cannot move files to another folder using lifecycle rules.
[reference] https://cloud.google.com/storage/docs/lifecycle"
"You used a command line tool to create a custom mode network on Google Cloud.
What is the work required to deploy an instance to multiple regions?";Create a subnet in the zone where you want to deploy your instance.;Create a subnet in the region where you want to deploy your instance.;Specify the zone in the region where you want to deploy the instance.;Specify only the region where you want to deploy theinstance.;;B;"Option 2 is the correct answer. If you created a custom mode network, you need to create a subnet for each region in order to deploy the instance to multiple regions.
Option 1 is incorrect. You need to create subnets in regions, not zones.
Option 3 is incorrect. There is no particular need to specify a zone.
Option 4 is incorrect. Instead of just specifying the region where you want to deploy the instance, specify the region before creating the subnet.
[reference]  https://cloud.google.com/vpc/docs/using-vpc"
"Your company uses multiple projects on Google Cloud. At that time, it is necessary to clarify the license management in different servers. However, it seems difficult for a regular VM to meet the hardware requirements which require a per-core or per-processor license.
Which Compute Engine VM should you use?";Preemptible VM;Single tenant VM;Single tenant node;Shielded VM;;C;"Option 3 is the correct answer. Single-tenant nodes help meet dedicated hardware requirements in customer-owned license usage (BYOL) scenarios that require per-core or per-processor licenses. Single-tenant nodes provide visibility into the underlying hardware so you can track core and processor usage. To track usage, the Compute Engine reports the ID of the physical server on which the VM is scheduled.
Option 1 is incorrect. Preemptible VMs are short-lived, affordable compute instances suitable for batch jobs and fault-tolerant workloads.
Option 2 is incorrect. There is no such thing as a single tenant VM.
Option 4 is incorrect. Shielded VMs are a feature that enhances security by ensuring that VMs have not been tampered with by boot-level or kernel-level malware or rootkits.
[reference]  https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes"
"Developers are developing applications using Docker containers and Cloud Pub / Sub. At that time, you need the ability to send a Pub / Sub message on upload using the Cloud Storage trigger mechanism. All files should be processed correctly and operational overhead should be minimized.
Which Google Cloud computing service should you use?";Compute Engine;Cloud GPU;Anthos;Cloud Run;;D;"Option 4 is the correct answer. In this scenario, use the Cloud Storage trigger mechanism to send a Pub / Sub message on upload. The requirement is to minimize operational overhead while ensuring that all files are processed correctly. By using Cloud Run, you can send Pub / Sub messages using the trigger mechanism of Cloud Storage.
Cloud Run is a managed computing platform that can run stateless containers that can be called via web requests or Pub / Sub events. Cloud Run is serverless and requires no infrastructure management, minimizing operational overhead.
Option 1 is incorrect. Compute Engine is not a managed service and requires infrastructure management, which increases operational overhead.
Option 2 is incorrect. Cloud GPU is a service that provides high performance GPUs that can be used for machine learning, scientific computing, and 3D display, and this time it is not relevant.
Option 3 is incorrect. Anthos is a state-of-the-art application management platform for consistent development and operation in cloud and on-premises environments, and this time it's irrelevant.
[reference]  https://cloud.google.com/run/docs"
"Your company needs to set up a hybrid cloud environment that integrates the on-premises environment and Google Cloud.
Which network service do you use to make a dedicated connection from your data center to Google Cloud?";VPC Network Peering;Google Cloud Carrier Internet Peering;Google Cloud Dedicated Interconnect;Google Cloud Direct Peering;;C;"Option 3 is the correct answer. You will need to use the Dedicated Interconnect to make a dedicated line connection from your data center to Google Cloud. Dedicated Interconnect is a direct physical connection between your on-premises network and your Google network.
Dedicated Interconnect provisions a Dedicated Interconnect connection between your Google network and your router at a common location.
Option 1 is incorrect. Google Cloud VPC network peering allows you to connect internal IP addresses regardless of whether two Virtual Private Cloud (VPC) networks belong to the same project or the same organization.
Option 2 is incorrect. Carrier peering allows you to use your service provider to access Google applications such as Google Workspace and get enterprise-class network services that connect your infrastructure to Google.
Option 4 is incorrect. Direct peering allows you to establish a direct peering connection between your Google Cloud user's business network and Google's edge network to send and receive high-throughput cloud traffic. However, this is not a physical connection. When used in Google Cloud, direct peering does not create custom routes in your VPC network. Traffic sent from resources in your VPC network goes through a route where the next hop is either the default Internet gateway (such as the default route) or the Cloud VPN tunnel.
[reference] https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview?hl=ja"
"One company uses Cloud Storage for its document sharing system. These documents need to be temporarily granted access to the required users. Also, this user does not have to be a Google Cloud user.
Which function should you use?"; Set an object ACL to allow access to the object.;Allow access to the object using a signed URL.;Set a custom role to give the user read permission.;Set a predefined role to give the user read permission.;;B;"Option 2 is the correct answer. Allows users time-limited read or write access to objects through signed URLs. Everyone who shares the URL can access the object at a specified time.
Option 1 is incorrect. By setting an object ACL to allow access to the object, it is possible to specify the user's ID and IP address, but medium- to long-term access will be permitted.
Option 3 is incorrect. If you set a custom role and give the user read permission, medium- to long-term access will be granted.
Option 4 is incorrect. If you set a predefined role and give the user read permission, medium- to long-term access will be granted.
[reference]https://cloud.google.com/storage/docs/managing-lifecycles/"
"You are developing an image processing application. The image is processed by Cloud Functions created in Python, triggered by the image being uploaded to Cloud Storage.
Which storage event do you want to use as the trigger event?";google.storage.object.finalize;google.storage.file.finalize;google.storage.object.archive;google.storage.object.delete;;A;"Option 1 is the correct answer. If you run `google.storage.object.finalize, this event will be sent when a new object is created (or an existing object is overwritten) in the bucket. This allows you to process the image triggered by the image being uploaded to Cloud Storage. By linking Cloud Functions, it is possible to execute a serverless application triggered by an event in which a new object is created.
Cloud Functions uses event-driven functions to handle events in your cloud infrastructure. For example, Cloud Functions can respond to change notifications sent by Google Cloud Storage. You can configure these notifications to be triggered in response to various events in your bucket (object creation, deletion, archiving, metadata updates).
Option 2 is incorrect. google.storage.file.finalize is not a valid command. Let's say google.storage.object.finalize.
Option 3 is incorrect. google.storage.object.archive will be sent when the live version of the object is archived or deleted.
Option 4 is incorrect. google.storage.object.delete will be sent when the object is completely deleted.
[reference] https://cloud.google.com/functions/docs/calling/storage"
"You want to create a set of custom images for a VM to configure multiple VMs.
From which resource can you create the image?";snap shot;disk;Another image;All of the above;;D;"Option 4 is the correct answer. It is possible to create an image from all of these targets and launch the VM. ComputeEngine allows you to create custom images from source disks, images, snapshots, and images stored in Cloud Storage, and use those images to create virtual machine (VM) instances.
[reference] https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images"
"Your company uses multiple Google Cloud billing accounts, and the billing process is mixed. You want to centralize all these projects into a single new billing account.
What should you do?";Create a new billing account in the console and set the payment method.;Create a new billing account from the CLI tool and set the payment method.;Ask Google Support to set up a new billing account.;Set multiple new billing users in your account settings.;;A;" Option 1 is the correct answer. After creating a new billing account from the console and setting the payment method, you need to centralize the created account by linking it to an existing project. This allows you to centralize your billing by linking all the projects you want to centralize your billing to a single new billing account.
Option 2 is incorrect. You cannot create a new billing account from the CLI tool.
Option 3 is incorrect. You don't need to contact Google Support to create a new billing account. It can be created by the user from the console screen.
Option 4 is incorrect. There is no such thing as setting multiple new billing users.
[reference]  https://cloud.google.com/billing/docs/how-to/manage-billing-account 
https://cloud.google.com/sdk/gcloud/reference/alpha/billing/accounts#COMMAND"
"You store large amounts of image and video content in Cloud Storage for internet education services and publish it on your application. Users all over the world are using this service.
Which Google Cloud services can you use to efficiently deliver the most frequently accessed content?";Cloud VPN;Google Cloud hybrid connection;VPC peering;Cloud CDN;;D;" Option 4 is the correct answer. Cloud CDN is a service that stores and distributes HTTP (S) load-balanced content in a cache near the user using Google's edge connection bases distributed all over the world. By caching content such as images and videos at the edge of the Google network, you can deliver content to users faster while reducing costs.
Option 1 is incorrect. Cloud VPN is a service that connects your on-premises or other public cloud networks to your Google VPC securely and at low cost over the Internet via IPSec VPN. It doesn't matter in this scenario.
Option 2 is incorrect. Google Cloud hybrid connectivity refers to the entire service for connecting infrastructure on-premises or other public clouds to Google Cloud. It doesn't matter in this scenario.
Option 3 is incorrect. VPC peering is a service that connects VPC networks so that workloads in different VPC networks can communicate internally. It doesn't matter in this scenario.
[reference] https://cloud.google.com/cdn/docs"
"Your company is developing an image distribution application. This application stores and shares images in a Cloud Storage bucket called ""test1"".
Which command should you use to allow the image to be read by all users?";gsutil iam ch -d objectViewer: allUsers gs: // test1;gsutil iam ch -d allUsers: Viewer gs: // test1;gsutil iam ch allUsers: objectViewer gs: // test1;gcloud iam ch objectViewer: allUsers gs: // test1;;C;"Option 3 is the correct answer. By running the'gsutil iam ch allUsers: objectViewer gs: // test1'command, you can set permissions for all users to read the images stored in the Cloud Storage bucket called test1. Use gsutil instead of gcloud for commands for Cloud Storage. ObjectViewer is also the correct way to grant read access to objects in your bucket. The gsutil iam ch command grants all users read permission for the images stored in the bucket. By writing gsutil iam ch allUsers: objectViewer after it, specify all users and set the read permission of the object.
Options 1 and 2 are incorrect. Granting -d removes the permission.
Option 4 is incorrect. gcloud iam ch objectViewer: allUsers gs: // test1 is not a valid command because ""objectViewer: allUsers"" is reversed.
[reference]  https://cloud.google.com/storage/docs/gsutil/commands/iam"
"As an infrastructure administrator, you are backing up your Cloud Datastore. The backup file should be stored in a bucket named test1-backup.
Which command should you use to perform the backup?";gcloud datastore backup gs: // test1-backup;gcloud datastore indexes gs: // test1-backup;gcloud datastore operations gs: // testi-backup;gcloud datastore export gs: // test1-backup --async;;D;"Option 4 is the correct answer. You can export Cloud Datastore entities to Cloud Storage by running the command `gcloud datastore export [destination url]`. The gcloud datastore command is a command for managing Cloud Datastore. gcloud datastore export is a command to execute data extraction, and it is executed by specifying the output destination as gcloud datastore export [output destination url] `.
The main actions are:
· Export: exports data to Google Cloud Storage
· Import: Imports data from Google Cloud Storage
· Indexes: Manages CloudFirestore indexes.

· Operations: Manages long-running operations of Cloud Firestore.
Option 1 is incorrect. The command gcloud datastore backup is not a valid command.
Option 2 is incorrect. indexes manages Cloud Firestore indexes.
Option 3 is incorrect. operations manage the long-running operations of Cloud Firestore.
[reference]  https://cloud.google.com/datastore/docs/export-import-entities"
As a developer, you are developing an application that performs reads and writes on two Cloud Storage buckets. What settings do applications need to have to get permission to read and write to this Cloud Storage bucket?;Create a service account and grant a bucket policy that provides read and write permissions,;Create a federation ID and grant a bucket policy that provides read and write permissions.;Create a service account and grant it a role that provides read and write permissions.;Create a federation ID and grant a bucket policy that provides read and write permissions.;;C;"Option 3 is the correct answer. Service accounts are non-human users and are used by applications and the like to access resources on the cloud without the involvement of end users or to perform actions. You can control permissions on resources by assigning an IAM role to a service account. Therefore, you need to grant a service account for your application to allow reads and writes to this Cloud Storage bucket.
Option 1 is incorrect. Control permissions with an IAM role, not a bucket policy.
Options 2 and 4 are incorrect. Federation ID is an authorization method used when delegating authentication to an external identity provider. You can use this to authenticate to your application or Google Cloud. Not appropriate for this requirement.
[reference]  https://cloud.google.com/iam/docs/best-practices-for-using-and-managing-service-accounts
https://cloud.google.com/storage/docs/access-control/iam"
"You are migrating your database system in your on-premises environment to Google Cloud. You have set a firewall rule to control ingress and egress traffic to the network for the VM, but it seems that the required traffic is blocked.
What is the method for diagnosing firewall traffic conditions?";Enable logging for subnets where firewalls are installed.;Enable firewall rule logging for firewall rules;Enable logging for VPCs with firewalls installed.;Monitor firewall rules using CloudDebugger.;;B;"Option 2 is the correct answer. You can use the firewall rule log to audit, validate, and analyze the effectiveness of firewall rules. For example, you can determine if a firewall rule designed to deny traffic is working as intended. Therefore, by analyzing the firewall rule log, it is possible to verify the communication status to see if the required traffic is blocked.
Option 1 is incorrect. There is no log called the log of the subnet where the firewall is installed. Use the VPC flow log to check access to the subnet.
Option 3 is incorrect. The VPC flow log records the network flow sent and received by the VM instance, and it is not possible to check the traffic status based on the firewall.
Option 4 is incorrect. Cloud Debugger is a service that allows you to investigate the status of a running application in real time without stopping or slowing down the execution speed. It is not possible to check the traffic status based on the firewall.
[reference]  https://cloud.google.com/vpc/docs/firewall-rules-logging"
"Your company has decided to set up a Docker container-based CI / CD environment on Google Cloud. You need to create a set of Docker images to better manage groups of containers.
Which command would you run to group the container images?";"Add labels using the ""gcloud container images add-label""command.";Add the configuration using the gcloud container images add-configuration command.;Add settings using the gcloud beta container images list-tags command.;Add tags using the gcloud container images add-tag command.;;D;"Option 4 is the correct answer. You can add tags to images by running the command `gcloud container images add-tag [image name before tagging] [image name after tagging]`. You can use this tag to manage your images. gcloud container is a command that deploys and manages a cluster of machines to run the container. gcloud beta container images add-tag allows you to tag images. Add the actual image name, such as `gcloud container images add-tag [image name before tagging] [image name after tagging]`.
Option 1 is incorrect. gcloud container images add-label is not a valid command.
Option 2 is incorrect. gcloud container images add-configuration is not a valid command.
Option 3 is incorrect. gcloud beta container images list-tags lists the tags.
[reference] https://cloud.google.com/container-registry/docs/managing#tagging_images"
"As an engineer, you are building an application on Google Cloud that uses a distributed architecture. This application uses Cloud Pub / Sub to send messages to components for data analysis. Each message only needs to be sent once, but it seems that it can be sent multiple times by mistake.
Which parameters should be investigated to determine the cause?"; --dead-letter-topic; --ack -deadline; --min-retry-delay; --max-retry-delay;;B;"Option 2 is the correct answer. --ack-deadline specifies the number of seconds Cloud Pub / Sub publishers will wait on a retry. If it cannot confirm that the subscriber has received the message within the specified number of seconds, it will retry.
If this value is too small, the subscriber may receive the message correctly, but the message may be sent multiple times. It is necessary to identify the cause of multiple messages being sent by mistake. Therefore, check --ack-deadline to see if the optimal send interval has been set.
Option 1 is incorrect. --dead-letter-topic is a topic that forwards undeliverable messages (dead letter topic). This time it's not undeliverable, so it's irrelevant.
Option 3 is incorrect. --min-retry-delay is the minimum send interval (time) for sending messages in succession. It's possible that the optimal number of seconds to send, not the send interval, is the issue, so check --ack-dead-line.
Option 4 is incorrect. --max-retry-delay is the maximum send interval (time) for sending messages in succession. It's possible that the optimal number of seconds to send, not the send interval, is the issue, so check --ack-dead-line. 
[reference] https://cloud.google.com/sdk/gcloud/reference/beta/pubsub/subscriptions/update"
"You have been asked to check the default region and zone settings.
Which command line commands do you use to display the default regions and zones for a particular project?";gcloud compute project-info describe --project [PROJECT-IDI;gcloud compute project-info add-metadata --project=[PROJECT-ID];gcloud compute project-info describe --project=[PROJECT-ID];gcloud compute project-info add-metadata --project[PROJECT-ID];;A;"Option 1 is the correct answer. You can get the compute service settings for the specified project by running the command `gcloud compute project-info describe --project [project ID]`. Use the `gcloud compute` command because the default region and zone settings are for compute services. By adding project-info describe, it becomes a command to get a specific project setting.
Option 3 is incorrect. gcloud compute project-info describe --project = [PROJECT-ID] Instead, enter the project name with gcloud compute project-info describe --project [project ID] in between.Options 2 and 4 are incorrect. gcloud compute project-info add-metadata is a command used to set metadata.
[reference] https://cloud.google.com/sdk/gcloud/reference/compute/project-info"
"As a developer, you are trying to create an image named testimage1 from a snapshot named test1.
Which command should you use?";gcloud compute disks create testimage1 --source-snapshot test1;gcloud compute images create testimage1 --source-snapshot = test1;gcloud images create testimage1 --source-snapshot = test1;gcloud create images create testimage1 --source-snapshot=test1;;B;"Option 2 is the correct answer. You can create a new virtual image from a snapshot by running the command `gcloud compute images create [new image name] --source-snapshot = [snapshot name]`.
Option 1 is incorrect. The gcloud compute disks create testimage1 is a command that creates disks to attach to a virtual machine. 
Option 3 is incorrect. gcloud images create testimage1 is out of order and is not a valid command.
Option 4 is incorrect. gcloud create images create testimage1 is out of order and is not a valid command.
[reference] https://cloud.google.com/sdk/gcloud/reference/compute/disks/create#--source-snapshot"
"You have set up a persistent disk to connect to an instance running as a service account. Then you want to set the metadata for the instance that runs as a service account.
Select the role you need to do this.";roles/ iam.instanceAdmin.v1;roles/ compute.serviceAccountCreator;roles/ iam.serviceAccountCreator;roles/ compute.instanceAdmin.v1;;D;"Option 4 is the correct answer. Roles / compute.instanceAdmin.v1 is required to populate the instance with metadata. Members with both roles / compute.instanceAdmin.v1 and roles / iam.serviceAccountUser can create and manage instances that use the service account. Specifically, the following permissions are set:
-Create an instance to run as a service account

-Connecting a persistent disk to an instance running as a service account

-Setting instance metadata for an instance running as a service account

-Connect using SSH to an instance running as a service account

-Reconfiguring an instance running as a service account 
Option 1 is incorrect. There is no role called roles / iam.instanceAdmin.v1. Option 2 is incorrect. There is no role called roles / compute.serviceAccountCreator.
Option 3 is incorrect. roles / iam.serviceAccountCreator has the right to create a service account and not the right to operate resources as a service account.
[reference] https://cloud.google.com/compute/docs/access/iam"
"Your company stores internal documents in Cloud Storage buckets and uses them as a shared document management system. As a security officer, you need to make sure that all data written to this storage is encrypted.
Which of the following is the procedure for encrypting data when storing it in a CloudStorage bucket?";Encryption is performed by default;Enable encryption when creating a Cloud Storage bucket.;Set encryption in the life cycle policy settings.;Set encryption by bucket policy.;;A;"Option 1 is the correct answer. All data stored in Cloud Storage is automatically encrypted by default. The encryption methods of Cloud Storage are as follows: 
■ Server-side encryption Encryption that takes place after Cloud Storage receives the data, where the data is encrypted and then written to disk and stored. Uses the following two encryption keys:
-Can be managed using your own encryption key. These keys act as additional encryption that enhances standard Cloud Storage encryption.
-KMS allows you to create and manage cryptographic keys. These keys act as additional encryption that enhances standard Cloud Storage encryption. 
■ Client-side encryption Encryption takes place before the data is sent to Cloud Storage. Such data is sent encrypted to Cloud Storage, but it is also encrypted on the server side. 
Options 2, 3 and 4 are incorrect. Encryption when writing to storage is performed automatically and does not need to be set separately by the user.
[reference] https://cloud.google.com/storage/docs/encryption"
"Your company is developing an application that performs data processing every 30 minutes. This application is developed on App Engine and runs batch jobs. There is a problem with the execution interval of this batch job and it needs to be improved.
Which file would you edit to fix this issue?";cron.yaml;config.yaml;setting.yaml;app.yaml;;A;"Option 1 is the correct answer. cron.yaml is a file for setting a scheduled task (cron job) that operates at a specified time or at regular intervals. It is necessary to modify cron.yaml in order to modify the periodic processing such as batch jobs.
Option 2 is incorrect. A file called config.yaml does not exist as a configuration file for App Engine.
Option 3 is incorrect. The file setting.yaml does not exist as a configuration file for App Engine.
Option 4 is incorrect. app.yaml is a file that describes settings related to the application, the entire network, and the programming language runtime. This time it doesn't matter.
[reference] https://cloud.google.com/appengine/docs/flexible/java/configuring-datastore-indexes-with-index-yaml#index_definitions"
"The company uses multiple Compute Engines to develop applications. This application needs redundancy to withstand zone failures by scaling up and down depending on the load.
Which feature of Compute Engine do you want to use?";Unmanaged instance group;Instance template;Managed instance group;Resource group;;C;"Option 3 is the correct answer. Managed instance groups allow multiple application operations on the same VM. This is best used when creating a redundant configuration that can withstand zone failures. Managed instance groups provide features such as auto-scaling, auto-repair, multi-zone deployment, and auto-update for scalable and highly available workload processing.
Option 1 is incorrect. Unmanaged instance groups are a good place to load balance a group of heterogeneous instances, or if you need to manage your own instances. Therefore, it is easier to set up this requirement by using a managed instance group.
Option 2 is incorrect. Instance templates are templates that you can use to create virtual machine instances and managed instance groups, and are not relevant here.
Option 4 is incorrect. Resource groups are a feature in Cloud Monitoring to define a set of resources as a group and are not relevant here.
[reference] https://cloud.google.com/compute/docs/instance-groups/"
"The development team used the service account to develop the application, but the error occurred due to poor access control between resources. You decide to identify the cause by viewing the audit log.
Which log should you check?";Policy deny audit log;Data access audit log;Management activity log;ID access audit log;;A;"Option 1 is the correct answer. If an error occurs because access control between resources is not possible, access is denied by the policy. Therefore, it is necessary to check the status of policy denials and to check the policy denial audit log. The policy denial audit log is recorded when the Google Cloud service denies access to a user or service account due to a policy violation.
Option 2 is incorrect. The data access audit log is irrelevant this time because it records API calls to read resource configurations and metadata, as well as user-driven API calls to create, modify, and read user-supplied resource data.
Option 3 is incorrect. This time it is irrelevant because the management activity log records events related to API calls and other management actions that change the resource's configuration or metadata.
Option 4 is incorrect. There is no audit log named ID Access Audit Log.
[reference] https://cloud.google.com/logging/docs/audit"
"As an infrastructure administrator, you manage Cloud Storage on Google Cloud. You want to know the creation time and content type of a stored object.
Which command do you use to get the metadata for the object?";gsutil version gs: // BUCKET NAME / OBJECT NAME;gsutil list gs: // BUCKET NAME / OBJECT NAME;gsutil describe gs: // BUCKET_NAME /OBJECT NAME;qsutil stat qs: // BUCKET NAME / OBJECT NAME;;D;"Option 4 is the correct answer. You can get the metadata of an object by running the command `gsutil stat gs: // [bucket name] / [object name]`.
Option 1 is incorrect. Running gsutil version will print the version of the gsutil tool.
Option 2 is incorrect. gsutil list is not a valid command.
Option 3 is incorrect. gsutil describe is not a valid command.
[reference] https://cloud.google.com/storage/docs/gsutil/commands/stat"
Your company must authorize the auditor to review the audit logs in order to perform an IT audit. Which of the following roles can be used to grant the minimum required permissions without granting more permissions than required to read the log?;roles/logging.privateLogReader;roles/logging.admin.privateLogReader;roles/logging-privateLogsViewer;roles/logging.admin.privateLogViewer;;C;"Option 3 is the correct answer. roles / logging.privateLogViewer grants read permission to all logs, including access transparency logs and data access audit logs. Only the roles / logging.privateLogsViewer should be set because the auditor only needs to be granted read permission.
Option 1 is incorrect. There is no role called roles / 
logging.privateLogReader.
Option 2 is incorrect. Since roles / logging.admin grants full permissions to Logging, it grants more permissions than required. This is an inappropriate setting that violates the principle of least privilege.
Option 4 is incorrect. roles / logging.admin.privateLogViewer does not grant read permission for data access audit logs, which is an inappropriate setting.
[reference] https://cloud.google.com/logging/docs/access-control"
"The company has decided to move a Windows server in an on-premises environment to Google Cloud. You would like to see a list of Windows Server images available as a server administrator.
Which command should you use for this?";gcloud compute images list --project windows-cloud--no-standard-images;gcloud compute images describe --project windows-cloud--no-standard-images;gcloud compute images list --project = windows-cloud--no-standard-images;gcloud compute images describe --windows-cloud;;A;"Option 1 is the correct answer. You can get the Windows Server image list by executing the command `gcloud compute images list --project [image project name] --no-standard-images` with windows-cloud specified in [image project name]. increase.
Options 2 and 4 are incorrect. `gcloud compute images describe` Used to get detailed information about a single image.
Option 3 is incorrect. gcloud compute images list --project = = windows-cloud --project, not --no-standard-images  Enter the project name by opening half-width characters like windows-cloud.
[reference] https://cloud.google.com/sdk/gcloud/reference/compute/images/list"
"You manage to grant roles to the member test-user@gmail.com for project name test1. Now it is necessary to grant the authority of the editor role to the project name test1.
Which command should you use?";gcloud projects add-iam-policy-binding test1 for'user: test-user@gmail.com' add'roles/editor';gcloud projects add-iam-policy-binding test1 -member='user:test-user@gmail.com'--role='roles/ editor';gcloud projects set-iam-policy-binding with --member'user:test-user@gmail.com' -role'roles / editor';gcloud projects add-iam-policy-binding--project=test1.member='user:test-user@gmail.com' -role ='roles / editor';;B;"Option 2 is the correct answer. By executing the command gcloud projects add-iam-policy-binding test1 --member = [member name] --role = [role name] `, you can specify a specific project and set a role for the member. You can grant specific permissions.
Option 1 is incorrect. The command for 'user: test-user@gmail.com'add'roles / editor' is incorrect. By executing --member = [member name] --role = [role name] `, you can specify a specific project, set a role for a member, and grant specific privileges.
Option 3 is incorrect. There is no subcommand called set-iam-policy-binding.
Option 4 is incorrect. The subcommand --project = test1 does not exist. Gcloud projects add-iam-policy-binding Enter [project name] to specify the project.
[reference] https://cloud.google.com/sdk/gcloud/reference/projects/add-iam-policy-binding"
"Your e-commerce (EC) company utilizes a MySQL database with a Cloud SQL server. You need to connect to your MySQL instance from the command line as a database administrator.
Which command do you use to connect to the database as the root user using the embedded client?";gcloud sql connect [INSTANCE-ID] --root = user;gsutil sql connect [INSTANCE-ID] --user = root;gsutil sql connect [INSTANCE-ID] --root = user;gcloud sql connect [INSTANCE-ID] --user = root;;D;"Option 4 is the correct answer. You can specify and connect to a specific database by running the command `gcloud sql connect [instance name] --user = [database user name]`.
Option 1 is incorrect. The --root parameter does not exist.
Options 2 and 3 are incorrect. The `gsutil` command is used to operate Cloud Storage and is not relevant here.
[reference] https://cloud.google.com/sdk/gcloud/reference/sql/connect"
"Your company stores EC site data in BigQuery and performs daily data analysis. You plan to run some big queries against BigQuery. You would like to know the cost of querying in advance.
Which bg command do you need to run?";Use bg command with the --cost flag.;Use bg command with the --dry_run flag.;Use g commandwith the dryRun parameter.;Use bg command with the cost parameter.;;B;"Option 2 is the correct answer. Queries are billed based on the number of bytes read, so specify the --dry_run flag of the bg command to get the estimated number of bytes to be read and estimate the price based on this number of bytes.
Option 1 is incorrect. The --cost flag does not exist in bg command
Option 3 is incorrect. The dryRun parameter does not exist in bg command
Option 4 is incorrect. The cost parameter does not exist in bg command
[reference]  https://cloud.google.com/bigquery/docs/best-practices-costs#bq"
"The application being developed by your company needs to work with multiple clients on Google Cloud. Therefore, it is necessary to switch between multiple working clients on the command line.
How should you switch clients?";Create a configuration for each client and activate it with the gcloudauth command.;Create a configuration for each client and activate it with the gcloud config configuration activate command.;Apply the configuration role for each client and activate it for each client with the cloud config configuration activate command.;Apply the settings role for each client and it for each client with the gcloudauth command.;;B;"Option 2 is the correct answer. You can quickly change the Cloud SDK (gcloud command) settings by running the command `gcloud config configurations activate [configuration name]`. This makes it easy to switch between working clients.
Option 1 is incorrect. The `gcloud auth` command is for authenticating your Google Cloud account and is not relevant here.
Options 3 and 4 are incorrect. Roles are permissions to Google Cloud resources that you can't use to change Cloud SDK (gcloud command) settings.
[reference]  https://cloud.google.com/sdk/gcloud/reference/config"
"You are using the Google Kubernetes Engine to set up your service. In doing so, you need to ensure that all internal clients send requests to stable internal IP addresses.
What type of service should you create?";ClusterIP;NodePort;LoadBalancer;ExternalName;;A;"Option 1 is the correct answer. Specify the Cluster IP type for all internal clients to send requests to a stable internal IP address. When you create a Service of type ClusterIP, the Service is assigned an internal (virtual) IP address that can only be communicated within the cluster.
Option 2 is incorrect. If you specify the NodePort type, you can access the Service from the outside with the IP address of the Node and the port number specified in advance. The NodePort type is irrelevant to internal network communication and is therefore incorrect.
Option 3 is incorrect. If you specify the LoadBalancer type, you can use the load balancer provided by Google Cloud to expose the Service to the outside of the cluster. The LoadBalancer type is irrelevant to internal network communication and is incorrect.
Option 4 is incorrect. If you specify the ExternalName type, when traffic is received, it can be forwarded to the external site according to the CNAME addressed to the external domain specified in advance. It is irrelevant to allow all internal clients to send requests to a stable internal IP address, which is incorrect.
[reference]  https://cloud.google.com/kubernetes-engine/docs/concepts/service"
"You need to assign Google Cloud project management privileges to users who are assigned as new Google Cloud administrators.
What set of permissions do you need to manage a Google Cloud project?";resourcemanager.projects.get;"resourcemanager.projects.get,
resourcemanager.projects.getlamPolicy
resourcemanager.projects.setlamPolicy";resourcemanager.projects.getlamPolicyresourcemanager.projects.setlamPolicy;resourcemanager.projects.getresourcemanager.projects.setlamPolicy;;B;"Option 2 is the correct answer. resourcemanager.projects.get is needed to get the project information and resourcemanager.projects.getIamPolicy is needed to get the IAM policy set in the project. In addition, resourcemanager.projects.setIamPolicy is required to set the project's IAM policy.
Options 1, 3 and 4 are incorrect. To manage your Google Cloud project, you need all the permissions included in Option 2.
[reference]  https://cloud.google.com/resource-manager/docs/access-control-proj"
"You have enabled audit logging for Google Cloud management. Then, the storage usage fee has increased significantly.
What type of audit log do you think is the cause?";Data access audit log;Data traffic audit log;System event audit log;Traffic audit log;;A;"Option 1 is the correct answer. If you enable the data access audit log, the log is generated every time data access occurs, which makes the data size very large and causes an increase in cost. Therefore, it is disabled by default except for BigQuery. Must be explicitly enabled to write data access audit logs for Google Cloud services other than BigQuery.
Option 2 is incorrect. There is no type of audit log called data traffic audit log.
Option 3 is incorrect. The system event audit log is a log when the configuration of Google Cloud resources is changed, and the data size does not become very large. System event audit logs are generated by Google's system and not by direct user action.
Option 4 is incorrect. There is no type of audit log called traffic audit log.
[reference]  https://cloud.google.com/logging/docs/audit"
"The team is using Google Cloud to develop their applications and uses a large number of custom images. You need to be able to release a new version of each image while maintaining the ability to roll back to a previous version if needed.
Which features of Compute Engine do you use for that?";Image management group;Image family;Resource group;Instance group;;B;"Option 2 is the correct answer. Use the image family to release a new version of each image while retaining the ability to roll back to a previous version as needed. Image families allow you to group related images and perform rollforward and rollback between specific image versions.
Option 1 is incorrect. A feature called image management groups does not exist in Compute Engine.
Option 3 is incorrect. Resource group is a function to manage a series of resources as a group in Cloud Monitoring.
Option 4 is incorrect. An instance group is a collection of virtual machine (VM) instances that can be managed as a single entity. Compute Engine has two types of instance groups: managed and unmanaged.
[reference] https://cloud.google.com/compute/docs/instance-groups/ 
https://cloud.google.com/compute/docs/images#image_families"
"You manage your account as an administrator of Google Cloud. You need to assign a new service account to an existing VM instance on Compute Engine.
Which command should you use?";gcloud compute assign-instances service-account;gcloud compute instances assign-service-account;gloud compute set instances-service-account;gcloud compute instances set-service-account;;D;"Option 4 is the correct answer. You can grant a service account to a Compute Engine instance by running the command `gcloud compute instances set-service-account [instance name] --service-account = [service account name]`. Perform an operation on the instances with the gcloud compute instances command. After that, by adding set-service-account, it becomes a command to assign a service account.
Option 1 is incorrect. assign-instances service-account is not a valid command.
Option 2 is incorrect. assign-service-account is not a valid command.
Option 3 is incorrect. set instances-service-account is not a valid command.
[reference]  https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-service-account"
"You are the administrator of the Google Cloud environment. Reports should be integrated into Cloud Monitoring Dashboards in the same operations suite to monitor resources distributed across multiple projects.
What kind of settings do you need for this?";In the Monitoring navigation panel, select Settings and run Add Google Cloud Project.;Configure one Operations Suite account, create a group in the Operations Suite and add the project name.;For each project, create an operations suite account and add the project name.;Configure a single Operations Suite account and link all projects to the same account.;;A;"Option 1 is the correct answer. In the Monitoring navigation panel, select Settings and then Add Google Cloud Project to monitor resources distributed across multiple projects in the same suite of Cloud Monitoring Dashboards. You can integrate the report into.
You can change your project's Cloud Monitoring configuration to monitor resources associated with multiple Google Cloud projects and accounts. Projects or accounts that you add to the metric scope in this way are called monitored projects. The setting procedure at that time is as follows.
• In the Monitoring navigation panel, select Settings.
• Select Add Google Cloud Project to add the project. Dashboards in the Operations Suite Cloud Monitoring make it easy to track how important metrics change over time.
The dashboard allows you to visualize data to debug the high latency of your application, track key metrics for your application, and more.
[reference] https://cloud.google.com/monitoring/settings"
"The company uses App Engine to build web applications. The developer is developing a component that sends CloudPub / Sub messages. The Cloud Pub / Sub API is currently disabled. It is necessary to use the service account to authenticate the application that uses the API.
What is the required response for an application to use Cloud Pub / Sub?";Create and configure a subscription on the CloudPub / SubAPI console screen.;Execution processing by CloudPub / SubAPI is automatically enabled by linking with the service.;Enable CloudPub / SubAPI in the Pub / Sub section of the Google Cloud Console.;Enable Cloud Pub / Sub API on the account screen of the Google Cloud console.;;C;"Option 3 is the correct answer. To set up a Pub / Sub topic subscription in your application, follow these steps:
1. Select an existing project or create a new one. The first time you use Google Cloud, a default project will be created.
2. In the Cloud Console Home section, make a note of your project ID. Use this value when setting up your current Cloud Storage project during the Cloud SDK initialization process. You also pass this ID to your Python script when you launch your publisher and subscriber applications.
3. Go to the Pub / Sub section of the Google Cloud Console. Follow the instructions to enable the API.
4. Click Create Topic. The publisher application sends a message to the topic. Use the name hello_topic.
5. On the Topic Details page, click Create Registration.
Therefore, it is necessary to enable CloudPub / SubAPI in the Pub / Sub section of the Google Cloud Console
[reference]  https://cloud.google.com/pubsub/docs/building-pubsub-messaging-system"
"One company decided to use Google Cloud to set up a Docker container-based CI / CD environment.
Which service is used to store and manage Docker container images?";Cloud Storage;Container Registry;Docker repository;Kubernetes Engine repository;;B;"Option 2 is the correct answer. Use the Container Registry when setting up a Docker container-based CI / CD environment using Google Cloud. Container Registry is a private container image registry that supports the Docker Image Manifest V2 and OCI image formats. This provides a subset of the features of the Artifact Registry.
Option 1 is incorrect. Cloud Storage is object storage for enterprises of all sizes. There is no limit to the amount of data that can be saved. You can get the data as many times as you want. Not available when setting up a Docker container-based CI / CD environment.
Options 3 and 4 are incorrect. These services are not Google Cloud services.
[reference]  https://cloud.google.com/container-registry"
When setting up the Compute Engine VM, it was necessary to check detailed information such as CPU type. Which command is used to get a list of all CPU types available in a particular zone;gcloud dns managed-zones describe;gcloud compute zones describe;gcloud zones describe;gcloud compute zones list;;B;"Option 2 is the correct answer. You can get a list of all CPU types available in a particular zone by running gcloud compute zones describe. gcloud is a command line tool for manipulating computing resources. VM operation is done via the gcloud compute.
Option 1 is incorrect. gcloud dns managed-zones describe is a command that displays the details of cloud DNS managed zones.
Option 3 is incorrect. gcloud zones describe is not a valid command.
Option 4 is incorrect. The gcloud compute zones list lists the Google Compute Engine zones.
[reference]  https://cloud.google.com/sdk/gcloud/reference/compute/zones/describe"
"You are an engineer in charge of web application development using Google Cloud. We are currently using the DeploymentManager template to prepare to deploy the preferences. You've modified a complex DeploymentManager template and want to make sure that all defined resource dependencies are properly met before committing to a project.
Which of the following is the proper confirmation method?";Run the DeploymentManager template for the same project to see the status of interdependent resources.;Use the preview option in the same project to run the Deployment Manager template to see the status of interdependent resources.;On the Stackdriver Logs page of the Google Cloud console, run the Deployment Manager template to see the status of interdependent resources.;On the Stackdriver Logs page of the Google Cloud console, use the preview option to run the Deplovment Manager template to see the status of interdependent resources.;;B;"Option 2 is the correct answer. You can use the preview option in the same project and then run the Deployment Manager template to ensure that all resource dependencies defined are properly met. The Deployment Manager service previews the configuration by extending the complete configuration and creating ""shell"" resources. Run the Deployment Manager template to see the status of the interdependent resources.
[reference] https://cloud.google.com/deployment-manager/docs/configuration/preview-configuration-file"
You have created an instance for SQL Server with Compute Engine to build a relational database. Which of the following is the easiest way to connect to this SQL Server?;Use the sqlcmd utility to connect to your SQL Server instance.;Set your Windows password in the Google Cloud console. Make sure the firewall rule for port 22 exists and log in to the instance with your credentials.;Install the RDP client on your desktop and use your credentials to log in to your instance.;Install the RDP client on your desktop, set your Windows username and password in Google Cloud Console, and log in to your instance using your credentials.;;A;" Option 1 is the correct answer. GCP allows you to configure SQL Server by installing a SQL Server license on Compute Engine. To connect to a SQL Server-configured instance from the Internet, you can use the sqlcmd utility to connect to a SQL Server instance.
[Overview of connection procedure]
1. Make sure you have installed the client and configured access to the instance.
2. Follow the steps similar to the examples on each page of Using SQL Server Quick Starts and Utilities to connect using the sqlcmd command.
3. To connect to your instance using SSL, use the steps in the Client-Initiated Encryption section and related sections of the Encrypting Connection to SQL Server on Linux page.
【reference】 https://cloud.google.com/sql/docs/sqlserver/connect-admin-ip"
"Your company hosts a web application on Google Cloud's Compute Engine. This application tends to have a temporary spike in load. Therefore, it is necessary to set an autoscaling policy.
What is the incorrect element available when setting the autoscaling policy?";Concurrency quantity;CPU usage;Number of requests;External HTTP (S) load balancer processing power;;A;"Option 1 is the correct answer. ComputeEngine can scale managed instance groups based on request count, CPU utilization, and external HTTP (S) load balancer processing power, but concurrency quantities do not trigger cluster resizing.
Options 2, 3 and 4 are incorrect because they are correct elements. ComputeEngine scales based on the number of requests, CPU utilization, and the processing power of the external HTTP (S) load balancer."
"As a result of IT audits, your company needs to set bucket-level access to Cloud Storage buckets. However, after applying bucket-level access control, some users who could access the objects in the bucket have now lost access.
Which of the following is the most likely cause?";They can't be accessed because you don't have the IAM permission to allow the user to access the objects in your bucket.;They cannot access it because there is no ACL that allows the user to access the objects in the bucket.;Uniform bucket policies do not allow you to control user permissions and must be changed to ACLs.;Cannot access because there is no firewall that allows the user to access the objects in the bucket.;;A;"Option 1 is the correct answer. When you enable uniform bucket-level access for a bucket, access control lists (ACLs) are disabled, and only bucket-level IAM permissions grant access to that bucket and the objects in it. Therefore, if a previously accessible user was accessing it through an ACL, it is likely that they are no longer able to access it because they do not have IAM privileges after configuring the bucket-level access.
Options 2 and 3 are incorrect. Since access is not possible because there is no IAM permission setting, access is not possible even if ACL is attached.Option 4 is incorrect. Firewalls set up traffic communication for virtual networks. This time, access is disabled after changing the bucket level access settings, so it is considered that the traffic communication control settings are irrelevant.

[reference} https://cloud.google.com/storage/docs/uniform-bucket-level-access"
Your company has decided to use multiple clouds, including Google Cloud, as a hybrid cloud. You will only be using Google Cloud resources in Asia. How can you prevent Google Cloud resources from being created outside of Asia?;Create constrained data lifecycle management policies at the organizational level of the resource hierarchy to limit where resources are available.;Create constrained data lifecycle management policies at the folder level of the resource hierarchy to limit where resources are available.;Create constrained policies at the folder level in the resource hierarchy to limit where resources are available.;Create constrained policies at the organizational level of the resource hierarchy to limit where resources are available.;;D;"Option 4 is the correct answer. Constraints are a standard way to limit where resources can be created, and you can apply constrained policies to limit resource usage in a particular region. If the policy is applied at the folder level, it is inefficient to set overall resource limits as in this case, as the policy must be applied to every folder individually. Therefore, you must create constrained policies at the organization level to limit resource usage at the organization level.
Option 1 is incorrect. The data lifecycle management policy is used for management such as deleting data after a specific period. It is incorrect because it has nothing to do with this scenario.
Options 2 and 3 are incorrect. If the policy is applied at the folder level, it would be inefficient to set overall resource limits as in this case, as the policy would have to be applied to every folder individually. Therefore, create constrained policies at the organization level to limit resource usage at the organization level.
[reference]  https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations"
"As a solution architect, you are building an image analysis system that uses an image identification API. Processing by this image identification API takes about 5 seconds for each image. The frequency of image processing is highly volatile, as there are periods when images are not uploaded and there are periods when many images are uploaded in a short period of time.
Which service should you use to build this image processing application?";Compute Engine;Cloud Dataflow;Cloud Functions;Container Registry;;C;"Option 3 is the correct answer. If the frequency of image processing is highly volatile, such as when images are not uploaded or when many images are uploaded in a short period of time, it is inefficient to use an application that always starts a VM. It will be. Therefore, it is best to use a serverless application that is charged only when it is used. This time, it is best to use Cloud Functions to execute the work process using the image identification API. Cloud Functions can develop Google Cloud event response processing such as uploading files to Cloud Storage serverlessly.
By using Cloud Functions, you can complete the analysis process within the time limit. You don't have to run the application continuously to see new image uploads, as there are periods when images aren't uploaded. Therefore, there is no need for a full-scale system configuration using App Engine Flexible, Cloud Engine, Kubernetes Engine, etc.
Option 1 is incorrect. Compute Engine is a service that allows you to rent virtual machines for an hourly fee. Costs will be incurred due to pay-as-you-go billing according to the startup time. This time, in addition to calling the process using the image identification API to execute the work process, it is more cost-effective to use Cloud Functions for the process using the virtual server because there is volatility in the execution frequency.
Option 2 is incorrect. Cloud Dataflow is a stream and batch processing service. Cloud Dataflow can perform ETL processing in a fully managed environment. It is not suitable for work processes like this one.
Option 4 is incorrect. Container Registry provides centralized management and vulnerability analysis of Docker images. Fine-grained access control also allows you to decide which users are allowed access to what. Therefore, it has nothing to do with this case.
[reference] https://cloud.google.com/functions/docs/how-to"
An instance of Compute Engine utilizes a persistent disk, and you were asked to clone this persistent disk. In that case, the specific elements must be the same on the source disk and the clone disk. Which elements should be the same?;zone;size;snap shot;VPC;;A;"Option 1 is the correct answer. When cloning a persistent disk, the source and clone disks must be in the same zone and region and of the same type.
Option 2 is incorrect. The size of the clone must be greater than or equal to the size of the source disk. It doesn't have to be the same.
Option 3 is incorrect. This time it doesn't matter because snapshots are used when data protection is needed to improve restoring, such as backup or disaster recovery.
Option 4 is incorrect. The VPCs do not have to be the same. The source and clone discs must be in the same zone and region and of the same type.
[reference]  https://cloud.google.com/compute/docs/disks/create-disk-from-source"
At your company, an in-house auditor is conducting an IT audit and wants to collect and analyze application log files generated from these servers for reporting purposes. Which of the following audit logs are available for projects, folders, and organizations?;Resource access audit log;User access audit log;Data access audit log;Policy access audit log;;C;"Option 3 is the correct answer. Cloud Audit Logs collects the following logs and can be used by users for analysis.

--Administrative activity audit log
--Data access audit log
--System event audit log
--Policy denial audit log 

Option 1 is incorrect. Cloud Audit Logs does not collect resource access audit logs.
Option 2 is incorrect. Cloud Audit Logs does not collect user access audit logs.
Option 4 is incorrect. Cloud Audit Logs does not collect policy access audit logs. 
[reference] https://cloud.google.com/logging/docs/audit"
"Preemptible virtual machines are shut down after a certain amount of time has passed since they were run.
Which of the following times is it?";12 hours;24 hours;36 hours;48 hours;;B;"Option 2 is the correct answer. If the preemptive machine is not shut down within 24 hours, Google will stop the instance. This stop is called a preempt.
Preemptible VM instances are much cheaper than standard VMs (60-91% discount). However, the Compute Engine may stop this instance if it needs to reclaim compute capacity for allocation to other VMs. Preemptable Instances are a feature that takes advantage of Compute Engine's surplus capacity, and availability depends on usage.
Preemptible instances behave like regular instances, but with the following limitations:
· The Compute Engine can stop a preemptible instance at any time due to a system event. It is unlikely that the Compute Engine will shut down a preemptible instance due to a system event, but it can vary from zone to zone on a daily basis, depending on the circumstances at the time.
· The Compute Engine always shuts down the preemptible instance after running for 24 hours. 
· Preemptible instances are finite Compute Engine resources and are not always available. 
Preemptible instances cannot become regular VM instances through Live migration. Also, it cannot be set to restart automatically in the event of a maintenance event.
· Because of these restrictions, preemptible instances are not covered by service level agreements. They are excluded from Compute Engine SLAs
・ Compute Engine Free tier for Google Cloud does not apply to preemptible instances."
You need to create a custom role on a resource for application development. Which of the following settings can give a user permission to create a custom role?;iam.roles.create;Compute.roles.create;iam.create.roles;Compute.create.roles;;A;"Option 1 is the correct answer. Users with iam.roles.create permissions are granted the right to create custom roles. Set an IAM role with the permissions as follows: 

resource ""google_organization_iam_custom_role""

""my-custom-role"" {

Role_id = ""myCustomRole""

Org_id = ""123456789""

Title = ""My Custom Role""

Description = ""A description""

Permissions = [

""iam.roles.list"", ""iam.roles.create"", ""iam.roles.delete""

] 

[reference]
https://cloud.google.com/sdk/gcloud/reference/iam/roles/create 
 Options 2, 3 and 4 are incorrect. These are the wrong role permissions."
A certain application uses Compute Engine to execute a daily batch job at 1am in the middle of the night to back up data. These jobs take about an hour to complete. Choose a Compute Engine configuration that minimizes costs from the following.;Use a micro-instance type 3-node cluster in Compute Engine.;Use N2 machine type preemptive VM instances in Compute Engine.;Use a single node calster of N2 machine type in Compute Engine.;Use E2 standard machine type preemptive VM instances in Compute Engine.;;D;"Option 4 is the correct answer. Since the purpose of this scenario is to execute the process for only one hour a day, it is necessary to select the best instance type to execute at a low price in the short term. In this case, you can minimize the cost by using a preemptible VM.
Preemptible VMs are instances that can be created and run at a much lower cost than regular instances. However, the Compute Engine can stop this instance if other tasks require access to the resource. Preemptable Instances are a feature that takes advantage of Compute Engine's surplus capacity, and availability depends on usage. The E2 machine type is a cost-optimized VM that provides up to 32 vCPUs with up to 8 GB per vCPU and up to 128 GB of memory. It is the most suitable type for finding the most cost-optimized processing like this time.
Option 1 is incorrect. Using a micro-instance type 3-node cluster is too high performance and is costly.
Options 2 and 3 are incorrect. For N2 machine types, up to 80 vCPUs and 8 GB of memory per vCPU are available on the Intel Cascade Lake CPU platform. This is more powerful and more expensive than the E2 machine type.

[reference] https://cloud.google.com/compute/docs/machine-types/ 
 https://cloud.google.com/compute/docs/instances/preemptible/"
The company is building a mobile app where users can share photos. This mobile app stores photos in Cloud Storage and created two buckets for redundancy. Which command should you use to synchronize the contents of the two buckets?;gcloud rsync;gsutil rsync;gsutil cp sync;gcloud cp sync;;B;"Option 2 is the correct answer. gsutil is a command line tool for working with Cloud Storage. The gsutil rsync command can synchronize a local directory to a bucket. You can use this to synchronize the contents of the two buckets.  For example, you can use the following commands to match the contents of :
gs: // example-bucket and the local directory of local-dir: 
gsutil -m rsync -r local-dir gs: // example-bucket  If you use the rsync -d flag, it will tell gsutil to delete files in the sync destination (gs: // example-bucket) that do not exist in the sync source (local-dir). In this way you can also sync between the two buckets. 
Options 1 and 4 are incorrect. A command line tool for gsutil to work with Cloud Storage, not gcloud.
Option 3 is incorrect. There is no command called gsutil cp sync.
[reference] https://cloud.google.com/storage/docs/working-with-big-data"
You are starting application develop using a managed VM instance group. The application needs to run only one instance of VM per project. How should you configure this instance group?;Disable autoscaling and set the number of instances to launch to1.;Enable autoscaling and set the number of instances to launch to1.;Disable autoscaling and set the minimum number of instances to 1 and the maximum number to 1.;Enable autoscaling and set the minimum number of instances to 1 and the maximum number to 1.;;D;"Option 4 is the correct answer. You can maintain 1 VM instance at all times by enabling autoscaling for your instance group and setting the minimum number of instances to 1 and the maximum number to 1. Autoscaling adds VMs to managed instance groups under heavy load and removes VMs when the need for VMs decreases. In other words, if the minimum number is 1, the number of instances is adjusted so that the minimum value is 1 at the time of scale-in, and if the maximum number is 1, the number of instances is adjusted so that the maximum value is 1 at the time of scale-out. As a result, the number of instances can always be 1.
[reference] https://cloud.google.com/compute/docs/autoscaler"
As an infrastructure manager, you have a monitoring system in place. Which notification method is incorrect when making notifications based on CPU usage conditions?;Email;Slack;PagerDuty;Teams;;D;"Option 4 is the correct answer. Notification settings using Teams are not possible. All three other options are valid notification channels that can be configured in Operations Suite Monitoring. Operations Suite Monitoring allows you to configure alert policies to notify you when an event occurs or when a particular system or custom metric violates a predefined rule. You can also use multiple conditions to define complex alert rules. Notifications can be received via email, SMS, Slack, PagerDuty and more.
Options 1, 2 and 3 are correct. You can receive Operation Suite notifications via email, SMS, Slack, PagerDuty and more.
[reference] https://cloud.google.com/monitoring?hl=ja#section-5"
As a system administrator, you are using the Operations Suite Logging to configure Google Cloud logging. Which service is incorrect destination for LogSync's log forwarding?;CloudSQL;Cloud Storage bucket;BigQuery dataset;Cloud Pub / Sub topic;;A;"Option 1 is the correct answer. CloudSQL is not available as a destination for logging exports. You can use a log router to forward specific logs to supported destinations in any Cloud project. The log destinations supported by Logging are:
- Saving them as JSON files in your Cloud Storage bucket. This provides low-cost long-term storage.
- Creating a table in your BigQuery dataset. This provides big data analysis function.
- Deliver JSON-formatted messages to Pub / Sub topics. 
- Logging integration with third parties such as Splunk is supported.
- Retaining log entries in the Cloud Logging log bucket. This provides Cloud Logging storage with a customizable retention period.x 
Options 2, 3 and 4 are incorrect because they are correct. Cloud Storage buckets, BigQuery datasets, and Cloud Pub / Sub topics are all three Log Destinations supported by Logging.
[reference]https://cloud.google.com/stackdriver/docs/solutions/gke/using-logs"
As a system administrator, you are performing log analysis for Operations Suite Logging. Which of the following has the ability to filter and display the logs?;Log Explorer;Log viewer;Log router;Log sync;;A;"Option 1 is the correct answer. Use Log Explorer in the Google Cloud Console to provide recommended queries that make it easier to find important logs. Log Explorer uses Boolean expressions to specify specific parts of a project's entire log entry. You can use such queries to select log entries from a particular log or log service, or log entries that meet the criteria for metadata or user-defined fields.
Option 2 is incorrect. Log Viewer is an analysis tool previously used by Log Explorer.
Option 3 is incorrect. Cloud Logging receives log entries through the Cloud Logging API, and in the process the log entries go through the log router. The log router sink matches each log entry against existing inclusion and exclusion filters to determine whether to send the log entry to a destination, such as a Cloud Logging bucket, or to exclude Cloud Logging ingestion altogether.
Option 4 is incorrect. Sync is used to forward logs to multiple destinations.
[reference] https://cloud.google.com/logging/docs/view/query-library-preview?hl=ja#kubernetes-filters"
"There is a problem with the application code and there has been an application error. As an application engineer, you need to use the Operations Suite to identify application code bottlenecks.
Which feature will generate a detailed report of the latencies that cause performance degradation? ";Cloud Debugger;Cloud Trace;Cloud Logging;Cloud Monitoring;;B;"Option 2 is the correct answer. Cloud Trace is a tool used for bug analysis in the operation suite. It automatically analyzes all traces of your application and produces detailed latency reports that cause poor performance. This feature allows you to use the Operations Suite to identify bottlenecks in your application code.
Option 1 is incorrect. Cloud Debugger is a Google Cloud feature that allows you to investigate the state of a running application in real time without stopping or slowing down the application.
Option 3 is incorrect. Cloud Logging is a tool for real-time log management and scalable analysis
Option 4 is incorrect. Cloud Monitoring provides visibility into application and infrastructure performance, availability, and health.
[Reference]https://cloud.google.com/trace/"
"You need to check the status of BigQuery to see what's wrong with BigQuery.
Where can you check the status of BigQuery service in Google Cloud?";Google Cloud Status Dashboard;BigQuery console;Cloud Customer Care Portfolio;TAM;;A;"Option 1 is the correct answer. The Google Cloud Status Dashboard (https://status.cloud.google.com/) provides information about the status of Google Cloud services. It is possible to check the status of the BigQuery service.
Option 2 is incorrect. Check in the Google Cloud Status Dashboard instead of the Bigquery console.
Option 3 is incorrect. The Cloud Customer Care Portfolio is a scalable and flexible set of services centered around user needs. This has nothing to do with checking the status of the service.
Option 4 is incorrect. By subscribing to Premium Support, you will receive support for critical user awareness workloads from a dedicated Technical Account Manager (TAM). This has nothing to do with checking the status of the service.
[reference]https: //status.cloud.google.com/"
Which of the following services has a pricing model based on the settings of the virtual machine?;BigQuery and CloudPub / Sub;Compute Engine and BigQuery;Kubernetes Engine and BigQuery;Compute Engine and Kubernetes Engine;;D;"Option 4 is the correct answer. Both Compute Engine and Kubernetes Engine base their costs on virtual machines. Compute Engine calculates disk size, machine type memory, and network usage in gigabytes (GB). With KubernetesEngine, the clusters you create will incur a flat rate of $ 0.10 per cluster for one hour after the free tier expires, plus CPU, memory, and ephemeral storage computing resources provisioned in the pod.
[reference] https://cloud.google.com/compute/all-pricing?hl=ja%20https%3A%2F%2Fcloud.google.com%2Fkubernetes-engine%2Fpricing%3Fhl%3Dja"
You are in charge of the task of starting and managing VMs using Compute Engine. Which feature do you use for dynamic provisioning based on a dedicated configuration file?;Managed instance group;Unmanaged instance group;Service account;Instance cluster;;A;"Option 1 is the correct answer. You can utilize a managed instance group (MIG) for dynamic provisioning based on a dedicated configuration file. MIG is a redundantly configurable group that allows applications to be deployed on multiple identical VMs. You can perform auto-scaling, auto-repair, region (multi-zone) deployment, auto-update, and more for the instances that make up a managed instance group for scalable and highly available workload processing. 
Option 2 is incorrect. non-managed instance groups allow load balancing across a set of VMs that you manage yourself. Managed instance groups are best suited for configurations where automatic provisioning is performed.
Option 3 is incorrect. Service accounts are special accounts used by applications and virtual machine (VM) instances, not users. The application uses the service account to make authorized API calls. It is approved as the service account itself, or as a Google Workspace or Cloud Identity user by domain-wide delegation.
Option 4 is incorrect. There is no group called an instance cluster, but if you are using Docker you can configure a cluster with VMs.
[reference] https://cloud.google.com/compute/docs/instance-groups"
"Your company uses Google Cloud billing accounts in multiple departments, and billing processing is mixed. To properly track your department's spending, you need to link your project to your billing account.
Who can perform this operation?";Billing account owner;Billing Account Viewer;Billing account user;Billing account creator;;C;"Option 3 is the correct answer. Billing Account User (roles / billing.user) can link the project to the billing account. The permissions for this role are heavily restricted due to the need for increased security and are commonly used in combination with the project creator. These two roles allow you to create a new project linked to the billing account to which the role is granted. 
Option 1 is incorrect. The role of billing account owner does not exist.
Option 2 is incorrect. The billing account viewer (roles / billing.viewer) is the right to view cost information and transactions for billing accounts. You cannot link or unlink projects.
Option 4 is incorrect. The billing account creator (roles / billing.creator) is authorized to create new billing accounts and cannot link or unlink projects.
[reference] https://cloud.google.com/billing/docs/how-to/billing-access/"
;;;;;;;
;;;;;;;
;;;;;;;
 ;;;;;;;